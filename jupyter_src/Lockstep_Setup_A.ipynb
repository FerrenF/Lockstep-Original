{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "import pandas as pd\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from fastparquet import ParquetFile\n",
    "\n",
    "verbosity = 5\n",
    "\n",
    "twibot_path = r\"/dataset/twibot22\"\n",
    "twibot_user = r\"/dataset/twibot22/user.json\"\n",
    "twibot_label = r\"/dataset/twibot22/label.csv\"\n",
    "twibot_edges = r\"/dataset/twibot22/edge.csv\"\n",
    "# Files in the path specified by twibot_path, that begin with %twibot_node_identifier_str%, will be assumed as node files and converted if needed.\n",
    "twibot_node_identifier_str = \"tweet_\"\n",
    "\n",
    "generated_data_output = r\"/dataset/twibot22/generated_data\" # output is saved in this directory\n",
    "ls_userdata_output = rf\"{generated_data_output}/userdata.jsonl\" # the desired filename of bot detail output\n",
    "\n",
    "# Some tasks might be multithreadable. Set the max number of workers here.\n",
    "concurrent_max_workers = 2\n",
    "\n",
    "# Set this to false to disable the use of threads when processing tweet files.\n",
    "threading = True\n",
    "\n",
    "def debug_print(m, level=5, r=None):\n",
    "    if level <= verbosity:\n",
    "        print(m)\n",
    "        if r:\n",
    "            raise r\n",
    "\n",
    "def is_data(name, _dir=generated_data_output):\n",
    "    file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "def get_data(name, _dir=generated_data_output,pqargs={},**kwargs):\n",
    "    if is_data(name, _dir):\n",
    "        file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "        print(f\"Loading existing data from {file_path}\")\n",
    "        pf = ParquetFile(file_path, **pqargs)\n",
    "        return pf.to_pandas(**kwargs)\n",
    "    return False\n",
    "\n",
    "def save_data(name, _dir=generated_data_output, df, **kwargs):\n",
    "    file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "    debug_print(f\"Saving data to {file_path}\", 3)\n",
    "    os.makedirs(_dir, exist_ok=True)\n",
    "    fastparquet.write(file_path, df, **kwargs)\n",
    "    #df.to_parquet(file_path, **kwargs)\n",
    "    return df\n",
    "\n",
    "# To quietly stop cell execution\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lockstep, the full recipe. Part A - Pre-processing\n",
    "\n",
    "### Step 1: Convert user detail json into parquet for analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# First, check if the data for users exists. If it does, skip this step.\n",
    "FLOW_SKIPPING_USERS_GEN = False\n",
    "if is_data(\"users\"):\n",
    "    FLOW_SKIPPING_USERS_GEN=True\n",
    "    debug_print(\"User data parquet already present. Skipping generation.\",5)\n",
    "else:\n",
    "\n",
    "    # The data does not exist, so it is time to create it from the raw source as specified in configuration.\n",
    "    debug_print(\"Reading label data...\",4)\n",
    "    with open(twibot_label, 'r') as fi:\n",
    "        udata_label = pd.read_csv(fi, header=0, names=[\"id\", \"label\"], encoding=\"UTF8\", index_col='id')\n",
    "    debug_print(display(udata_label.head(5)),5)\n",
    "\n",
    "    with open(twibot_user, 'r') as fi:\n",
    "        udata_detail = pd.read_json(fi, orient='records')\n",
    "\n",
    "    # Merge the data we have from the labels with the complete user records.\n",
    "    debug_print(\"Merging label data...\",4)\n",
    "    udata_detail.set_index('id')\n",
    "    udata_detail=udata_detail.join(udata_label, on=['id'])\n",
    "\n",
    "    debug_print(f\"collected details of {udata_detail.shape} users.\",4)\n",
    "    debug_print(display(udata_detail.head(5)),5)\n",
    "\n",
    "    del udata_label\n",
    "    gc.collect()\n",
    "\n",
    "# Persists: udata_detail"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Column pruning.\n",
    "\n",
    "We don't need all of these and can remove them very early on. We can also do the work of flattening certain object columns early.\n",
    "\n",
    "Columns to remove:\n",
    "    verified,withheld,pinned_tweet_id,protected,profile_image_url\n",
    "Columns to flatten:\n",
    "    public_metrics,entities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# If this flag has been set, then the users dataset has likely already been generated and has had the unwanted columns dropped.\n",
    "if not FLOW_SKIPPING_USERS_GEN:\n",
    "    trash_columns = ['verified','withheld','pinned_tweet_id','protected','profile_image_url']\n",
    "    flatten_columns = ['public_metrics','entities']\n",
    "\n",
    "    debug_print(\"Dropping columns: \"+json.dumps(trash_columns),5)\n",
    "    udata_detail.drop(columns=trash_columns, errors='ignore',inplace=True)\n",
    "\n",
    "    debug_print(\"Flattening columns: \"+json.dumps(flatten_columns),5)\n",
    "    for col in flatten_columns:\n",
    "        attributes_df = pd.json_normalize(udata_detail[col])\n",
    "        udata_detail = pd.concat([udata_detail.drop(columns=[col]), attributes_df], axis=1)\n",
    "\n",
    "    udata_detail['id'] = udata_detail['id'].apply(lambda x: int(str(x).strip('ut')) if str(x)[0] in \"ut\" else int(x)).astype('UInt64').fillna(0)\n",
    "    debug_print(f\"New Shape: {udata_detail.shape}\",5)\n",
    "    if verbosity >= 5:\n",
    "        display(udata_detail.head(1))\n",
    "\n",
    "    debug_print(f\"Saving data as parquet at {generated_data_output}\",1)\n",
    "    save_data(\"users\", udata_detail)\n",
    "\n",
    "    # It's debatable how effective this is in Jupyter but I really do need every last byte.\n",
    "    del udata_detail, trash_columns, flatten_columns\n",
    "    gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checkpoint 1, quick tests!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's ensure we have these columns so far. Note the description.name features - this is intentional.\n",
    "\n",
    "required_columns = ['created_at',\n",
    "    'description',\n",
    "    'id',\n",
    "    'location',\n",
    "    'name',\n",
    "    'url',\n",
    "    'username',\n",
    "    'label',\n",
    "    'followers_count',\n",
    "    'following_count',\n",
    "    'tweet_count',\n",
    "    'listed_count',\n",
    "    'url.urls',\n",
    "    'description.urls',\n",
    "    'description.mentions',\n",
    "    'description.hashtags',\n",
    "    'description.cashtags']\n",
    "\n",
    "# Test the user data.\n",
    "debug_print(f\"Loading data from parquet at {generated_data_output}\", 1)\n",
    "udata_detail = get_data(\"users\")\n",
    "\n",
    "debug_print(f\"Loaded parquet. Shape: {udata_detail.shape}\", 5)\n",
    "if verbosity >= 5:\n",
    "    display(udata_detail.head(3))\n",
    "\n",
    "# Check for required columns.\n",
    "missing_columns = [col for col in required_columns if col not in udata_detail.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing in the dataset: {missing_columns}\")\n",
    "else:\n",
    "    debug_print(\"All required columns are present.\", 2)\n",
    "\n",
    "# Print some statistics about the dataset.\n",
    "try:\n",
    "    stats = {\n",
    "        \"total_rows\": len(udata_detail),\n",
    "        \"column_counts\": udata_detail.count().to_dict(),\n",
    "        \"missing_values\": udata_detail.isnull().sum().to_dict(),\n",
    "    }\n",
    "    debug_print(\"Dataset statistics:\", 3)\n",
    "    if verbosity >= 3:\n",
    "        for key, value in stats.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    debug_print(f\"Failed to compute statistics: {e}\", 1)\n",
    "\n",
    "# Cleanup\n",
    "del udata_detail\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Convert tweet JSON files to parquet.\n",
    "\n",
    "Twibot-22 tweet data or 'nodes' are originally in JSON format. The original code for Lockstep would read and operate with this JSON, also.\n",
    "The problem is, this is not fast enough for the firehose that would be a social media platform's activity. JSON's performance starts to suffer in very large datasets, something that the parquet format boasts excellent performance in. Hopefully, future applications of this code need not convert such large source JSON files like this - it's very time consuming.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import ijson\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Define default values for columns\n",
    "default_values = {\n",
    "    **{col: 0 for col in ['quote_count', 'like_count', 'retweet_count', 'reply_count']},\n",
    "    **{col: 0 for col in ['id', 'conversation_id', 'author_id', 'in_reply_to_user_id']},\n",
    "    **{col: '' for col in [\"text\", \"source\"]},\n",
    "    **{col: pd.NA for col in [\"urls\", \"annotations\", \"media\", \"user_mentions\",\n",
    "                              \"hashtags\", \"cashtags\", \"symbols\", \"geo\", \"location\"]},\n",
    "}\n",
    "\n",
    "# Columns to drop\n",
    "tweet_drop_cols = [\n",
    "    'reply_settings',\n",
    "    'context_annotations',\n",
    "    'withheld',\n",
    "    'possibly_sensitive',\n",
    "    'attachments',\n",
    "    'referenced_tweets',\n",
    "]\n",
    "\n",
    "\n",
    "def process_file(tweetFile):\n",
    "\n",
    "    def app_clr_col(i, t):\n",
    "        append_parq(i, t)\n",
    "        i.clear()\n",
    "\n",
    "    try:\n",
    "        ij_buffer_size_kb = 256\n",
    "        ij_bsize = ij_buffer_size_kb * 1000 # kb\n",
    "        chunk_size = 500000\n",
    "        target = f\"{twibot_path}/{tweetFile}\"\n",
    "        t_stem = Path(target).stem\n",
    "        targetOutput = Path(rf\"{generated_data_output}/{t_stem}.parquet\")\n",
    "        iteration_records = []\n",
    "\n",
    "        if not is_data(t_stem):\n",
    "            debug_print(f\"Input: {target} Output: {targetOutput}\", 5)\n",
    "            with open(target, 'r') as fi:\n",
    "                tlen = sum(1 for line in fi)\n",
    "                fi.seek(0)\n",
    "                parser = ijson.items(fi, 'item', use_float=True, buf_size=ij_bsize)\n",
    "\n",
    "                for i, record in enumerate(parser):\n",
    "                    iteration_records.append(record)\n",
    "                    if (i + 1) % chunk_size == 0:\n",
    "                        app_clr_col(iteration_records, targetOutput)\n",
    "\n",
    "                if iteration_records:\n",
    "                    app_clr_col(iteration_records, targetOutput)\n",
    "\n",
    "                del tlen, parser\n",
    "                gc.collect()\n",
    "\n",
    "            debug_print(f\"Finished converting file: {targetOutput}\", 5)\n",
    "\n",
    "    except Exception as e:\n",
    "        debug_print(f\"Error processing file {Path(target)}: {e}\", 2)\n",
    "        print(traceback.format_exception(e))\n",
    "\n",
    "\n",
    "def append_parq(df_conv, target, partition_columns=None):\n",
    "    global default_values, tweet_drop_cols\n",
    "    df_conv = pd.DataFrame(df_conv)\n",
    "\n",
    "    # Ensure all default columns exist\n",
    "    for col, default_value in default_values.items():\n",
    "        if col not in df_conv.columns:\n",
    "            df_conv[col] = default_value\n",
    "\n",
    "    # Flatten 'public_metrics' and 'entities' columns\n",
    "    flatten_columns = ['public_metrics', 'entities']\n",
    "\n",
    "    for col in flatten_columns:\n",
    "       if col in df_conv.columns:\n",
    "            try:\n",
    "                flattened = pd.json_normalize(\n",
    "                    df_conv[col].dropna().apply(\n",
    "                        lambda x: x if isinstance(x, dict) else {}\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Ensure all flattened columns have non-python object types\n",
    "                for key in flattened.columns:\n",
    "                    if key in df_conv.columns:\n",
    "                        flattened[key] = flattened[key].apply(\n",
    "                            lambda x: json.dumps(x) if isinstance(x, (list, dict)) else x\n",
    "                        )\n",
    "\n",
    "                existing_keys = [key for key in flattened.columns if key in df_conv.columns]\n",
    "                df_conv.loc[flattened.index, existing_keys] = flattened[existing_keys]\n",
    "\n",
    "            except Exception as e:\n",
    "                debug_print(f\"Failed to normalize column {col}: {repr(e)}\", 2)\n",
    "\n",
    "    # Remove the original flattened columns and additionally, the drop columns specified at the beginning of this cell.\n",
    "    df_conv.drop(columns=flatten_columns + tweet_drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Ensure types on integer columns\n",
    "    int_cols = ['quote_count', 'like_count', 'retweet_count', 'reply_count']\n",
    "    for col in int_cols:\n",
    "        df_conv[col] = df_conv[col].astype('UInt32').fillna(0)\n",
    "\n",
    "    # Ensure types on Int64 columns. Additionally, for columns containing string user IDs, strip it to the int64 digits so it can be stored more efficiently.\n",
    "    int64_cols = ['id', 'conversation_id', 'author_id', 'in_reply_to_user_id']\n",
    "    for col in int64_cols:\n",
    "        df_conv[col] = df_conv[col].fillna(0).apply(\n",
    "            lambda x: int(str(x).lstrip('ut')) if pd.notnull(x) and isinstance(x, str) and str(x)[0] in \"ut\"\n",
    "            else int(x) if pd.notnull(x) else 0\n",
    "        ).astype('UInt64')\n",
    "\n",
    "    # Ensure types on string columns\n",
    "    str_cols = [\"text\", \"source\"]\n",
    "    for col in str_cols:\n",
    "        df_conv[col] = df_conv[col].astype('string')\n",
    "\n",
    "    # Cast object columns into strings (just to make sure...?)\n",
    "    obj_cols = [\"urls\", \"annotations\", \"media\", \"user_mentions\", \"hashtags\",\n",
    "                \"cashtags\", \"symbols\", \"geo\", \"location\"]\n",
    "    for col in obj_cols:\n",
    "        df_conv[col] = df_conv[col].astype(str)\n",
    "\n",
    "    # Cast the created_at column into a pandas supported timestamp\n",
    "    df_conv[\"created_at\"] = pd.to_datetime(df_conv[\"created_at\"]).dt.tz_convert(None)\n",
    "\n",
    "    try:\n",
    "        if target.exists():\n",
    "            df_conv.to_parquet(target, engine='fastparquet', append=True)\n",
    "        else:\n",
    "            debug_print(f\"Created new parquet: {target} with dtypes {df_conv.dtypes}\",5)\n",
    "            debug_print(f\"Sample: {df_conv.head(1)}\",5)\n",
    "            df_conv.to_parquet(target, engine='fastparquet', index=False, partition_on=partition_columns)\n",
    "\n",
    "    except Exception as e:\n",
    "        debug_print(f\"Failed to create or modify: {target} with dtypes {df_conv.dtypes}. Exception: {e}\",2)\n",
    "        debug_print(f\"Sample: {df_conv.head(1)}\",4)\n",
    "        debug_print(f\"Last 2 rows:\\n{df_conv.tail(2)}\", 5)\n",
    "        debug_print(f\"Column null counts:\\n{df_conv.isnull().sum()}\", 4)\n",
    "        raise e\n",
    "\n",
    "    del df_conv\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# BEGIN CELL CODE\n",
    "tweetNodeFiles = list(filter(lambda fileName: twibot_node_identifier_str in fileName,\n",
    "                             [child.name for child in Path(twibot_path).iterdir()]))\n",
    "debug_print(f\"Found nodes: {tweetNodeFiles}\",3)\n",
    "\n",
    "if threading:\n",
    "    with ThreadPoolExecutor(max_workers=concurrent_max_workers) as executor:\n",
    "        list(tqdm(executor.map(process_file, tweetNodeFiles), total=len(tweetNodeFiles), desc=\"Processing Files\"))\n",
    "else:\n",
    "    for file in tweetNodeFiles:\n",
    "        debug_print(\"Processing: \"+file, 5)\n",
    "        process_file(file)\n",
    "\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checkpoint 2: Test tweet dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import fastparquet\n",
    "debug_print(\"Previewing tweet dataset nodes\",5)\n",
    "tweetNodeFiles = list(filter(lambda fileName: twibot_node_identifier_str in fileName,\n",
    "                             [child.name for child in Path(twibot_path).iterdir()]))\n",
    "debug_print(f\"Found nodes: {tweetNodeFiles}\",3)\n",
    "\n",
    "for node in tweetNodeFiles:\n",
    "    try_load = get_data(node) # Optimally, we have a parquet\n",
    "    display(try_load.head(1))\n",
    "    debug_print(f\"Loaded author list! Types:\\n {try_load.dtypes} \\n ...Releasing resources.\",5)\n",
    "    del try_load\n",
    "    gc.collect()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Convert relationships 'edge' file into a parquet.\n",
    "\n",
    "We can use the partitioning feature of parquet files to make queries to the converted dataset a lot faster, based on the limited number of categorical types that a relationship can be."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for present\n",
    "\n",
    "if is_data(\"edges\"):\n",
    "    debug_print(\"User relationship edges parquet already present. Skipping generation.\",5)\n",
    "else:\n",
    "    targetOutput = Path(rf\"{generated_data_output}/edges.parquet\")\n",
    "    debug_print(\"Reading edges data...\",4)\n",
    "    with open(twibot_edges, 'r') as fi:\n",
    "        udata_edges = pd.read_csv(fi, header=0, names=[\"id1\", \"relationship\", \"id2\"], encoding=\"UTF8\")\n",
    "\n",
    "    debug_print(display(udata_edges.head(5)),5)\n",
    "    debug_print(f\"Found {udata_edges.shape[0]} edges in edge file. Converting to parks and recreation.\",4)\n",
    "    save_data('edges', df=udata_edges, partition_on=['relationship'])\n",
    "    gc.collect()\n",
    "\n",
    "    # Persists: udata_detail, shuffle_method\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# End Setup Process A\n",
    "\n",
    "By now you should have a set of parquet files representing the much larger json files that all of the tweets were stored in. Additionally, you should have a users.parquet that contains a limited number of the original columns contained in the user information, merged with the label for the user."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
