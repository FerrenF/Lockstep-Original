{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f1cfb3-7b76-4d0b-b10c-17c4c79403ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "import pandas as pd\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import fastparquet\n",
    "from fastparquet import ParquetFile\n",
    "verbosity = 5\n",
    "\n",
    "twibot_path = r\"/dataset/twibot22\"\n",
    "twibot_user = r\"/dataset/twibot22/user.json\"\n",
    "twibot_label = r\"/dataset/twibot22/label.csv\"\n",
    "twibot_edges = r\"/dataset/twibot22/edge.csv\"\n",
    "\n",
    "# Files in the path specified by twibot_path, that begin with %twibot_node_identifier_str%, will be assumed as node files and converted if needed.\n",
    "twibot_node_identifier_str = \"tweet_\" \n",
    "\n",
    "generated_data_output = r\"/dataset/twibot22/generated_data\" # output is saved in this directory\n",
    "ls_userdata_output = rf\"{generated_data_output}/userdata.jsonl\" # the desired filename of bot detail output\n",
    "\n",
    "# Some tasks might be multithreadable. Set the max number of workers here.\n",
    "concurrent_max_workers = 2\n",
    "\n",
    "# Set this to false to disable the use of threads when processing tweet files.\n",
    "threading = True \n",
    "\n",
    "def debug_print(m, level=5, r=None):\n",
    "    if level <= verbosity:\n",
    "        print(m)\n",
    "        if r:\n",
    "            raise r\n",
    "\n",
    "def is_data(name, _dir=generated_data_output, ext=\".parquet\"):\n",
    "    file_path = os.path.join(_dir, f\"{name}{ext}\")\n",
    "    return os.path.exists(file_path)\n",
    "    \n",
    "def get_data(name, _dir=generated_data_output,pqargs={},**kwargs):\n",
    "    if is_data(name, _dir):\n",
    "        file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "        print(f\"Loading existing data from {file_path}\")\n",
    "        pf = fastparquet.ParquetFile(file_path, **pqargs)\n",
    "        return pf.to_pandas(**kwargs)\n",
    "    return False\n",
    "        \n",
    "def save_data(name, df, _dir=generated_data_output, **kwargs):\n",
    "    file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "    debug_print(f\"Saving data to {file_path}\", 3)\n",
    "    os.makedirs(_dir, exist_ok=True)\n",
    "    fastparquet.write(file_path, df, **kwargs)\n",
    "    #df.to_parquet(file_path, **kwargs)\n",
    "    return df        \n",
    "\n",
    "# To quietly stop cell execution\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b719f-c57e-4fb4-92f1-39ad552695ee",
   "metadata": {},
   "source": [
    "## Lockstep, the full recipe. Part A - Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ace7df-bebb-4029-9edd-514e5bae62b0",
   "metadata": {},
   "source": [
    "### Step 1: Convert user detail json into parquet for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283146e0-1fe6-4846-95b0-08d82119bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User data parquet already present. Skipping generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First, check if the data for users exists. If it does, skip this step.\n",
    "FLOW_SKIPPING_USERS_GEN = False\n",
    "if is_data(\"users\"):\n",
    "    FLOW_SKIPPING_USERS_GEN=True\n",
    "    debug_print(\"User data parquet already present. Skipping generation.\",5)\n",
    "else:\n",
    "\n",
    "    # The data does not exist, so it is time to create it from the raw source as specified in configuration.\n",
    "    debug_print(\"Reading label data...\",4)\n",
    "    with open(twibot_label, 'r') as fi:\n",
    "        udata_label = pd.read_csv(fi, header=0, names=[\"id\", \"label\"], encoding=\"UTF8\", index_col='id')\n",
    "    debug_print(display(udata_label.head(5)),5)\n",
    "\n",
    "    with open(twibot_user, 'r') as fi:\n",
    "        udata_detail = pd.read_json(fi, orient='records')\n",
    "\n",
    "    # Merge the data we have from the labels with the complete user records.\n",
    "    debug_print(\"Merging label data...\",4)\n",
    "    udata_detail.set_index('id')\n",
    "    udata_detail=udata_detail.join(udata_label, on=['id'])\n",
    "    \n",
    "    debug_print(f\"collected details of {udata_detail.shape} users.\",4)\n",
    "    debug_print(display(udata_detail.head(5)),5)\n",
    "    \n",
    "    del udata_label\n",
    "    gc.collect()\n",
    "    \n",
    "# Persists: udata_detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b6c44-61c9-417f-8dd0-49f53ca4debd",
   "metadata": {},
   "source": [
    "### Step 2: Column pruning.\n",
    "\n",
    "We don't need all of these and can remove them very early on. We can also do the work of flattening certain object columns early.\n",
    "\n",
    "Columns to remove:\n",
    "    verified,withheld,pinned_tweet_id,protected,profile_image_url\n",
    "Columns to flatten:\n",
    "    public_metrics,entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba1c6e1-e030-4a7d-875d-6885ca6abd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# If this flag has been set, then the users dataset has likely already been generated and has had the unwanted columns dropped.\n",
    "if not FLOW_SKIPPING_USERS_GEN:\n",
    "    trash_columns = ['verified','withheld','pinned_tweet_id','protected','profile_image_url']\n",
    "    flatten_columns = ['public_metrics','entities']\n",
    "    \n",
    "    debug_print(\"Dropping columns: \"+json.dumps(trash_columns),5)\n",
    "    udata_detail.drop(columns=trash_columns, errors='ignore',inplace=True)\n",
    "    \n",
    "    debug_print(\"Flattening columns: \"+json.dumps(flatten_columns),5)\n",
    "    for col in flatten_columns:\n",
    "        attributes_df = pd.json_normalize(udata_detail[col])\n",
    "        # Remove the first level, join the second level, flatten anything else.\n",
    "        # e.g. {entities: {url: {urls: [{item:key},{item:key}]}, {description: {urls: []}} -> {url.urls: \"[{item:key},{item:key}]\", description.urls: \"[]\"}\n",
    "\n",
    "        # From the single level dict we made above, if the key does not exist as a column in udata_detail, then we make it. The default for these columns shall be \"[]\". \n",
    "        # Then, update the values for our columns with that we have in our normalized json df.\n",
    "        udata_detail = pd.concat([udata_detail.drop(columns=[col]), attributes_df], axis=1)\n",
    "        \n",
    "    udata_detail['id'] = udata_detail['id'].apply(lambda x: int(str(x).strip('ut')) if str(x)[0] in \"ut\" else int(x)).astype('UInt64').fillna(0)\n",
    "    debug_print(f\"New Shape: {udata_detail.shape}\",5)\n",
    "    if verbosity >= 5:\n",
    "        display(udata_detail.head(1))\n",
    "    \n",
    "    debug_print(f\"Saving data as parquet at {generated_data_output}\",1)\n",
    "    save_data(\"users\", udata_detail)\n",
    "\n",
    "    # It's debatable how effective this is in Jupyter but I really do need every last byte.\n",
    "    del udata_detail, trash_columns, flatten_columns\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ccb5e1-48b2-4c7f-bb62-acbc1ac30aad",
   "metadata": {},
   "source": [
    "## Checkpoint 1, quick tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4443a3-f1f9-4511-b927-7905fd35a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from parquet at /dataset/twibot22/generated_data\n",
      "Loading existing data from /dataset/twibot22/generated_data/users.parquet\n",
      "Loaded parquet. Shape: (1000000, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>username</th>\n",
       "      <th>label</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>url.urls</th>\n",
       "      <th>description.urls</th>\n",
       "      <th>description.mentions</th>\n",
       "      <th>description.hashtags</th>\n",
       "      <th>description.cashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-16 02:02:55+00:00</td>\n",
       "      <td>Theoretical Computer Scientist. See also https...</td>\n",
       "      <td>1217628182611927040</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>Boaz Barak</td>\n",
       "      <td>https://t.co/BoMip9FF17</td>\n",
       "      <td>boazbaraktcs</td>\n",
       "      <td>human</td>\n",
       "      <td>7316</td>\n",
       "      <td>215</td>\n",
       "      <td>3098</td>\n",
       "      <td>69</td>\n",
       "      <td>[{'start': 0, 'end': 23, 'url': 'https://t.co/...</td>\n",
       "      <td>[{'start': 41, 'end': 64, 'url': 'https://t.co...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-02 17:56:46+00:00</td>\n",
       "      <td>creative _</td>\n",
       "      <td>2664730894</td>\n",
       "      <td>ðŸŽˆ</td>\n",
       "      <td>olawale ðŸ’¨</td>\n",
       "      <td></td>\n",
       "      <td>wale_io</td>\n",
       "      <td>human</td>\n",
       "      <td>123</td>\n",
       "      <td>1090</td>\n",
       "      <td>1823</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-30 12:10:45+00:00</td>\n",
       "      <td>ðŸ‘½</td>\n",
       "      <td>1266703520205549568</td>\n",
       "      <td>None</td>\n",
       "      <td>panagiota_.b</td>\n",
       "      <td></td>\n",
       "      <td>b_panagiota</td>\n",
       "      <td>human</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at  \\\n",
       "0 2020-01-16 02:02:55+00:00   \n",
       "1 2014-07-02 17:56:46+00:00   \n",
       "2 2020-05-30 12:10:45+00:00   \n",
       "\n",
       "                                         description                   id  \\\n",
       "0  Theoretical Computer Scientist. See also https...  1217628182611927040   \n",
       "1                                         creative _           2664730894   \n",
       "2                                                  ðŸ‘½  1266703520205549568   \n",
       "\n",
       "        location          name                      url      username  label  \\\n",
       "0  Cambridge, MA    Boaz Barak  https://t.co/BoMip9FF17  boazbaraktcs  human   \n",
       "1              ðŸŽˆ     olawale ðŸ’¨                                wale_io  human   \n",
       "2           None  panagiota_.b                            b_panagiota  human   \n",
       "\n",
       "   followers_count  following_count  tweet_count  listed_count  \\\n",
       "0             7316              215         3098            69   \n",
       "1              123             1090         1823             0   \n",
       "2                3               62           66             0   \n",
       "\n",
       "                                            url.urls  \\\n",
       "0  [{'start': 0, 'end': 23, 'url': 'https://t.co/...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "\n",
       "                                    description.urls description.mentions  \\\n",
       "0  [{'start': 41, 'end': 64, 'url': 'https://t.co...                 None   \n",
       "1                                               None                 None   \n",
       "2                                               None                 None   \n",
       "\n",
       "  description.hashtags description.cashtags  \n",
       "0                 None                 None  \n",
       "1                 None                 None  \n",
       "2                 None                 None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required columns are present.\n",
      "Dataset statistics:\n",
      "total_rows: 1000000\n",
      "column_counts: {'created_at': 1000000, 'description': 1000000, 'id': 1000000, 'location': 708458, 'name': 1000000, 'url': 1000000, 'username': 1000000, 'label': 1000000, 'followers_count': 1000000, 'following_count': 1000000, 'tweet_count': 1000000, 'listed_count': 1000000, 'url.urls': 516923, 'description.urls': 87083, 'description.mentions': 213448, 'description.hashtags': 167865, 'description.cashtags': 2874}\n",
      "missing_values: {'created_at': 0, 'description': 0, 'id': 0, 'location': 291542, 'name': 0, 'url': 0, 'username': 0, 'label': 0, 'followers_count': 0, 'following_count': 0, 'tweet_count': 0, 'listed_count': 0, 'url.urls': 483077, 'description.urls': 912917, 'description.mentions': 786552, 'description.hashtags': 832135, 'description.cashtags': 997126}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's ensure we have these columns so far. Note the description.name features - this is intentional.\n",
    "\n",
    "required_columns = ['created_at',\n",
    "    'description',\n",
    "    'id',\n",
    "    'location',\n",
    "    'name',\n",
    "    'url',\n",
    "    'username',\n",
    "    'label',\n",
    "    'followers_count',\n",
    "    'following_count',\n",
    "    'tweet_count',\n",
    "    'listed_count',\n",
    "    'url.urls',\n",
    "    'description.urls',\n",
    "    'description.mentions',\n",
    "    'description.hashtags',\n",
    "    'description.cashtags']\n",
    "\n",
    "# Test the user data.\n",
    "debug_print(f\"Loading data from parquet at {generated_data_output}\", 1)\n",
    "udata_detail = get_data(\"users\")\n",
    "\n",
    "debug_print(f\"Loaded parquet. Shape: {udata_detail.shape}\", 5)\n",
    "if verbosity >= 5:\n",
    "    display(udata_detail.head(3))\n",
    "\n",
    "# Check for required columns.\n",
    "missing_columns = [col for col in required_columns if col not in udata_detail.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing in the dataset: {missing_columns}\")\n",
    "else:\n",
    "    debug_print(\"All required columns are present.\", 2)\n",
    "\n",
    "# Print some statistics about the dataset.\n",
    "try:\n",
    "    stats = {\n",
    "        \"total_rows\": len(udata_detail),\n",
    "        \"column_counts\": udata_detail.count().to_dict(),\n",
    "        \"missing_values\": udata_detail.isnull().sum().to_dict(),\n",
    "    }\n",
    "    debug_print(\"Dataset statistics:\", 3)\n",
    "    if verbosity >= 3:\n",
    "        for key, value in stats.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    debug_print(f\"Failed to compute statistics: {e}\", 1)\n",
    "\n",
    "# Cleanup\n",
    "del udata_detail\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab199c4-53f3-489e-8104-7f0536e8184e",
   "metadata": {},
   "source": [
    "### Step 3: Convert tweet JSON files to parquet.\n",
    "\n",
    "Twibot-22 tweet data or 'nodes' are originally in JSON format. The original code for Lockstep would read and operate with this JSON, also. \n",
    "The problem is, this is not fast enough for the firehose that would be a social media platform's activity. JSON's performance starts to suffer in very large datasets, something that the parquet format boasts excellent performance in. Hopefully, future applications of this code need not convert such large source JSON files like this - it's very time consuming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e64244-5a17-48dc-9340-d8cde9ae8aa6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import ijson\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Define default values for columns\n",
    "default_values = {\n",
    "    **{col: 0 for col in ['quote_count', 'like_count', 'retweet_count', 'reply_count']},\n",
    "    **{col: 0 for col in ['id', 'conversation_id', 'author_id', 'in_reply_to_user_id']},\n",
    "    **{col: '' for col in [\"text\", \"source\"]},\n",
    "    **{col: pd.NA for col in [\"urls\", \"annotations\", \"media\", \"user_mentions\", \n",
    "                              \"hashtags\", \"cashtags\", \"symbols\", \"geo\", \"location\"]},\n",
    "}\n",
    "\n",
    "# Columns to drop\n",
    "tweet_drop_cols = [\n",
    "    'reply_settings',\n",
    "    'context_annotations',\n",
    "    'withheld',\n",
    "    'possibly_sensitive',\n",
    "    'attachments',\n",
    "    'referenced_tweets',\n",
    "]\n",
    "\n",
    "\n",
    "def process_file(tweetFile):\n",
    "\n",
    "    def app_clr_col(i, t):\n",
    "        append_parq(i, t)\n",
    "        i.clear()\n",
    "                    \n",
    "    try:\n",
    "        ij_buffer_size_kb = 256\n",
    "        ij_bsize = ij_buffer_size_kb * 1000 # kb\n",
    "        chunk_size = 500000\n",
    "        target = f\"{twibot_path}/{tweetFile}\"\n",
    "        t_stem = Path(target).stem\n",
    "        targetOutput = Path(rf\"{generated_data_output}/{t_stem}.parquet\")\n",
    "        iteration_records = []\n",
    "    \n",
    "        if not is_data(t_stem):\n",
    "            debug_print(f\"Input: {target} Output: {targetOutput}\", 5)\n",
    "            with open(target, 'r') as fi:\n",
    "                tlen = sum(1 for line in fi)\n",
    "                fi.seek(0)\n",
    "                parser = ijson.items(fi, 'item', use_float=True, buf_size=ij_bsize)               \n",
    "                \n",
    "                for i, record in enumerate(parser):                                           \n",
    "                    iteration_records.append(record)\n",
    "                    if (i + 1) % chunk_size == 0: \n",
    "                        app_clr_col(iteration_records, targetOutput)\n",
    "    \n",
    "                if iteration_records:\n",
    "                    app_clr_col(iteration_records, targetOutput)\n",
    "                    \n",
    "                del tlen, parser\n",
    "                gc.collect()\n",
    "                    \n",
    "            debug_print(f\"Finished converting file: {targetOutput}\", 5)\n",
    "            \n",
    "    except Exception as e: \n",
    "        debug_print(f\"Error processing file {Path(target)}: {e}\", 2) \n",
    "        print(traceback.format_exception(e))\n",
    "\n",
    "\n",
    "def append_parq(df_conv, target, partition_columns=None):\n",
    "    global default_values, tweet_drop_cols\n",
    "    df_conv = pd.DataFrame(df_conv)\n",
    "\n",
    "    # Ensure all default columns exist\n",
    "    for col, default_value in default_values.items():\n",
    "        if col not in df_conv.columns:\n",
    "            df_conv[col] = default_value\n",
    "    \n",
    "    # Flatten 'public_metrics' and 'entities' columns\n",
    "    flatten_columns = ['public_metrics', 'entities']\n",
    "\n",
    "    for col in flatten_columns:\n",
    "       if col in df_conv.columns:\n",
    "            try:\n",
    "                flattened = pd.json_normalize(\n",
    "                    df_conv[col].dropna().apply(\n",
    "                        lambda x: x if isinstance(x, dict) else {}\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Ensure all flattened columns have non-python object types\n",
    "                for key in flattened.columns:\n",
    "                    if key in df_conv.columns:\n",
    "                        flattened[key] = flattened[key].apply(\n",
    "                            lambda x: json.dumps(x) if isinstance(x, (list, dict)) else x\n",
    "                        )\n",
    "                        \n",
    "                existing_keys = [key for key in flattened.columns if key in df_conv.columns]\n",
    "                df_conv.loc[flattened.index, existing_keys] = flattened[existing_keys]\n",
    "            \n",
    "            except Exception as e:\n",
    "                debug_print(f\"Failed to normalize column {col}: {repr(e)}\", 2)\n",
    "            \n",
    "    # Remove the original flattened columns and additionally, the drop columns specified at the beginning of this cell.\n",
    "    df_conv.drop(columns=flatten_columns + tweet_drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Ensure types on integer columns\n",
    "    int_cols = ['quote_count', 'like_count', 'retweet_count', 'reply_count']\n",
    "    for col in int_cols:\n",
    "        df_conv[col] = df_conv[col].astype('UInt32').fillna(0)\n",
    "    \n",
    "    # Ensure types on Int64 columns. Additionally, for columns containing string user IDs, strip it to the int64 digits so it can be stored more efficiently.\n",
    "    int64_cols = ['id', 'conversation_id', 'author_id', 'in_reply_to_user_id']\n",
    "    for col in int64_cols:\n",
    "        df_conv[col] = df_conv[col].fillna(0).apply(\n",
    "            lambda x: int(str(x).lstrip('ut')) if pd.notnull(x) and isinstance(x, str) and str(x)[0] in \"ut\" \n",
    "            else int(x) if pd.notnull(x) else 0\n",
    "        ).astype('UInt64')\n",
    "    \n",
    "    # Ensure types on string columns\n",
    "    str_cols = [\"text\", \"source\"]\n",
    "    for col in str_cols:\n",
    "        df_conv[col] = df_conv[col].astype('string')\n",
    "    \n",
    "    # Cast object columns into strings (just to make sure...?)\n",
    "    obj_cols = [\"urls\", \"annotations\", \"media\", \"user_mentions\", \"hashtags\", \n",
    "                \"cashtags\", \"symbols\", \"geo\", \"location\"]\n",
    "    for col in obj_cols:\n",
    "        df_conv[col] = df_conv[col].astype(str)\n",
    "\n",
    "    # Cast the created_at column into a pandas supported timestamp\n",
    "    df_conv[\"created_at\"] = pd.to_datetime(df_conv[\"created_at\"]).dt.tz_convert(None)  \n",
    "    \n",
    "    try:\n",
    "        if target.exists():\n",
    "            df_conv.to_parquet(target, engine='fastparquet', append=True)\n",
    "        else:\n",
    "            debug_print(f\"Created new parquet: {target} with dtypes {df_conv.dtypes}\",5)\n",
    "            debug_print(f\"Sample: {df_conv.head(1)}\",5)\n",
    "            df_conv.to_parquet(target, engine='fastparquet', index=False, partition_on=partition_columns)\n",
    "            \n",
    "    except Exception as e:\n",
    "        debug_print(f\"Failed to create or modify: {target} with dtypes {df_conv.dtypes}. Exception: {e}\",2)\n",
    "        debug_print(f\"Sample: {df_conv.head(1)}\",4)\n",
    "        debug_print(f\"Last 2 rows:\\n{df_conv.tail(2)}\", 5)\n",
    "        debug_print(f\"Column null counts:\\n{df_conv.isnull().sum()}\", 4)\n",
    "        raise e\n",
    "        \n",
    "    del df_conv\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# BEGIN CELL CODE\n",
    "tweetNodeFiles = list(filter(lambda fileName: twibot_node_identifier_str in fileName,\n",
    "                             [child.name for child in Path(twibot_path).iterdir()]))\n",
    "debug_print(f\"Found nodes: {tweetNodeFiles}\",3)\n",
    "\n",
    "if threading:\n",
    "    with ThreadPoolExecutor(max_workers=concurrent_max_workers) as executor:\n",
    "        list(tqdm(executor.map(process_file, tweetNodeFiles), total=len(tweetNodeFiles), desc=\"Processing Files\"))\n",
    "else:\n",
    "    for file in tweetNodeFiles:\n",
    "        debug_print(\"Processing: \"+file, 5)\n",
    "        process_file(file)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3fadc-62f6-4416-a41b-85a092583901",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Checkpoint 2: Test tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592301c5-f5ad-4f64-a212-454e2139b38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing tweet dataset nodes\n",
      "Found nodes: ['tweet_4', 'tweet_8', 'tweet_7', 'tweet_3', 'tweet_0', 'tweet_6', 'tweet_1', 'tweet_2', 'tweet_5']\n",
      "Loading existing data from /dataset/twibot22/generated_data/tweet_4.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>urls</th>\n",
       "      <th>annotations</th>\n",
       "      <th>media</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cashtags</th>\n",
       "      <th>symbols</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1178736848744660994</td>\n",
       "      <td>1489704397554790400</td>\n",
       "      <td>2022-02-04 20:56:20</td>\n",
       "      <td>None</td>\n",
       "      <td>1489704397554790400</td>\n",
       "      <td>1382478319883587584</td>\n",
       "      <td>fa</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>@mah_saros73 ðŸ™ƒÚ©Ø§Ø´ Ù…Ù† Ù‡Ù…Ù‡ Ø¨ÙˆØ¯Ù… Ø¨Ø§ Ù‡Ù…Ù‡ Ø¯Ù‡Ø§Ù† Ù‡Ø§ Ù…...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[{\"screen_name\": \"mah_saros73\", \"name\": \"mahsa...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             author_id      conversation_id          created_at   geo  \\\n",
       "0  1178736848744660994  1489704397554790400 2022-02-04 20:56:20  None   \n",
       "\n",
       "                    id  in_reply_to_user_id lang  \\\n",
       "0  1489704397554790400  1382478319883587584   fa   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                                                text  quote_count  ...  \\\n",
       "0  @mah_saros73 ðŸ™ƒÚ©Ø§Ø´ Ù…Ù† Ù‡Ù…Ù‡ Ø¨ÙˆØ¯Ù… Ø¨Ø§ Ù‡Ù…Ù‡ Ø¯Ù‡Ø§Ù† Ù‡Ø§ Ù…...            0  ...   \n",
       "\n",
       "   retweet_count  reply_count  urls annotations media  \\\n",
       "0              0            0    []        <NA>   nan   \n",
       "\n",
       "                                       user_mentions hashtags cashtags  \\\n",
       "0  [{\"screen_name\": \"mah_saros73\", \"name\": \"mahsa...       []     <NA>   \n",
       "\n",
       "  symbols location  \n",
       "0      []     <NA>  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded author list! Types:\n",
      " author_id                      UInt64\n",
      "conversation_id                UInt64\n",
      "created_at             datetime64[ns]\n",
      "geo                            object\n",
      "id                             UInt64\n",
      "in_reply_to_user_id            UInt64\n",
      "lang                           object\n",
      "source                         object\n",
      "text                           object\n",
      "quote_count                    UInt32\n",
      "like_count                     UInt32\n",
      "retweet_count                  UInt32\n",
      "reply_count                    UInt32\n",
      "urls                           object\n",
      "annotations                    object\n",
      "media                          object\n",
      "user_mentions                  object\n",
      "hashtags                       object\n",
      "cashtags                       object\n",
      "symbols                        object\n",
      "location                       object\n",
      "dtype: object \n",
      " ...Releasing resources.\n",
      "Loading existing data from /dataset/twibot22/generated_data/tweet_8.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>urls</th>\n",
       "      <th>annotations</th>\n",
       "      <th>media</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cashtags</th>\n",
       "      <th>symbols</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>730877400662212609</td>\n",
       "      <td>1391411268519616518</td>\n",
       "      <td>2021-05-09 15:14:32</td>\n",
       "      <td>None</td>\n",
       "      <td>1391411268519616518</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>Brand new R. Missing interview with Spain's @m...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"url\": \"https://t.co/zKt3gNRTcp\", \"expanded_...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[{\"screen_name\": \"Muzikalia\", \"name\": \"Muzikal...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            author_id      conversation_id          created_at   geo  \\\n",
       "0  730877400662212609  1391411268519616518 2021-05-09 15:14:32  None   \n",
       "\n",
       "                    id  in_reply_to_user_id lang  \\\n",
       "0  1391411268519616518                    0   en   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "\n",
       "                                                text  quote_count  ...  \\\n",
       "0  Brand new R. Missing interview with Spain's @m...            0  ...   \n",
       "\n",
       "   retweet_count  reply_count  \\\n",
       "0              1            0   \n",
       "\n",
       "                                                urls annotations media  \\\n",
       "0  [{\"url\": \"https://t.co/zKt3gNRTcp\", \"expanded_...        <NA>   nan   \n",
       "\n",
       "                                       user_mentions hashtags cashtags  \\\n",
       "0  [{\"screen_name\": \"Muzikalia\", \"name\": \"Muzikal...       []     <NA>   \n",
       "\n",
       "  symbols location  \n",
       "0      []     <NA>  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded author list! Types:\n",
      " author_id                      UInt64\n",
      "conversation_id                UInt64\n",
      "created_at             datetime64[ns]\n",
      "geo                            object\n",
      "id                             UInt64\n",
      "in_reply_to_user_id            UInt64\n",
      "lang                           object\n",
      "source                         object\n",
      "text                           object\n",
      "quote_count                    UInt32\n",
      "like_count                     UInt32\n",
      "retweet_count                  UInt32\n",
      "reply_count                    UInt32\n",
      "urls                           object\n",
      "annotations                    object\n",
      "media                          object\n",
      "user_mentions                  object\n",
      "hashtags                       object\n",
      "cashtags                       object\n",
      "symbols                        object\n",
      "location                       object\n",
      "dtype: object \n",
      " ...Releasing resources.\n",
      "Loading existing data from /dataset/twibot22/generated_data/tweet_7.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import fastparquet\n",
    "debug_print(\"Previewing tweet dataset nodes\",5)\n",
    "tweetNodeFiles = list(filter(lambda fileName: twibot_node_identifier_str in fileName,\n",
    "                             [child.stem for child in Path(twibot_path).iterdir()]))\n",
    "debug_print(f\"Found nodes: {tweetNodeFiles}\",3)\n",
    "\n",
    "for node in tweetNodeFiles:    \n",
    "    try_load = get_data(node) # Optimally, we have a parquet\n",
    "    if type(try_load) is bool:\n",
    "        print(\"Problem loading node file...\")\n",
    "        raise StopExecution\n",
    "    display(try_load.head(1))\n",
    "    debug_print(f\"Loaded author list! Types:\\n {try_load.dtypes} \\n ...Releasing resources.\",5)\n",
    "    del try_load\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e18599-9de0-4e09-ad89-ce34ae2694cf",
   "metadata": {},
   "source": [
    "### Step 4: Convert relationships 'edge' file into a parquet.\n",
    "\n",
    "We can use the partitioning feature of parquet files to make queries to the converted dataset a lot faster, based on the limited number of categorical types that a relationship can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "734ff278-6f11-4b92-bab7-5e320313118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User relationship edges parquet already present. Skipping generation.\n"
     ]
    }
   ],
   "source": [
    "# Check for present\n",
    "\n",
    "if is_data(\"edges\"):\n",
    "    debug_print(\"User relationship edges parquet already present. Skipping generation.\",5)\n",
    "else:\n",
    "    targetOutput = Path(rf\"{generated_data_output}/edges.parquet\")    \n",
    "    debug_print(\"Reading edges data...\",4)\n",
    "    with open(twibot_edges, 'r') as fi:\n",
    "        udata_edges = pd.read_csv(fi, header=0, names=[\"id1\", \"relationship\", \"id2\"], encoding=\"UTF8\")\n",
    "                                                       \n",
    "    debug_print(display(udata_edges.head(5)),5)    \n",
    "    debug_print(f\"Found {udata_edges.shape[0]} edges in edge file. Converting to parks and recreation.\",4)\n",
    "    save_data('edges', df=udata_edges, partition_on=['relationship'])\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f99510-b09e-4815-a9f9-774306b6eed6",
   "metadata": {},
   "source": [
    "### Step 5: Test edge file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c174d95-f106-4795-8e24-eb909d66a02c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m try_load \u001b[38;5;241m=\u001b[39m get_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m display(try_load\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m try_load\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m25\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "try_load = get_data(\"edges\")\n",
    "display(try_load.loc['u' in try_load.index.values].head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be270c-5e35-4f3a-a95e-49bc713dfc82",
   "metadata": {},
   "source": [
    "# End Setup Process A\n",
    "\n",
    "By now you should have a set of parquet files representing the much larger json files that all of the tweets were stored in. Additionally, you should have a users.parquet that contains a limited number of the original columns contained in the user information, merged with the label for the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
