{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f1cfb3-7b76-4d0b-b10c-17c4c79403ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "import pandas as pd\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from fastparquet import ParquetFile\n",
    "\n",
    "verbosity = 5\n",
    "\n",
    "twibot_path = r\"/dataset/twibot22\"\n",
    "twibot_user = r\"/dataset/twibot22/user.json\"\n",
    "twibot_label = r\"/dataset/twibot22/label.csv\"\n",
    "\n",
    "# Some tasks might be multithreadable. Set the max number of workers here.\n",
    "concurrent_max_workers = 2\n",
    "\n",
    "# Files in the path specified by twibot_path, that begin with %twibot_node_identifier_str%, will be assumed as node files and converted if needed.\n",
    "twibot_node_identifier_str = \"tweet_\" \n",
    "\n",
    "generated_data_output = r\"/dataset/twibot22/generated_data\" # output is saved in this directory\n",
    "ls_userdata_output = rf\"{generated_data_output}/userdata.jsonl\" # the desired filename of bot detail output\n",
    "\n",
    "sample_set_size_per_label = 50000 # per label, sample this many users\n",
    "sample_set_stratification = True # If at any point during selection our set becomes unbalanced, should we stratify?\n",
    "sample_set_strategy = (1,0) # (1over/0under, 1major/0minor)\n",
    "sample_randomization_before_selection = True # Should we shuffle the samples after selecting and before constraining the set?\n",
    "sample_randomization_after_constraints = True # Should we shuffle the samples after selecting, constraints, and stratification of the set?\n",
    "\n",
    "graph_sampling_depth = 3\n",
    "\n",
    "def debug_print(m, level=5, r=None):\n",
    "    if level <= verbosity:\n",
    "        print(m)\n",
    "        if r:\n",
    "            raise r\n",
    "\n",
    "def is_data(name, _dir=generated_data_output):\n",
    "    file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "    return os.path.exists(file_path)\n",
    "    \n",
    "    \n",
    "def get_data(name, _dir=generated_data_output,pqargs={},**kwargs):\n",
    "    if is_data(name, _dir):\n",
    "        file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "        print(f\"Loading existing data from {file_path}\")\n",
    "        pf = ParquetFile(file_path, **pqargs)\n",
    "        return pf.to_pandas(**kwargs)\n",
    "    return False\n",
    "        \n",
    "def save_data(name, df, _dir=generated_data_output, **kwargs):\n",
    "    file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "    debug_print(f\"Saving data to {file_path}\", 3)\n",
    "    os.makedirs(_dir, exist_ok=True)\n",
    "    fastparquet.write(file_path, df, **kwargs)\n",
    "    #df.to_parquet(file_path, **kwargs)\n",
    "    return df        \n",
    "\n",
    "def _shuffle(df):\n",
    "    return df.sample(frac = 1)\n",
    "    \n",
    "shuffle_method = _shuffle\n",
    "\n",
    "# To quietly stop cell execution\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "import json\n",
    "def get_post_counts():\n",
    "    tweetNodeFilesParquet = list(filter(lambda fileName: twibot_node_identifier_str in fileName, \n",
    "                                        [child.name for child in Path(generated_data_output).iterdir()]))\n",
    "    post_count_dict = defaultdict(int)\n",
    "    debug_print(f\"Called: get_post_counts\", 5)\n",
    "    for targetFile in tweetNodeFilesParquet:\n",
    "        targetInput = Path(f\"{generated_data_output}/{targetFile}\")\n",
    "        try:\n",
    "            debug_print(\"Looking in \" + targetInput.__str__(), 5)\n",
    "            pf = ParquetFile(targetInput)\n",
    "            df = pf.to_pandas(columns=['author_id'])\n",
    "            for uid in df['author_id']:\n",
    "                post_count_dict[uid] = post_count_dict[uid] + 1\n",
    "            del pf, df\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            debug_print(f\"Failed to load node parquet: {e}\", 5)\n",
    "            raise RuntimeError(\"Error processing Parquet files.\")\n",
    "    debug_print(f\"Completed: get_post_counts\", 5)\n",
    "    return post_count_dict\n",
    "    \n",
    "NODE_FILE_LIST = list(filter(lambda fileName: twibot_node_identifier_str in fileName, \n",
    "                                        [child.name for child in Path(generated_data_output).iterdir()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b719f-c57e-4fb4-92f1-39ad552695ee",
   "metadata": {},
   "source": [
    "## Lockstep, the full recipe, Part 1: Pre-processing Stage 2\n",
    "### Step 5: Prepare additional feature columns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5176df9e-3058-4393-b193-bdde4c360430",
   "metadata": {},
   "source": [
    "Reserve types, names, so that pandas does not try to do it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708e8f81-735f-4096-b647-e1fab191ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def ensure_required_columns(target_dataframe):\n",
    "    new_columns = {\n",
    "        \n",
    "    # Phase 1\n",
    "    'following_followers_ratio' : 'float', \n",
    "    'tweet_followers_ratio' : 'float', \n",
    "    'tweet_following_ratio' : 'float', \n",
    "    'sampled_post_count': 'int32',\n",
    "\n",
    "    'profile_desc_len': 'uint32', \n",
    "    'profile_username_len': 'int16', \n",
    "    'profile_has_location': 'boolean', \n",
    "\n",
    "    # TODO: Significance debatable\n",
    "    'profile_desc_mentions_count': 'int16', \n",
    "    'profile_desc_hashtag_count' : 'int16', \n",
    "    'profile_desc_url_count' : 'int16', \n",
    "\n",
    "    # Phase 2\n",
    "    'tweet_has_media_ratio': 'float', \n",
    "    'tweet_has_geo_ratio': 'float', \n",
    "\n",
    "\n",
    "    # Phase 3\n",
    "    'total_rt': 'uint32', \n",
    "    'total_likes': 'uint32', \n",
    "    'total_quotes': 'uint32',\n",
    "    'average_rt': 'float', \n",
    "    'average_likes': 'float', \n",
    "    'average_quotes': 'float', \n",
    "\n",
    "    'likes_chi': 'float', \n",
    "    'rts_chi': 'float', \n",
    "\n",
    "    'likes_zero_ratio': 'float', \n",
    "    'rts_zero_ratio' : 'float', \n",
    "\n",
    "\n",
    "     # Phase 4\n",
    "    'entropy_between_post_times': 'float', \n",
    "    'entropy_between_post_hours': 'float', \n",
    "    'entropy_between_post_days': 'float', \n",
    "    'entropy_between_post_weekdays': 'float', \n",
    "\n",
    "\n",
    "\n",
    "     # Phase 5\n",
    "    'tweet_has_hashtags_ratio': 'float',\n",
    "    'tweet_has_urls_ratio': 'float', \n",
    "    'tweet_urls_total': 'uint32', \n",
    "    'tweet_hashtags_total': 'uint32', \n",
    "    'avg_hashtags_in_tweet': 'float',\n",
    "    'avg_urls_in_tweet': 'float', \n",
    "    'tweet_urls_top_x': 'object', \n",
    "    'tweet_hashtags_top_x': 'object', \n",
    "\n",
    "\n",
    "    # What happened to these? A.K.A TODO, with a possibility of won't. They just aren't strong enough factors for the computation required.\n",
    "    'tweet_has_hashtag_weekday_entropy': 'float', \n",
    "    'tweet_has_hashtag_hour_entropy': 'float', \n",
    "    'tweet_has_url_weekday_entropy': 'float', \n",
    "    'tweet_has_url_hour_entropy': 'float', \n",
    "    }\n",
    "    \n",
    "    for column, dtype in new_columns.items(): \n",
    "        if column not in target_dataframe.columns:\n",
    "            target_dataframe[column] = pd.Series(dtype=dtype) \n",
    "        else:\n",
    "            target_dataframe[column] = pd.astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3526fab-d4fc-44db-9745-a6dcb55377b0",
   "metadata": {},
   "source": [
    "## Step 6: Populate additional feature columns\n",
    "### Warning! This step takes -time-. Up to ten minutes+. \n",
    "\n",
    "There's only so much you can do on limited hardware when you are already using concurrency to your benefit and vectorized methods as much as possible. Ten minutes is an okay ask to be able to pre-process a million users and seventy million posts on a variety of stats, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f89084-6b13-4ea5-b715-88327ce0d9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from users parquet at /dataset/twibot22/generated_data\n",
      "Loading existing data from /dataset/twibot22/generated_data/users.parquet\n",
      "Loaded users parquet.\n",
      "Shape: (1000000, 17)\n",
      "Types: created_at              datetime64[ns, UTC]\n",
      "description                          object\n",
      "location                             object\n",
      "name                                 object\n",
      "url                                  object\n",
      "username                             object\n",
      "label                                object\n",
      "followers_count                       int64\n",
      "following_count                       int64\n",
      "tweet_count                           int64\n",
      "listed_count                          int64\n",
      "url.urls                             object\n",
      "description.urls                     object\n",
      "description.mentions                 object\n",
      "description.hashtags                 object\n",
      "description.cashtags                 object\n",
      "posts                                object\n",
      "dtype: object\n",
      "Index: UInt64\n",
      "Total users: 1000000. Pre-processing.\n",
      "Running phase_1\n",
      "Called: get_post_counts\n",
      "Looking in /dataset/twibot22/generated_data/tweet_7.parquet\n",
      "Looking in /dataset/twibot22/generated_data/tweet_8.parquet\n",
      "Looking in /dataset/twibot22/generated_data/tweet_4.parquet\n",
      "Looking in /dataset/twibot22/generated_data/tweet_1.parquet\n",
      "Looking in /dataset/twibot22/generated_data/tweet_6.parquet\n",
      "Looking in /dataset/twibot22/generated_data/tweet_2.parquet\n",
      "Looking in /dataset/twibot22/generated_data/tweet_5.parquet\n",
      "Looking in /dataset/twibot22/generated_data/tweet_0.parquet\n",
      "Looking in /dataset/twibot22/generated_data/tweet_3.parquet\n",
      "Completed: get_post_counts\n",
      "Appended new features to dataframe. New shape: (1000000, 55)\n",
      "Running phase_2\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_7.parquet...\n",
      "Running phase_3\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_8.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_4.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_1.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_6.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_2.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_5.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_0.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_3.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_7.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_8.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_4.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_1.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_6.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_2.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_5.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_0.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_3.parquet...\n",
      "Running phase_4append_stats_phase_2e completed successfully.\n",
      "\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_7.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_8.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_4.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_1.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_6.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_2.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_5.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_0.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_3.parquet...\n",
      "Running phase_5\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_7.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_8.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_4.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_1.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_6.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_2.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_5.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_0.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_3.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557223/763313118.py:203: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<FloatingArray>\n",
      "[  60559.0, 2732441.0,  149112.0,  365014.0,      <NA>,      <NA>,  277517.0,\n",
      " 3987671.0,  323205.0,      49.0,\n",
      " ...\n",
      "      47.0,  168394.0,       2.0,  106157.0,       1.0,      14.0,     248.0,\n",
      "     579.0,   28504.0,       0.0]\n",
      "Length: 1000000, dtype: Float64' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  target_dataframe.update(agg_results)\n",
      "/tmp/ipykernel_1557223/763313118.py:203: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<FloatingArray>\n",
      "[ 18512.0,    239.0,     25.0,   1469.0,     <NA>,     <NA>, 302629.0,\n",
      "    657.0,   2657.0,    179.0,\n",
      " ...\n",
      "     55.0,      1.0,     67.0,   1943.0,      3.0,     54.0,    456.0,\n",
      "   1585.0,      4.0,      2.0]\n",
      "Length: 1000000, dtype: Float64' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  target_dataframe.update(agg_results)\n",
      "/tmp/ipykernel_1557223/763313118.py:203: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<FloatingArray>\n",
      "[263.0,   3.0,   0.0,  13.0,  <NA>,  <NA>, 865.0,   6.0, 106.0,   4.0,\n",
      " ...\n",
      "   0.0,   0.0,   0.0,   2.0,   0.0,   0.0,   1.0,   0.0,   0.0,   0.0]\n",
      "Length: 1000000, dtype: Float64' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  target_dataframe.update(agg_results)\n",
      "/tmp/ipykernel_1557223/763313118.py:203: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<FloatingArray>\n",
      "[  53.121929824561406,   2735.1761761761763,   2366.8571428571427,\n",
      "   1414.7829457364342,                 <NA>,                 <NA>,\n",
      "    278.9115577889447,    5105.852752880922,   268.21991701244815,\n",
      "   1.8148148148148149,\n",
      " ...\n",
      "    1.119047619047619,    3177.245283018868, 0.046511627906976744,\n",
      "   1965.8703703703704, 0.023255813953488372,   0.3333333333333333,\n",
      "    5.767441860465116,   13.785714285714286,    619.6521739130435,\n",
      "                  0.0]\n",
      "Length: 1000000, dtype: Float64' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  target_dataframe.update(agg_results)\n",
      "/tmp/ipykernel_1557223/763313118.py:203: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<FloatingArray>\n",
      "[   16.23859649122807,  0.23923923923923923,   0.3968253968253968,\n",
      "   5.6937984496124034,                 <NA>,                 <NA>,\n",
      "    304.1497487437186,   0.8412291933418694,   2.2049792531120334,\n",
      "     6.62962962962963,\n",
      " ...\n",
      "   1.3095238095238095, 0.018867924528301886,    1.558139534883721,\n",
      "    35.98148148148148,  0.06976744186046512,   1.2857142857142858,\n",
      "   10.604651162790697,    37.73809523809524,  0.08695652173913043,\n",
      "  0.04878048780487805]\n",
      "Length: 1000000, dtype: Float64' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  target_dataframe.update(agg_results)\n",
      "/tmp/ipykernel_1557223/763313118.py:203: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<FloatingArray>\n",
      "[   0.2307017543859649,  0.003003003003003003,                   0.0,\n",
      "  0.050387596899224806,                  <NA>,                  <NA>,\n",
      "    0.8693467336683417, 0.0076824583866837385,   0.08796680497925312,\n",
      "   0.14814814814814814,\n",
      " ...\n",
      "                   0.0,                   0.0,                   0.0,\n",
      "  0.037037037037037035,                   0.0,                   0.0,\n",
      "  0.023255813953488372,                   0.0,                   0.0,\n",
      "                   0.0]\n",
      "Length: 1000000, dtype: Float64' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  target_dataframe.update(agg_results)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append_stats_phase_3 completed successfully.\n",
      "append_stats_phase_4 completed successfully.\n",
      "append_stats_phase_5 completed successfully.\n",
      "Saving all aggregated information for reload...\n",
      "Finished. Reload kernel and cell 1 to clear memory, then proceed to the next setup steps.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from pathlib import Path\n",
    "from fastparquet import ParquetFile\n",
    "import json\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED\n",
    "from scipy.stats import entropy\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "import math\n",
    "from scipy.stats import chisquare\n",
    "from decimal import Decimal\n",
    "import re\n",
    "import shutil\n",
    "import datetime as dt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# Create a lock to use for updating the user details dataframe, to prevent race conditions.\n",
    "update_lock = threading.Lock()\n",
    "\n",
    "# Create a lock to use for reading data from parquet into memory\n",
    "read_lock = threading.Lock()\n",
    "\n",
    "chunk_size = 100000\n",
    "user_detail_data = {}\n",
    "\n",
    "# Get all of the initial data that we have\n",
    "debug_print(f\"Loading data from users parquet at {generated_data_output}\",1)\n",
    "try:\n",
    "    user_detail_data = get_data(\"users\")\n",
    "    user_detail_data['id'] = user_detail_data['id'].astype('UInt64')\n",
    "    user_detail_data.set_index('id',inplace=True)\n",
    "    user_detail_data['posts'] = [[] for _ in range(len(user_detail_data))]\n",
    "except Exception as e:\n",
    "    debug_print(f\"Failed to load user parquet. {e}\",5)  \n",
    "    raise StopExecution\n",
    "\n",
    "debug_print(\"Loaded users parquet.\",4)\n",
    "debug_print(f\"Shape: {user_detail_data.shape}\", 5)\n",
    "debug_print(f\"Types: {user_detail_data.dtypes}\", 5)\n",
    "debug_print(f\"Index: {user_detail_data.index.dtype}\", 5)\n",
    "\n",
    "# Unused. At one point, was used to calculate the entropy of intervals between posts out of a set of posts. But then, I realized it was worthless unless you had -all the posts in sequence-. We don't.\n",
    "def calculate_time_interval_entropy(time_intervals, num_bins='auto'):\n",
    "    if len(time_intervals) <= 1:\n",
    "        return 0  # Not enough data to calculate meaningful entropy\n",
    "    # Create a histogram of time intervals to obtain frequency distribution\n",
    "    hist, bin_edges = np.histogram(time_intervals, bins=num_bins, density=True)\n",
    "    hist = hist[hist > 0]\n",
    "    probabilities = hist / hist.sum()\n",
    "    return entropy(probabilities)\n",
    "\n",
    "def get_post_chunks(cols = '*', index=\"author_id\", pqargs={}, pdkwargs={}, margs={}):\n",
    "    # Result: Dataframe, index with one or more features.\n",
    "    # Index: from users\n",
    "    global NODE_FILE_LIST, read_lock\n",
    "    result_builder = None\n",
    "    with read_lock:\n",
    "        for targetFile in NODE_FILE_LIST:\n",
    "            targetInput = Path(f\"{generated_data_output}/{targetFile}\")\n",
    "            debug_print(f\"Extracting from {targetInput.__str__()}...\", 5)\n",
    "            \n",
    "            if cols != '*':\n",
    "                # Set the columns to pull from the parquet, either through pqargs directly or here, through cols\n",
    "                pdkwargs['columns'] = cols\n",
    "                \n",
    "            pdkwargs['index'] = index       \n",
    "            try:\n",
    "                pfinput = ParquetFile(targetInput, **pqargs)    \n",
    "                process_group = pfinput.to_pandas(**pdkwargs)  \n",
    "                result_builder = pd.concat([result_builder, process_group])         \n",
    "            except Exception as e:\n",
    "                debug_print(f\"Failed to load node parquet: {e}\", 5)\n",
    "                raise RuntimeError(\"Error processing Parquet files.\")\n",
    "    return result_builder\n",
    "\n",
    "def append_stats_phase_1(target_dataframe):\n",
    "    global update_lock\n",
    "    print(\"Running phase_1\")\n",
    "    with update_lock:\n",
    "        post_counts = get_post_counts()\n",
    "        target_dataframe['sampled_post_count'] = target_dataframe.index.map(lambda x: post_counts.get(int(str(x).strip('ut')),0))\n",
    "\n",
    "        target_dataframe['profile_desc_len'] = target_dataframe['description'].apply(len).fillna(0)\n",
    "        target_dataframe['profile_username_len'] = target_dataframe['username'].apply(len).fillna(0)\n",
    "        target_dataframe['profile_has_location'] = target_dataframe['location'].apply(lambda v: len(v)>1 if v is not None else False)\n",
    "        \n",
    "        target_dataframe['following_followers_ratio'] = target_dataframe['following_count'].div(target_dataframe['followers_count'], fill_value=0).fillna(0)\n",
    "        target_dataframe['tweet_followers_ratio'] = target_dataframe['tweet_count'].div(target_dataframe['followers_count'], fill_value=0).fillna(0)\n",
    "        target_dataframe['tweet_following_ratio'] = target_dataframe['tweet_count'].div(target_dataframe['following_count'], fill_value=0).fillna(0)\n",
    "        #['following_followers_ratio', 'tweet_followers_ratio', 'tweet_following_ratio']\n",
    "        #todo:\n",
    "        #target_dataframe['profile_desc_mentions_count'] = target_dataframe['tweet_count']\n",
    "        #target_dataframe['profile_desc_hashtag_count']\n",
    "        #target_dataframe['profile_desc_url_count']\n",
    "    debug_print(f\"Appended new features to dataframe. New shape: {target_dataframe.shape}\", 5)\n",
    "    del post_counts\n",
    "\n",
    "def append_stats_phase_2e(target_dataframe):\n",
    "    global update_lock\n",
    "    print(\"Running phase_2\")\n",
    "    \n",
    "    def filt(x):\n",
    "        \"\"\"Filter function to check valid entries.\"\"\"\n",
    "        return not (x is None or pd.isna(x) or x in ['None', 'nan', '<NA>', []])\n",
    "    \n",
    "    columns_to_process = ['geo', 'media']\n",
    "    post_chunks_ = get_post_chunks(cols=['geo', 'media'])\n",
    "    \n",
    "    aggs = {\n",
    "        f'tweet_has_{col}_ratio': pd.NamedAgg(\n",
    "            column=col, \n",
    "            aggfunc=lambda s: s.apply(filt).mean()\n",
    "        ) \n",
    "        for col in columns_to_process\n",
    "    }\n",
    "    \n",
    "    grpFrame = post_chunks_.groupby(\"author_id\", sort=False)\n",
    "    countFrame = grpFrame.agg(**aggs)\n",
    "   \n",
    "    with update_lock:       \n",
    "        target_dataframe.update(countFrame)\n",
    "\n",
    "def append_stats_phase_3(target_dataframe):\n",
    "    print(\"Running phase_3\")\n",
    "    def extract_leading_digits(series):\n",
    "        series = series.dropna()\n",
    "        series = series[series > 0] \n",
    "        if series.empty:\n",
    "            return None\n",
    "        leading_digits = (series.astype(int) // 10 ** (np.floor(np.log10(series)).astype(int))).astype(int)\n",
    "        return leading_digits\n",
    "    \n",
    "    def calculate_chi_for_group(series, benford_probs):\n",
    "        leading_digits = extract_leading_digits(series)\n",
    "        if leading_digits is None:\n",
    "            return np.nan\n",
    "        observed_counts = np.bincount(leading_digits, minlength=10)[1:10]  # Skip 0 (invalid leading digit)\n",
    "        total_observed = observed_counts.sum()\n",
    "        if total_observed == 0:\n",
    "            return np.nan\n",
    "        chi_squared = np.sum((observed_counts - total_observed * benford_probs) ** 2 / (total_observed * benford_probs))\n",
    "        return chi_squared    \n",
    "    \n",
    "    \n",
    "    def calculate_chi_vectorized(series, benford_probs):\n",
    "        \"\"\"\n",
    "        Vectorized calculation of the chi-squared distance for a column.\n",
    "        \"\"\"\n",
    "        series = series.dropna()\n",
    "        series = series[series > 0]\n",
    "\n",
    "        if series.empty:\n",
    "            return np.nan\n",
    "\n",
    "        first_digits = (series.astype(int) // 10 ** (np.floor(np.log10(series)).astype(int))).astype(int)\n",
    "        observed_counts = np.bincount(first_digits, minlength=10)[1:10]\n",
    "        total_observed = observed_counts.sum()\n",
    "        if total_observed == 0:\n",
    "            return np.nan\n",
    "\n",
    "        return np.sum((observed_counts - total_observed * benford_probs) ** 2 / (total_observed * benford_probs))\n",
    "\n",
    "    def chunk_3_optimized(target_dataframe, columns):\n",
    "        global update_lock\n",
    "        post_chunks_ = get_post_chunks(cols=columns if isinstance(columns, list) else [columns])\n",
    "        aggs = {\n",
    "            'total_rt': ('retweet_count', 'sum'),\n",
    "            'total_likes': ('like_count', 'sum'),\n",
    "            'total_quotes': ('quote_count', 'sum'),\n",
    "            \n",
    "            'average_rt': ('retweet_count', 'mean'),\n",
    "            'average_likes': ('like_count', 'mean'),\n",
    "            'average_quotes': ('quote_count', 'mean'),\n",
    "        }\n",
    "\n",
    "        grouped = post_chunks_.groupby('author_id', sort=False)\n",
    "        agg_results = grouped.agg(**aggs)\n",
    "        \n",
    "        # Compute chi-squared for Benford's Law\n",
    "        benford_probs = np.log10(1 + 1 / np.arange(1, 10))\n",
    "        \n",
    "        def calculate_chi(grouped_data, column_name):\n",
    "            results = {}\n",
    "            for author_id, group in grouped_data:\n",
    "                chi_value = calculate_chi_for_group(group[column_name], benford_probs)\n",
    "                results[author_id] = chi_value\n",
    "            return pd.Series(results)\n",
    "        \n",
    "        likes_chi = calculate_chi(grouped, 'like_count')\n",
    "        rts_chi = calculate_chi(grouped, 'retweet_count')\n",
    "\n",
    "        agg_results['likes_chi'] = likes_chi\n",
    "        agg_results['rts_chi'] = rts_chi\n",
    "        \n",
    "        with update_lock:\n",
    "            target_dataframe.update(agg_results)\n",
    "        \n",
    "    chunk_3_optimized(target_dataframe, ['retweet_count', 'like_count', 'quote_count'])\n",
    "\n",
    "\n",
    "def append_stats_phase_4(target_dataframe):\n",
    "    print(\"Running phase_4\")\n",
    "\n",
    "    def calculate_entropy_from_probs(probabilities):\n",
    "        \"\"\"Calculate entropy given pre-computed probabilities.\"\"\"\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-9))  # Avoid log2(0)\n",
    "\n",
    "    def chunk_4_(target_dataframe, column):\n",
    "        # Extract post chunks\n",
    "        global update_lock\n",
    "        post_chunks_ = get_post_chunks(cols=[column])\n",
    "        post_chunks_['hour'] = post_chunks_[column].dt.hour\n",
    "        post_chunks_['day'] = post_chunks_[column].dt.day\n",
    "        post_chunks_['weekday'] = post_chunks_[column].dt.weekday\n",
    "\n",
    "        # Pre-compute value counts and normalize for each group\n",
    "        grouped = post_chunks_.groupby('author_id', sort=False)\n",
    "\n",
    "        # Normalize probabilities for each column\n",
    "        hour_probs = grouped['hour'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "        day_probs = grouped['day'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "        weekday_probs = grouped['weekday'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "\n",
    "        # Calculate entropy for each group\n",
    "        entropy_hour = -np.sum(hour_probs * np.log2(hour_probs + 1e-9), axis=1)\n",
    "        entropy_day = -np.sum(day_probs * np.log2(day_probs + 1e-9), axis=1)\n",
    "        entropy_weekday = -np.sum(weekday_probs * np.log2(weekday_probs + 1e-9), axis=1)\n",
    "\n",
    "        aggregated = pd.DataFrame({\n",
    "            'entropy_between_post_hours': entropy_hour,\n",
    "            'entropy_between_post_days': entropy_day,\n",
    "            'entropy_between_post_weekdays': entropy_weekday\n",
    "        })\n",
    "\n",
    "        # Update the target_dataframe within a locked context\n",
    "        with update_lock:\n",
    "            target_dataframe.update(aggregated)\n",
    "\n",
    "    chunk_4_(target_dataframe, 'created_at')\n",
    "\n",
    "def append_stats_phase_5(target_dataframe):\n",
    "\n",
    "    print(\"Running phase_5\")\n",
    "    def get_bag(series):\n",
    "        \"\"\"\n",
    "        Extracts domain names from the 'expanded_url' or retrieves the 'tag' properties in the series.\n",
    "        Each entry in the series is a list of dictionaries.\n",
    "        \"\"\"\n",
    "        bag = []\n",
    "        for item in series.dropna():  # Skip NaN entries\n",
    "            if item not in ['<NA>', 'nan', '[]']:  # Ignore invalid entries\n",
    "                try:\n",
    "                    item = json.loads(item) \n",
    "                    for subitem in item:\n",
    "                        if isinstance(subitem, dict):\n",
    "                            if 'expanded_url' in subitem:\n",
    "                                url = subitem['expanded_url']\n",
    "                                domain = url.split('/')[2] if '//' in url else url\n",
    "                                bag.append(domain)\n",
    "                            elif 'tag' in subitem:\n",
    "                                bag.append(subitem['tag'])\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    continue\n",
    "        return bag\n",
    "\n",
    "    def get_top_ten(series):\n",
    "        \"\"\"\n",
    "        Counts occurrences of each URL or tag from get_bag() output\n",
    "        and returns the top ten as a dictionary with counts.\n",
    "        \"\"\"\n",
    "        bag = get_bag(series)\n",
    "        return dict(Counter(bag).most_common(10))\n",
    "        \n",
    "    def filt_to_notna_mask(x):\n",
    "        if x is None or pd.isna(x):\n",
    "            return False    \n",
    "        if x in ['<NA>','[]']:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def chunk_5_(target_dataframe, columns):\n",
    "        \"\"\"\n",
    "            Process a group of columns: urls, hashtags\n",
    "        \"\"\"\n",
    "        post_chunks_ = get_post_chunks(cols=columns)  # Retrieve relevant chunks\n",
    "\n",
    "        # Define aggregations\n",
    "        aggs = {}\n",
    "        for column in columns:\n",
    "            # tweet_has_x_ratio: number of tweets in which there is at least 1 x as a ratio of all\n",
    "            # avg_x_in_tweet: average number of x per tweet\n",
    "            # tweet_x_total: total x in all tweets\n",
    "            # tweet_x_top_x: the top 10, in frequency of occurence, x among all tweets\n",
    "            aggs.update({\n",
    "                f'tweet_has_{column}_ratio': pd.NamedAgg(column=column, aggfunc=lambda x: x.apply(filt_to_notna_mask).mean()),\n",
    "                f'avg_{column}_in_tweet': pd.NamedAgg(column=column, aggfunc=lambda x: x.str.len().sum() / x.size),\n",
    "                f'tweet_{column}_total': pd.NamedAgg(column=column, aggfunc=lambda x: x.str.len().sum()),\n",
    "                f'tweet_{column}_top_x': pd.NamedAgg(column=column, aggfunc=lambda x: get_top_ten(x)) # Start here\n",
    "            })\n",
    "\n",
    "        # Group and aggregate with our methods\n",
    "        grpFrame = post_chunks_.groupby(\"author_id\", sort=False)\n",
    "        countFrame_update = grpFrame.agg(**aggs)\n",
    "\n",
    "        with update_lock:\n",
    "            # update the target dataframe in place\n",
    "            for column in columns:\n",
    "                target_dataframe.update(countFrame_update)\n",
    "        \n",
    "    # Process each column\n",
    "    chunk_5_(target_dataframe, ['urls', 'hashtags'])\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "user_list_length = user_detail_data.shape[0]\n",
    "debug_print(f\"Total users: {user_detail_data.shape[0]}. Pre-processing.\", 3)\n",
    "ensure_required_columns(user_detail_data)\n",
    "append_stats_phase_1(user_detail_data)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class ThreadedStatsProcessor:\n",
    "    def __init__(self, user_detail_data):\n",
    "        self.user_detail_data = user_detail_data\n",
    "\n",
    "    def run_all_phases(self):\n",
    "        methods = [\n",
    "            append_stats_phase_2e,\n",
    "            append_stats_phase_3,\n",
    "            append_stats_phase_4,\n",
    "            append_stats_phase_5\n",
    "        ]\n",
    "\n",
    "        max_concurrent_threads = concurrent_max_workers # Limit the number of threads\n",
    "        with ThreadPoolExecutor(max_workers=max_concurrent_threads) as executor:\n",
    "            # Submit tasks to the executor\n",
    "            futures = {executor.submit(method, self.user_detail_data): method.__name__ for method in methods}\n",
    "\n",
    "            for future in futures:\n",
    "                method_name = futures[future]\n",
    "                try:\n",
    "                    future.result() \n",
    "                    print(f\"{method_name} completed successfully.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {method_name}: {e} {e.__traceback__} {e.__cause__}\")\n",
    "                    raise e\n",
    "                    \n",
    "processor = ThreadedStatsProcessor(user_detail_data)\n",
    "processor.run_all_phases()\n",
    "\n",
    "def storeData(db):\n",
    "    dbfile = open('temp', 'ab')\n",
    "    pickle.dump(db, dbfile)                    \n",
    "    dbfile.close()\n",
    "    \n",
    "print(\"Saving all aggregated information for reload...\")\n",
    "storeData(processor.user_detail_data)\n",
    "print(\"Finished. Reload kernel and cell 1 to clear memory, then proceed to the next setup steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd885b2-7200-4c05-b31c-160d2605702b",
   "metadata": {},
   "source": [
    "# Step 7: Reload kernel, load pickled populated data, and then save data in it's final parquet format.\n",
    "At this point, there is a ton of memory wasted in Jupyter. Perhaps you might be able to run this cell in sequence, but you'll get better and faster results from restarting the kernel, loading the constants/commons in cell 1, and then skipping directly to the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a2e3c1-c290-49a7-b9cb-00b4e65de313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   created_at  \\\n",
      "id                                              \n",
      "1217628182611927040 2020-01-16 02:02:55+00:00   \n",
      "\n",
      "                                                           description  \\\n",
      "id                                                                       \n",
      "1217628182611927040  Theoretical Computer Scientist. See also https...   \n",
      "\n",
      "                          location        name                      url  \\\n",
      "id                                                                        \n",
      "1217628182611927040  Cambridge, MA  Boaz Barak  https://t.co/BoMip9FF17   \n",
      "\n",
      "                         username  label  followers_count  following_count  \\\n",
      "id                                                                           \n",
      "1217628182611927040  boazbaraktcs  human             7316              215   \n",
      "\n",
      "                     tweet_count  ...  tweet_urls_total tweet_hashtags_total  \\\n",
      "id                                ...                                          \n",
      "1217628182611927040         3098  ...           39940.0              27598.0   \n",
      "\n",
      "                    avg_hashtags_in_tweet avg_urls_in_tweet  \\\n",
      "id                                                            \n",
      "1217628182611927040             24.208772         35.035088   \n",
      "\n",
      "                                                      tweet_urls_top_x  \\\n",
      "id                                                                       \n",
      "1217628182611927040  {'twitter.com': 110, 'horoscoponegro.com': 29,...   \n",
      "\n",
      "                                                  tweet_hashtags_top_x  \\\n",
      "id                                                                       \n",
      "1217628182611927040  {'Aries': 323, 'ARIES': 286, 'aries': 3, 'Part...   \n",
      "\n",
      "                    tweet_has_hashtag_weekday_entropy  \\\n",
      "id                                                      \n",
      "1217628182611927040                               NaN   \n",
      "\n",
      "                     tweet_has_hashtag_hour_entropy  \\\n",
      "id                                                    \n",
      "1217628182611927040                             NaN   \n",
      "\n",
      "                     tweet_has_url_weekday_entropy  tweet_has_url_hour_entropy  \n",
      "id                                                                              \n",
      "1217628182611927040                            NaN                         NaN  \n",
      "\n",
      "[1 rows x 55 columns]\n",
      "Saving data to /dataset/twibot22/generated_data/assembled_user_details.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>username</th>\n",
       "      <th>label</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet_urls_total</th>\n",
       "      <th>tweet_hashtags_total</th>\n",
       "      <th>avg_hashtags_in_tweet</th>\n",
       "      <th>avg_urls_in_tweet</th>\n",
       "      <th>tweet_urls_top_x</th>\n",
       "      <th>tweet_hashtags_top_x</th>\n",
       "      <th>tweet_has_hashtag_weekday_entropy</th>\n",
       "      <th>tweet_has_hashtag_hour_entropy</th>\n",
       "      <th>tweet_has_url_weekday_entropy</th>\n",
       "      <th>tweet_has_url_hour_entropy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1217628182611927040</th>\n",
       "      <td>2020-01-16 02:02:55+00:00</td>\n",
       "      <td>Theoretical Computer Scientist. See also https...</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>Boaz Barak</td>\n",
       "      <td>https://t.co/BoMip9FF17</td>\n",
       "      <td>boazbaraktcs</td>\n",
       "      <td>human</td>\n",
       "      <td>7316</td>\n",
       "      <td>215</td>\n",
       "      <td>3098</td>\n",
       "      <td>...</td>\n",
       "      <td>39940.0</td>\n",
       "      <td>27598.0</td>\n",
       "      <td>24.208772</td>\n",
       "      <td>35.035088</td>\n",
       "      <td>{'twitter.com': 110, 'horoscoponegro.com': 29,...</td>\n",
       "      <td>{'Aries': 323, 'ARIES': 286, 'aries': 3, 'Part...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664730894</th>\n",
       "      <td>2014-07-02 17:56:46+00:00</td>\n",
       "      <td>creative _</td>\n",
       "      <td>üéà</td>\n",
       "      <td>olawale üí®</td>\n",
       "      <td></td>\n",
       "      <td>wale_io</td>\n",
       "      <td>human</td>\n",
       "      <td>123</td>\n",
       "      <td>1090</td>\n",
       "      <td>1823</td>\n",
       "      <td>...</td>\n",
       "      <td>5154.0</td>\n",
       "      <td>4005.0</td>\n",
       "      <td>4.009009</td>\n",
       "      <td>5.159159</td>\n",
       "      <td>{'twitter.com': 5, 'xkcd.com': 1, 'swag.github...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266703520205549568</th>\n",
       "      <td>2020-05-30 12:10:45+00:00</td>\n",
       "      <td>üëΩ</td>\n",
       "      <td>None</td>\n",
       "      <td>panagiota_.b</td>\n",
       "      <td></td>\n",
       "      <td>b_panagiota</td>\n",
       "      <td>human</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089159225148882949</th>\n",
       "      <td>2019-01-26 13:52:49+00:00</td>\n",
       "      <td>mama to maya. ABIM research pathway fellow @UV...</td>\n",
       "      <td>Charlottesville, VA</td>\n",
       "      <td>Jacqueline Hodges, MD MPH</td>\n",
       "      <td></td>\n",
       "      <td>jachodges_md</td>\n",
       "      <td>human</td>\n",
       "      <td>350</td>\n",
       "      <td>577</td>\n",
       "      <td>237</td>\n",
       "      <td>...</td>\n",
       "      <td>12324.0</td>\n",
       "      <td>2976.0</td>\n",
       "      <td>11.534884</td>\n",
       "      <td>47.767442</td>\n",
       "      <td>{'twitter.com': 61}</td>\n",
       "      <td>{'professionalizeMICROBIOLOGY': 3, 'COVID19': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36741729</th>\n",
       "      <td>2009-04-30 19:01:42+00:00</td>\n",
       "      <td>Father / SWT Alumnus / Longhorn Fan</td>\n",
       "      <td>United States</td>\n",
       "      <td>Matthew Stubblefield</td>\n",
       "      <td></td>\n",
       "      <td>Matthew_Brody</td>\n",
       "      <td>bot</td>\n",
       "      <td>240</td>\n",
       "      <td>297</td>\n",
       "      <td>3713</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151138281</th>\n",
       "      <td>2013-02-05 14:50:17+00:00</td>\n",
       "      <td>„Ç§„É©„Çπ„ÉàAC„ÅØÈ´òÂìÅË≥™„Ç§„É©„Çπ„Éà„Ç¢„Éº„Éà/Âπ¥Ë≥ÄÁä∂Á≠â„ÅåÂÖ®ÁÑ°ÊñôDLÂèØËÉΩ‚ô™AI„Éô„ÇØ„Çø„Éº„ÉªEPSÂΩ¢ÂºèÁ¥†ÊùêÂÖ®...</td>\n",
       "      <td>‚ÜìÂà©Áî®ËÄÖ600‰∏á‰∫∫ÁÑ°ÊñôÁ¥†Êùê„Çµ„Ç§„Éà‚Üì„ÄÄÂïÜÁî®Âà©Áî®Á∑®ÈõÜÔºØÔº´Ë°®Ë®ò‰∏çË¶Å</td>\n",
       "      <td>„Éï„É™„ÉºÁ¥†ÊùêÈõÜ„Åã„Çè„ÅÑ„ÅÑÁÑ°Êñô„Ç§„É©„Çπ„ÉàAC/„Åä„Åó„ÇÉ„Çå„Éï„É¨„Éº„É†Êû†‚òÖIllustAC„Ç§„É©„Çπ„Éà„É¨„Éº„Çø„Éº</td>\n",
       "      <td>https://t.co/L6PE11Blkl</td>\n",
       "      <td>Illustratorjpn</td>\n",
       "      <td>human</td>\n",
       "      <td>1877</td>\n",
       "      <td>2057</td>\n",
       "      <td>101849</td>\n",
       "      <td>...</td>\n",
       "      <td>10230.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>243.571429</td>\n",
       "      <td>{'www.ac-illust.com': 40, 'twitter.com': 4, 'm...</td>\n",
       "      <td>{'artfair': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339035361</th>\n",
       "      <td>2013-04-09 12:09:34+00:00</td>\n",
       "      <td>next‚û¨Êú™ÂÆö Á¥´ÔΩ•Á∑ëÔæÉÔæûÔΩ®ÔΩØÔΩ∑ Ëâ≤„ÄÖ„Å™Êõ≤ËÅ¥„Åç„Åæ„Åô</td>\n",
       "      <td>OKAYAMA CITY</td>\n",
       "      <td>„Çä„Çá„ÅÜ„ÇÑ„Çì</td>\n",
       "      <td>https://t.co/NjDtATyqGc</td>\n",
       "      <td>_y3oa</td>\n",
       "      <td>human</td>\n",
       "      <td>13952</td>\n",
       "      <td>5334</td>\n",
       "      <td>1137495</td>\n",
       "      <td>...</td>\n",
       "      <td>597.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>3.790698</td>\n",
       "      <td>13.883721</td>\n",
       "      <td>{'c.cocacola.co.jp': 1, 'www.nmb48.com': 1, 'g...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318636852</th>\n",
       "      <td>2011-06-16 20:09:29+00:00</td>\n",
       "      <td>Heart of a lion with a Mind of a maniac. Louis...</td>\n",
       "      <td>Lake Charles, LA</td>\n",
       "      <td>Gavin Cecchini</td>\n",
       "      <td></td>\n",
       "      <td>GavinCecchini2</td>\n",
       "      <td>human</td>\n",
       "      <td>13743</td>\n",
       "      <td>183</td>\n",
       "      <td>964</td>\n",
       "      <td>...</td>\n",
       "      <td>2934.0</td>\n",
       "      <td>1605.0</td>\n",
       "      <td>38.214286</td>\n",
       "      <td>69.857143</td>\n",
       "      <td>{'twitter.com': 8, 'www.instagram.com': 5, 'ww...</td>\n",
       "      <td>{'ÊúÄÊñ∞Ë®ò‰∫ã': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43443354</th>\n",
       "      <td>2009-05-30 00:25:19+00:00</td>\n",
       "      <td>Marketplace Minister, Christ follower, Indepen...</td>\n",
       "      <td>Rockhampton Australia</td>\n",
       "      <td>Martin Allan</td>\n",
       "      <td>https://t.co/r3R5Bkng9m</td>\n",
       "      <td>MartinfromOz</td>\n",
       "      <td>human</td>\n",
       "      <td>2460</td>\n",
       "      <td>2935</td>\n",
       "      <td>35256</td>\n",
       "      <td>...</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>5.913043</td>\n",
       "      <td>36.086957</td>\n",
       "      <td>{'twitter.com': 6, 'youtu.be': 2, 'wordle.dani...</td>\n",
       "      <td>{'UPDATE': 1, 'StructureFire': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29471345</th>\n",
       "      <td>2009-04-07 15:33:46+00:00</td>\n",
       "      <td>I'm a CELEBRITY make up artist wanting every w...</td>\n",
       "      <td>Philadephia,PA</td>\n",
       "      <td>Tiffany/Beauty Mark</td>\n",
       "      <td>https://t.co/N0wEYcP0ud</td>\n",
       "      <td>Makeupbytiffy</td>\n",
       "      <td>human</td>\n",
       "      <td>471</td>\n",
       "      <td>520</td>\n",
       "      <td>14792</td>\n",
       "      <td>...</td>\n",
       "      <td>5540.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2.024390</td>\n",
       "      <td>135.121951</td>\n",
       "      <td>{'bit.ly': 28, 'www.instagram.com': 10}</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows √ó 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   created_at  \\\n",
       "id                                              \n",
       "1217628182611927040 2020-01-16 02:02:55+00:00   \n",
       "2664730894          2014-07-02 17:56:46+00:00   \n",
       "1266703520205549568 2020-05-30 12:10:45+00:00   \n",
       "1089159225148882949 2019-01-26 13:52:49+00:00   \n",
       "36741729            2009-04-30 19:01:42+00:00   \n",
       "...                                       ...   \n",
       "1151138281          2013-02-05 14:50:17+00:00   \n",
       "1339035361          2013-04-09 12:09:34+00:00   \n",
       "318636852           2011-06-16 20:09:29+00:00   \n",
       "43443354            2009-05-30 00:25:19+00:00   \n",
       "29471345            2009-04-07 15:33:46+00:00   \n",
       "\n",
       "                                                           description  \\\n",
       "id                                                                       \n",
       "1217628182611927040  Theoretical Computer Scientist. See also https...   \n",
       "2664730894                                                  creative _   \n",
       "1266703520205549568                                                  üëΩ   \n",
       "1089159225148882949  mama to maya. ABIM research pathway fellow @UV...   \n",
       "36741729                           Father / SWT Alumnus / Longhorn Fan   \n",
       "...                                                                ...   \n",
       "1151138281           „Ç§„É©„Çπ„ÉàAC„ÅØÈ´òÂìÅË≥™„Ç§„É©„Çπ„Éà„Ç¢„Éº„Éà/Âπ¥Ë≥ÄÁä∂Á≠â„ÅåÂÖ®ÁÑ°ÊñôDLÂèØËÉΩ‚ô™AI„Éô„ÇØ„Çø„Éº„ÉªEPSÂΩ¢ÂºèÁ¥†ÊùêÂÖ®...   \n",
       "1339035361                                   next‚û¨Êú™ÂÆö Á¥´ÔΩ•Á∑ëÔæÉÔæûÔΩ®ÔΩØÔΩ∑ Ëâ≤„ÄÖ„Å™Êõ≤ËÅ¥„Åç„Åæ„Åô   \n",
       "318636852            Heart of a lion with a Mind of a maniac. Louis...   \n",
       "43443354             Marketplace Minister, Christ follower, Indepen...   \n",
       "29471345             I'm a CELEBRITY make up artist wanting every w...   \n",
       "\n",
       "                                           location  \\\n",
       "id                                                    \n",
       "1217628182611927040                   Cambridge, MA   \n",
       "2664730894                                        üéà   \n",
       "1266703520205549568                            None   \n",
       "1089159225148882949             Charlottesville, VA   \n",
       "36741729                              United States   \n",
       "...                                             ...   \n",
       "1151138281           ‚ÜìÂà©Áî®ËÄÖ600‰∏á‰∫∫ÁÑ°ÊñôÁ¥†Êùê„Çµ„Ç§„Éà‚Üì„ÄÄÂïÜÁî®Âà©Áî®Á∑®ÈõÜÔºØÔº´Ë°®Ë®ò‰∏çË¶Å   \n",
       "1339035361                             OKAYAMA CITY   \n",
       "318636852                          Lake Charles, LA   \n",
       "43443354                      Rockhampton Australia   \n",
       "29471345                             Philadephia,PA   \n",
       "\n",
       "                                                              name  \\\n",
       "id                                                                   \n",
       "1217628182611927040                                     Boaz Barak   \n",
       "2664730894                                               olawale üí®   \n",
       "1266703520205549568                                   panagiota_.b   \n",
       "1089159225148882949                      Jacqueline Hodges, MD MPH   \n",
       "36741729                                      Matthew Stubblefield   \n",
       "...                                                            ...   \n",
       "1151138281           „Éï„É™„ÉºÁ¥†ÊùêÈõÜ„Åã„Çè„ÅÑ„ÅÑÁÑ°Êñô„Ç§„É©„Çπ„ÉàAC/„Åä„Åó„ÇÉ„Çå„Éï„É¨„Éº„É†Êû†‚òÖIllustAC„Ç§„É©„Çπ„Éà„É¨„Éº„Çø„Éº   \n",
       "1339035361                                                   „Çä„Çá„ÅÜ„ÇÑ„Çì   \n",
       "318636852                                           Gavin Cecchini   \n",
       "43443354                                              Martin Allan   \n",
       "29471345                                       Tiffany/Beauty Mark   \n",
       "\n",
       "                                         url        username  label  \\\n",
       "id                                                                    \n",
       "1217628182611927040  https://t.co/BoMip9FF17    boazbaraktcs  human   \n",
       "2664730894                                           wale_io  human   \n",
       "1266703520205549568                              b_panagiota  human   \n",
       "1089159225148882949                             jachodges_md  human   \n",
       "36741729                                       Matthew_Brody    bot   \n",
       "...                                      ...             ...    ...   \n",
       "1151138281           https://t.co/L6PE11Blkl  Illustratorjpn  human   \n",
       "1339035361           https://t.co/NjDtATyqGc           _y3oa  human   \n",
       "318636852                                     GavinCecchini2  human   \n",
       "43443354             https://t.co/r3R5Bkng9m    MartinfromOz  human   \n",
       "29471345             https://t.co/N0wEYcP0ud   Makeupbytiffy  human   \n",
       "\n",
       "                     followers_count  following_count  tweet_count  ...  \\\n",
       "id                                                                  ...   \n",
       "1217628182611927040             7316              215         3098  ...   \n",
       "2664730894                       123             1090         1823  ...   \n",
       "1266703520205549568                3               62           66  ...   \n",
       "1089159225148882949              350              577          237  ...   \n",
       "36741729                         240              297         3713  ...   \n",
       "...                              ...              ...          ...  ...   \n",
       "1151138281                      1877             2057       101849  ...   \n",
       "1339035361                     13952             5334      1137495  ...   \n",
       "318636852                      13743              183          964  ...   \n",
       "43443354                        2460             2935        35256  ...   \n",
       "29471345                         471              520        14792  ...   \n",
       "\n",
       "                     tweet_urls_total tweet_hashtags_total  \\\n",
       "id                                                           \n",
       "1217628182611927040           39940.0              27598.0   \n",
       "2664730894                     5154.0               4005.0   \n",
       "1266703520205549568             252.0                252.0   \n",
       "1089159225148882949           12324.0               2976.0   \n",
       "36741729                          NaN                  NaN   \n",
       "...                               ...                  ...   \n",
       "1151138281                    10230.0                126.0   \n",
       "1339035361                      597.0                163.0   \n",
       "318636852                      2934.0               1605.0   \n",
       "43443354                       1660.0                272.0   \n",
       "29471345                       5540.0                 83.0   \n",
       "\n",
       "                    avg_hashtags_in_tweet avg_urls_in_tweet  \\\n",
       "id                                                            \n",
       "1217628182611927040             24.208772         35.035088   \n",
       "2664730894                       4.009009          5.159159   \n",
       "1266703520205549568              4.000000          4.000000   \n",
       "1089159225148882949             11.534884         47.767442   \n",
       "36741729                              NaN               NaN   \n",
       "...                                   ...               ...   \n",
       "1151138281                       3.000000        243.571429   \n",
       "1339035361                       3.790698         13.883721   \n",
       "318636852                       38.214286         69.857143   \n",
       "43443354                         5.913043         36.086957   \n",
       "29471345                         2.024390        135.121951   \n",
       "\n",
       "                                                      tweet_urls_top_x  \\\n",
       "id                                                                       \n",
       "1217628182611927040  {'twitter.com': 110, 'horoscoponegro.com': 29,...   \n",
       "2664730894           {'twitter.com': 5, 'xkcd.com': 1, 'swag.github...   \n",
       "1266703520205549568                                                 {}   \n",
       "1089159225148882949                                {'twitter.com': 61}   \n",
       "36741729                                                           NaN   \n",
       "...                                                                ...   \n",
       "1151138281           {'www.ac-illust.com': 40, 'twitter.com': 4, 'm...   \n",
       "1339035361           {'c.cocacola.co.jp': 1, 'www.nmb48.com': 1, 'g...   \n",
       "318636852            {'twitter.com': 8, 'www.instagram.com': 5, 'ww...   \n",
       "43443354             {'twitter.com': 6, 'youtu.be': 2, 'wordle.dani...   \n",
       "29471345                       {'bit.ly': 28, 'www.instagram.com': 10}   \n",
       "\n",
       "                                                  tweet_hashtags_top_x  \\\n",
       "id                                                                       \n",
       "1217628182611927040  {'Aries': 323, 'ARIES': 286, 'aries': 3, 'Part...   \n",
       "2664730894                                                          {}   \n",
       "1266703520205549568                                                 {}   \n",
       "1089159225148882949  {'professionalizeMICROBIOLOGY': 3, 'COVID19': ...   \n",
       "36741729                                                           NaN   \n",
       "...                                                                ...   \n",
       "1151138281                                              {'artfair': 1}   \n",
       "1339035361                                                          {}   \n",
       "318636852                                                  {'ÊúÄÊñ∞Ë®ò‰∫ã': 1}   \n",
       "43443354                             {'UPDATE': 1, 'StructureFire': 1}   \n",
       "29471345                                                            {}   \n",
       "\n",
       "                    tweet_has_hashtag_weekday_entropy  \\\n",
       "id                                                      \n",
       "1217628182611927040                               NaN   \n",
       "2664730894                                        NaN   \n",
       "1266703520205549568                               NaN   \n",
       "1089159225148882949                               NaN   \n",
       "36741729                                          NaN   \n",
       "...                                               ...   \n",
       "1151138281                                        NaN   \n",
       "1339035361                                        NaN   \n",
       "318636852                                         NaN   \n",
       "43443354                                          NaN   \n",
       "29471345                                          NaN   \n",
       "\n",
       "                     tweet_has_hashtag_hour_entropy  \\\n",
       "id                                                    \n",
       "1217628182611927040                             NaN   \n",
       "2664730894                                      NaN   \n",
       "1266703520205549568                             NaN   \n",
       "1089159225148882949                             NaN   \n",
       "36741729                                        NaN   \n",
       "...                                             ...   \n",
       "1151138281                                      NaN   \n",
       "1339035361                                      NaN   \n",
       "318636852                                       NaN   \n",
       "43443354                                        NaN   \n",
       "29471345                                        NaN   \n",
       "\n",
       "                     tweet_has_url_weekday_entropy  tweet_has_url_hour_entropy  \n",
       "id                                                                              \n",
       "1217628182611927040                            NaN                         NaN  \n",
       "2664730894                                     NaN                         NaN  \n",
       "1266703520205549568                            NaN                         NaN  \n",
       "1089159225148882949                            NaN                         NaN  \n",
       "36741729                                       NaN                         NaN  \n",
       "...                                            ...                         ...  \n",
       "1151138281                                     NaN                         NaN  \n",
       "1339035361                                     NaN                         NaN  \n",
       "318636852                                      NaN                         NaN  \n",
       "43443354                                       NaN                         NaN  \n",
       "29471345                                       NaN                         NaN  \n",
       "\n",
       "[1000000 rows x 55 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import fastparquet\n",
    "def sdata(name, _dir=generated_data_output, df=None, **kwargs):\n",
    "    if df is None:\n",
    "            raise ValueError(\"No dataframe provided to save.\")\n",
    "    file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "    print(f\"Saving data to {file_path}\")\n",
    "    os.makedirs(_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    fastparquet.write(file_path, data=df)\n",
    "    return df        \n",
    "    \n",
    "def _shuffle(df):\n",
    "    return df.sample(frac = 1)\n",
    "    \n",
    "def loadData():\n",
    "    dbfile = open('temp', 'rb')    \n",
    "    return pickle.load(dbfile)\n",
    "    \n",
    "l_data = loadData()\n",
    "print(l_data.head(1))\n",
    "\n",
    "sdata(\"assembled_user_details\", df=l_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65bade80-ac15-45ab-b285-b4a2a56b5fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing data from /dataset/twibot22/generated_data/assembled_user_details.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>username</th>\n",
       "      <th>label</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet_urls_total</th>\n",
       "      <th>tweet_hashtags_total</th>\n",
       "      <th>avg_hashtags_in_tweet</th>\n",
       "      <th>avg_urls_in_tweet</th>\n",
       "      <th>tweet_urls_top_x</th>\n",
       "      <th>tweet_hashtags_top_x</th>\n",
       "      <th>tweet_has_hashtag_weekday_entropy</th>\n",
       "      <th>tweet_has_hashtag_hour_entropy</th>\n",
       "      <th>tweet_has_url_weekday_entropy</th>\n",
       "      <th>tweet_has_url_hour_entropy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1217628182611927040</th>\n",
       "      <td>2020-01-16 02:02:55+00:00</td>\n",
       "      <td>Theoretical Computer Scientist. See also https...</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>Boaz Barak</td>\n",
       "      <td>https://t.co/BoMip9FF17</td>\n",
       "      <td>boazbaraktcs</td>\n",
       "      <td>human</td>\n",
       "      <td>7316</td>\n",
       "      <td>215</td>\n",
       "      <td>3098</td>\n",
       "      <td>...</td>\n",
       "      <td>39940.0</td>\n",
       "      <td>27598.0</td>\n",
       "      <td>24.208772</td>\n",
       "      <td>35.035088</td>\n",
       "      <td>{'twitter.com': 110, 'horoscoponegro.com': 29,...</td>\n",
       "      <td>{'Aries': 323, 'ARIES': 286, 'aries': 3, 'Part...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664730894</th>\n",
       "      <td>2014-07-02 17:56:46+00:00</td>\n",
       "      <td>creative _</td>\n",
       "      <td>üéà</td>\n",
       "      <td>olawale üí®</td>\n",
       "      <td></td>\n",
       "      <td>wale_io</td>\n",
       "      <td>human</td>\n",
       "      <td>123</td>\n",
       "      <td>1090</td>\n",
       "      <td>1823</td>\n",
       "      <td>...</td>\n",
       "      <td>5154.0</td>\n",
       "      <td>4005.0</td>\n",
       "      <td>4.009009</td>\n",
       "      <td>5.159159</td>\n",
       "      <td>{'twitter.com': 5, 'xkcd.com': 1, 'swag.github...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089159225148882949</th>\n",
       "      <td>2019-01-26 13:52:49+00:00</td>\n",
       "      <td>mama to maya. ABIM research pathway fellow @UV...</td>\n",
       "      <td>Charlottesville, VA</td>\n",
       "      <td>Jacqueline Hodges, MD MPH</td>\n",
       "      <td></td>\n",
       "      <td>jachodges_md</td>\n",
       "      <td>human</td>\n",
       "      <td>350</td>\n",
       "      <td>577</td>\n",
       "      <td>237</td>\n",
       "      <td>...</td>\n",
       "      <td>12324.0</td>\n",
       "      <td>2976.0</td>\n",
       "      <td>11.534884</td>\n",
       "      <td>47.767442</td>\n",
       "      <td>{'twitter.com': 61}</td>\n",
       "      <td>{'professionalizeMICROBIOLOGY': 3, 'COVID19': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15211869</th>\n",
       "      <td>2008-06-23 20:59:59+00:00</td>\n",
       "      <td>Director, Knowledge Ecology International, an ...</td>\n",
       "      <td>√úT: 38.911326,-77.04508</td>\n",
       "      <td>James Love</td>\n",
       "      <td>https://t.co/mcNZxOR7gv</td>\n",
       "      <td>jamie_love</td>\n",
       "      <td>human</td>\n",
       "      <td>10299</td>\n",
       "      <td>2166</td>\n",
       "      <td>57397</td>\n",
       "      <td>...</td>\n",
       "      <td>130600.0</td>\n",
       "      <td>65339.0</td>\n",
       "      <td>54.223237</td>\n",
       "      <td>108.381743</td>\n",
       "      <td>{'twitter.com': 396, 'bit.ly': 93, 'ow.ly': 27...</td>\n",
       "      <td>{'cdntech': 88, 'WITCanada2021': 40, 'CTAConne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138814032</th>\n",
       "      <td>2010-04-30 17:36:51+00:00</td>\n",
       "      <td>Militante peronista. Vicepresidenta de la Rep√∫...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Cristina Kirchner</td>\n",
       "      <td>https://t.co/P8WemOJelF</td>\n",
       "      <td>CFKArgentina</td>\n",
       "      <td>human</td>\n",
       "      <td>5994250</td>\n",
       "      <td>241</td>\n",
       "      <td>15538</td>\n",
       "      <td>...</td>\n",
       "      <td>80816.0</td>\n",
       "      <td>15119.0</td>\n",
       "      <td>14.939723</td>\n",
       "      <td>79.857708</td>\n",
       "      <td>{'twitter.com': 292, 'bit.ly': 20, 'penntoday....</td>\n",
       "      <td>{'LISprochat': 22, 'ASEEVC': 17, 'BiotechCommo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457554412</th>\n",
       "      <td>2012-01-07 15:05:53+00:00</td>\n",
       "      <td>They/Them\\nhttps://t.co/UvtxD9uZtX</td>\n",
       "      <td>London, England</td>\n",
       "      <td>samsmith</td>\n",
       "      <td>https://t.co/UvtxD9uZtX</td>\n",
       "      <td>samsmith</td>\n",
       "      <td>human</td>\n",
       "      <td>7982826</td>\n",
       "      <td>1302</td>\n",
       "      <td>14644</td>\n",
       "      <td>...</td>\n",
       "      <td>221345.0</td>\n",
       "      <td>111886.0</td>\n",
       "      <td>111.997998</td>\n",
       "      <td>221.566567</td>\n",
       "      <td>{'twitter.com': 648, 'ow.ly': 392, 'bit.ly': 1...</td>\n",
       "      <td>{'CX': 106, 'CustomerExperience': 83, 'CCM': 6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465283662</th>\n",
       "      <td>2014-04-27 00:20:12+00:00</td>\n",
       "      <td>paper tweets, dms are open</td>\n",
       "      <td>None</td>\n",
       "      <td>AK</td>\n",
       "      <td></td>\n",
       "      <td>ak92501</td>\n",
       "      <td>bot</td>\n",
       "      <td>45541</td>\n",
       "      <td>1206</td>\n",
       "      <td>9194</td>\n",
       "      <td>...</td>\n",
       "      <td>306610.0</td>\n",
       "      <td>45330.0</td>\n",
       "      <td>24.423491</td>\n",
       "      <td>165.199353</td>\n",
       "      <td>{'arxiv.org': 622, 'twitter.com': 380, 'github...</td>\n",
       "      <td>{'5G': 41, 'MobiledgeX': 34, 'TelcoEdgeCloud':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467973039883182090</th>\n",
       "      <td>2021-12-06 21:44:04+00:00</td>\n",
       "      <td>https://t.co/Hmg5gBvd9A</td>\n",
       "      <td>None</td>\n",
       "      <td>ÿµÿßÿ±ÿß</td>\n",
       "      <td></td>\n",
       "      <td>_3rw_</td>\n",
       "      <td>human</td>\n",
       "      <td>1573</td>\n",
       "      <td>1688</td>\n",
       "      <td>146</td>\n",
       "      <td>...</td>\n",
       "      <td>26861.0</td>\n",
       "      <td>12236.0</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>58.393478</td>\n",
       "      <td>{'twitter.com': 128, 'bit.ly': 4, 'www.ferrari...</td>\n",
       "      <td>{'RoyalRumble': 129, 'royalrumble': 30, 'Sanre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234059290</th>\n",
       "      <td>2011-01-04 19:11:39+00:00</td>\n",
       "      <td>Come for the science (genetics &amp; cell biology)...</td>\n",
       "      <td>Salt Lake City,UT, USA</td>\n",
       "      <td>Professor Booty PhD</td>\n",
       "      <td>https://t.co/pKcvVO96Yk</td>\n",
       "      <td>ProfBootyPhD</td>\n",
       "      <td>human</td>\n",
       "      <td>4694</td>\n",
       "      <td>4739</td>\n",
       "      <td>91381</td>\n",
       "      <td>...</td>\n",
       "      <td>121595.0</td>\n",
       "      <td>53362.0</td>\n",
       "      <td>39.941617</td>\n",
       "      <td>91.014222</td>\n",
       "      <td>{'twitter.com': 398, 'bit.ly': 31, 'youtu.be':...</td>\n",
       "      <td>{'COVID19': 54, 'AzadiKaAmritMahotsav': 26, 'I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142890104853106688</th>\n",
       "      <td>2019-06-23 20:20:09+00:00</td>\n",
       "      <td>Quite likely the most eminent worm wrangler in...</td>\n",
       "      <td>None</td>\n",
       "      <td>wyomingwormboy</td>\n",
       "      <td></td>\n",
       "      <td>wyomingwormboy</td>\n",
       "      <td>human</td>\n",
       "      <td>2913</td>\n",
       "      <td>587</td>\n",
       "      <td>3343</td>\n",
       "      <td>...</td>\n",
       "      <td>73917.0</td>\n",
       "      <td>20143.0</td>\n",
       "      <td>17.500434</td>\n",
       "      <td>64.219809</td>\n",
       "      <td>{'twitter.com': 152, '0.zailuo.cn': 29, 'www.n...</td>\n",
       "      <td>{'PEC': 93, 'ETH': 58, 'BTC': 58, 'neuroscienc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   created_at  \\\n",
       "id                                              \n",
       "1217628182611927040 2020-01-16 02:02:55+00:00   \n",
       "2664730894          2014-07-02 17:56:46+00:00   \n",
       "1089159225148882949 2019-01-26 13:52:49+00:00   \n",
       "15211869            2008-06-23 20:59:59+00:00   \n",
       "138814032           2010-04-30 17:36:51+00:00   \n",
       "457554412           2012-01-07 15:05:53+00:00   \n",
       "2465283662          2014-04-27 00:20:12+00:00   \n",
       "1467973039883182090 2021-12-06 21:44:04+00:00   \n",
       "234059290           2011-01-04 19:11:39+00:00   \n",
       "1142890104853106688 2019-06-23 20:20:09+00:00   \n",
       "\n",
       "                                                           description  \\\n",
       "id                                                                       \n",
       "1217628182611927040  Theoretical Computer Scientist. See also https...   \n",
       "2664730894                                                  creative _   \n",
       "1089159225148882949  mama to maya. ABIM research pathway fellow @UV...   \n",
       "15211869             Director, Knowledge Ecology International, an ...   \n",
       "138814032            Militante peronista. Vicepresidenta de la Rep√∫...   \n",
       "457554412                           They/Them\\nhttps://t.co/UvtxD9uZtX   \n",
       "2465283662                                  paper tweets, dms are open   \n",
       "1467973039883182090                            https://t.co/Hmg5gBvd9A   \n",
       "234059290            Come for the science (genetics & cell biology)...   \n",
       "1142890104853106688  Quite likely the most eminent worm wrangler in...   \n",
       "\n",
       "                                    location                       name  \\\n",
       "id                                                                        \n",
       "1217628182611927040            Cambridge, MA                 Boaz Barak   \n",
       "2664730894                                 üéà                  olawale üí®   \n",
       "1089159225148882949      Charlottesville, VA  Jacqueline Hodges, MD MPH   \n",
       "15211869             √úT: 38.911326,-77.04508                 James Love   \n",
       "138814032                          Argentina          Cristina Kirchner   \n",
       "457554412                    London, England                   samsmith   \n",
       "2465283662                              None                         AK   \n",
       "1467973039883182090                     None                       ÿµÿßÿ±ÿß   \n",
       "234059290             Salt Lake City,UT, USA        Professor Booty PhD   \n",
       "1142890104853106688                     None             wyomingwormboy   \n",
       "\n",
       "                                         url        username  label  \\\n",
       "id                                                                    \n",
       "1217628182611927040  https://t.co/BoMip9FF17    boazbaraktcs  human   \n",
       "2664730894                                           wale_io  human   \n",
       "1089159225148882949                             jachodges_md  human   \n",
       "15211869             https://t.co/mcNZxOR7gv      jamie_love  human   \n",
       "138814032            https://t.co/P8WemOJelF    CFKArgentina  human   \n",
       "457554412            https://t.co/UvtxD9uZtX        samsmith  human   \n",
       "2465283662                                           ak92501    bot   \n",
       "1467973039883182090                                    _3rw_  human   \n",
       "234059290            https://t.co/pKcvVO96Yk    ProfBootyPhD  human   \n",
       "1142890104853106688                           wyomingwormboy  human   \n",
       "\n",
       "                     followers_count  following_count  tweet_count  ...  \\\n",
       "id                                                                  ...   \n",
       "1217628182611927040             7316              215         3098  ...   \n",
       "2664730894                       123             1090         1823  ...   \n",
       "1089159225148882949              350              577          237  ...   \n",
       "15211869                       10299             2166        57397  ...   \n",
       "138814032                    5994250              241        15538  ...   \n",
       "457554412                    7982826             1302        14644  ...   \n",
       "2465283662                     45541             1206         9194  ...   \n",
       "1467973039883182090             1573             1688          146  ...   \n",
       "234059290                       4694             4739        91381  ...   \n",
       "1142890104853106688             2913              587         3343  ...   \n",
       "\n",
       "                     tweet_urls_total tweet_hashtags_total  \\\n",
       "id                                                           \n",
       "1217628182611927040           39940.0              27598.0   \n",
       "2664730894                     5154.0               4005.0   \n",
       "1089159225148882949           12324.0               2976.0   \n",
       "15211869                     130600.0              65339.0   \n",
       "138814032                     80816.0              15119.0   \n",
       "457554412                    221345.0             111886.0   \n",
       "2465283662                   306610.0              45330.0   \n",
       "1467973039883182090           26861.0              12236.0   \n",
       "234059290                    121595.0              53362.0   \n",
       "1142890104853106688           73917.0              20143.0   \n",
       "\n",
       "                    avg_hashtags_in_tweet avg_urls_in_tweet  \\\n",
       "id                                                            \n",
       "1217628182611927040             24.208772         35.035088   \n",
       "2664730894                       4.009009          5.159159   \n",
       "1089159225148882949             11.534884         47.767442   \n",
       "15211869                        54.223237        108.381743   \n",
       "138814032                       14.939723         79.857708   \n",
       "457554412                      111.997998        221.566567   \n",
       "2465283662                      24.423491        165.199353   \n",
       "1467973039883182090             26.600000         58.393478   \n",
       "234059290                       39.941617         91.014222   \n",
       "1142890104853106688             17.500434         64.219809   \n",
       "\n",
       "                                                      tweet_urls_top_x  \\\n",
       "id                                                                       \n",
       "1217628182611927040  {'twitter.com': 110, 'horoscoponegro.com': 29,...   \n",
       "2664730894           {'twitter.com': 5, 'xkcd.com': 1, 'swag.github...   \n",
       "1089159225148882949                                {'twitter.com': 61}   \n",
       "15211869             {'twitter.com': 396, 'bit.ly': 93, 'ow.ly': 27...   \n",
       "138814032            {'twitter.com': 292, 'bit.ly': 20, 'penntoday....   \n",
       "457554412            {'twitter.com': 648, 'ow.ly': 392, 'bit.ly': 1...   \n",
       "2465283662           {'arxiv.org': 622, 'twitter.com': 380, 'github...   \n",
       "1467973039883182090  {'twitter.com': 128, 'bit.ly': 4, 'www.ferrari...   \n",
       "234059290            {'twitter.com': 398, 'bit.ly': 31, 'youtu.be':...   \n",
       "1142890104853106688  {'twitter.com': 152, '0.zailuo.cn': 29, 'www.n...   \n",
       "\n",
       "                                                  tweet_hashtags_top_x  \\\n",
       "id                                                                       \n",
       "1217628182611927040  {'Aries': 323, 'ARIES': 286, 'aries': 3, 'Part...   \n",
       "2664730894                                                          {}   \n",
       "1089159225148882949  {'professionalizeMICROBIOLOGY': 3, 'COVID19': ...   \n",
       "15211869             {'cdntech': 88, 'WITCanada2021': 40, 'CTAConne...   \n",
       "138814032            {'LISprochat': 22, 'ASEEVC': 17, 'BiotechCommo...   \n",
       "457554412            {'CX': 106, 'CustomerExperience': 83, 'CCM': 6...   \n",
       "2465283662           {'5G': 41, 'MobiledgeX': 34, 'TelcoEdgeCloud':...   \n",
       "1467973039883182090  {'RoyalRumble': 129, 'royalrumble': 30, 'Sanre...   \n",
       "234059290            {'COVID19': 54, 'AzadiKaAmritMahotsav': 26, 'I...   \n",
       "1142890104853106688  {'PEC': 93, 'ETH': 58, 'BTC': 58, 'neuroscienc...   \n",
       "\n",
       "                    tweet_has_hashtag_weekday_entropy  \\\n",
       "id                                                      \n",
       "1217628182611927040                               NaN   \n",
       "2664730894                                        NaN   \n",
       "1089159225148882949                               NaN   \n",
       "15211869                                          NaN   \n",
       "138814032                                         NaN   \n",
       "457554412                                         NaN   \n",
       "2465283662                                        NaN   \n",
       "1467973039883182090                               NaN   \n",
       "234059290                                         NaN   \n",
       "1142890104853106688                               NaN   \n",
       "\n",
       "                     tweet_has_hashtag_hour_entropy  \\\n",
       "id                                                    \n",
       "1217628182611927040                             NaN   \n",
       "2664730894                                      NaN   \n",
       "1089159225148882949                             NaN   \n",
       "15211869                                        NaN   \n",
       "138814032                                       NaN   \n",
       "457554412                                       NaN   \n",
       "2465283662                                      NaN   \n",
       "1467973039883182090                             NaN   \n",
       "234059290                                       NaN   \n",
       "1142890104853106688                             NaN   \n",
       "\n",
       "                     tweet_has_url_weekday_entropy  tweet_has_url_hour_entropy  \n",
       "id                                                                              \n",
       "1217628182611927040                            NaN                         NaN  \n",
       "2664730894                                     NaN                         NaN  \n",
       "1089159225148882949                            NaN                         NaN  \n",
       "15211869                                       NaN                         NaN  \n",
       "138814032                                      NaN                         NaN  \n",
       "457554412                                      NaN                         NaN  \n",
       "2465283662                                     NaN                         NaN  \n",
       "1467973039883182090                            NaN                         NaN  \n",
       "234059290                                      NaN                         NaN  \n",
       "1142890104853106688                            NaN                         NaN  \n",
       "\n",
       "[10 rows x 55 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_detail_data = get_data(\"assembled_user_details\")\n",
    "display(user_detail_data.loc[(user_detail_data['tweet_has_media_ratio'] > 0) & (user_detail_data['tweet_has_media_ratio'] < 1)].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2463c-6fb5-476b-a1c5-5db69913c36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
