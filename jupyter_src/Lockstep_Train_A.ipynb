{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287cbec8-1d22-4de4-98d7-fc89a2707f03",
   "metadata": {},
   "source": [
    "# Lockstep, the full recipe, Part 2: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f1cfb3-7b76-4d0b-b10c-17c4c79403ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from fastparquet import ParquetFile\n",
    "\n",
    "verbosity = 5\n",
    "\n",
    "twibot_path = r\"/dataset/twibot22\"\n",
    "twibot_user = r\"/dataset/twibot22/user.json\"\n",
    "twibot_label = r\"/dataset/twibot22/label.csv\"\n",
    "twibot_graph_file = f\"{twibot_path}/edge.csv\"\n",
    "\n",
    "generated_data_output = r\"/dataset/twibot22/generated_data\" # output is saved in this directory\n",
    "ls_selected_indices_output = os.path.join(generated_data_output, \"selected.csv\")\n",
    "ls_userdata_output_parquet = \"assembled_user_details.parquet\"\n",
    "url_model_output = os.path.join(generated_data_output,\"url_model.pkl\")\n",
    "url_model_prediction_output = os.path.join(generated_data_output,\"url_predict.csv\")\n",
    "\n",
    "\n",
    "gnn_output_folder = os.path.join(generated_data_output,\"gs_output\")\n",
    "gnn_output_dataset_name = os.path.join(gnn_output_folder,\"gnn_graph_data.pt\")\n",
    "gnn_output_checkpoint_name = os.path.join(gnn_output_folder,\"graphsage_model.ckpt\")\n",
    "gnn_output_relmap_name = os.path.join(gnn_output_folder,\"relationship_map.pkl\")\n",
    "gnn_output_map_name = os.path.join(gnn_output_folder,\"index_map.json\")\n",
    "gnn_output_predictions_name =  os.path.join(gnn_output_folder,\"gs_predictions.csv\")\n",
    "gnn_output_model_path = os.path.join(gnn_output_folder,\"graphsage_final.pt\")\n",
    "\n",
    "# Files in the path specified by twibot_path, that begin with %twibot_node_identifier_str%, will be assumed as node files and converted if needed.\n",
    "twibot_node_identifier_str = \"tweet_\" \n",
    "NODE_FILE_LIST = list(filter(lambda fileName: twibot_node_identifier_str in fileName, \n",
    "                                        [child.name for child in Path(generated_data_output).iterdir()]))\n",
    "\n",
    "\n",
    "\n",
    "concurrent_max_workers = 2\n",
    "\n",
    "sample_set_constrain_minimum_posts = 25\n",
    "sample_set_save_selected_indices = True\n",
    "sample_set_size_per_label = 50000 # per label, sample this many users\n",
    "sample_set_stratification = True # If at any point during selection our set becomes unbalanced, should we stratify?\n",
    "sample_set_randomization = True # Should the _shuffle method be run on rows before the data is used on anything more?\n",
    "sample_set_sampling_strategy = (1,0) # (1over/0under, 1major/0minor)\n",
    "\n",
    "gnn_dataset_build_num_pass = 3\n",
    "gnn_doing_experiment = False\n",
    "gnn_data_load_from_file = True\n",
    "gnn_load_from_checkpoint = True\n",
    "gnn_model_type = \"graphsage\"  # Options: \"dropedge_gcn\", \"gcn\", or \"graphsage\"\n",
    "\n",
    "scores = {}\n",
    "\n",
    "if not Path(gnn_output_folder).exists:\n",
    "    os.mkdir(gnn_output_folder)\n",
    "    \n",
    "def debug_print(m, level=5, r=None):\n",
    "    if level <= verbosity:\n",
    "        print(m)\n",
    "        if r:\n",
    "            raise r\n",
    "    \n",
    "def is_data(name, _dir=generated_data_output):\n",
    "    file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "    return os.path.exists(file_path)\n",
    "    \n",
    "def get_data(name, _dir=generated_data_output, pqargs={}, **kwargs):\n",
    "    if is_data(name, _dir):\n",
    "        file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "        print(f\"Loading existing data from {file_path}\")\n",
    "        #return pd.read_parquet(file_path)\n",
    "        pf = ParquetFile(file_path, **pqargs)\n",
    "        return pf.to_pandas(**kwargs)\n",
    "    return False\n",
    "        \n",
    "def save_data(name, _dir=generated_data_output, df=None, **kwargs):\n",
    "    if df is None:\n",
    "            raise ValueError(\"No dataframe provided to save.\")\n",
    "    file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "    print(f\"Saving data to {file_path}\")\n",
    "    os.makedirs(_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    df.to_parquet(file_path, **kwargs)\n",
    "    return df      \n",
    "    \n",
    "def _shuffle(df):\n",
    "    return df.sample(frac = 1)\n",
    "    \n",
    "shuffle_method = _shuffle\n",
    "\n",
    "# To quietly stop cell execution\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "import json\n",
    "def get_post_counts():\n",
    "    tweetNodeFilesParquet = list(filter(lambda fileName: twibot_node_identifier_str in fileName, \n",
    "                                        [child.name for child in Path(generated_data_output).iterdir()]))\n",
    "    post_count_dict = defaultdict(int)\n",
    "    debug_print(f\"Called: get_post_counts\", 5)\n",
    "    for targetFile in tweetNodeFilesParquet:\n",
    "        targetInput = Path(f\"{generated_data_output}/{targetFile}\")\n",
    "        try:\n",
    "            debug_print(\"Looking in \" + targetInput.__str__(), 5)\n",
    "            pf = ParquetFile(targetInput)\n",
    "            df = pf.to_pandas(columns=['author_id'])\n",
    "            for uid in df['author_id']:\n",
    "                post_count_dict[uid] = post_count_dict[uid] + 1\n",
    "            del pf, df\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            debug_print(f\"Failed to load node parquet: {e}\", 5)\n",
    "            raise RuntimeError(\"Error processing Parquet files.\")\n",
    "    debug_print(f\"Completed: get_post_counts\", 5)\n",
    "    return post_count_dict\n",
    "    \n",
    "def get_post_chunks(cols = '*', index=\"author_id\", pqargs={}, pdkwargs={}, margs={}):\n",
    "    # Result: Dataframe, index with one or more features.\n",
    "    # Index: from users\n",
    "    global NODE_FILE_LIST\n",
    "    result_builder = None\n",
    "    for targetFile in NODE_FILE_LIST:\n",
    "        targetInput = Path(f\"{generated_data_output}/{targetFile}\")\n",
    "        debug_print(f\"Extracting from {targetInput.__str__()}...\", 5)\n",
    "        \n",
    "        if cols != '*':\n",
    "            # Set the columns to pull from the parquet, either through pqargs directly or here, through cols\n",
    "            pdkwargs['columns'] = cols\n",
    "            \n",
    "        pdkwargs['index'] = index       \n",
    "        try:\n",
    "            pfinput = ParquetFile(targetInput, **pqargs)    \n",
    "            process_group = pfinput.to_pandas(**pdkwargs)  \n",
    "            result_builder = pd.concat([result_builder, process_group])         \n",
    "        except Exception as e:\n",
    "            debug_print(f\"Failed to load node parquet: {e}\", 5)\n",
    "            raise RuntimeError(\"Error processing Parquet files.\")\n",
    "    return result_builder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b719f-c57e-4fb4-92f1-39ad552695ee",
   "metadata": {},
   "source": [
    "# Lockstep, the full recipe, Part 2: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b5ab4-aae1-4a52-b0fc-b00426fb3e26",
   "metadata": {},
   "source": [
    "## Stage 1: Sample Selection, Constraint, Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75492b5b-eb89-48a6-8309-bdc3da9a2cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from parquet at /dataset/twibot22/generated_data\n",
      "Loading existing data from /dataset/twibot22/generated_data/assembled_user_details.parquet\n",
      "Loaded parquet. Column names: created_at                           datetime64[ns, UTC]\n",
      "description                                       object\n",
      "location                                          object\n",
      "name                                              object\n",
      "url                                               object\n",
      "username                                          object\n",
      "label                                             object\n",
      "followers_count                                    int64\n",
      "following_count                                    int64\n",
      "tweet_count                                        int64\n",
      "listed_count                                       int64\n",
      "url.urls                                          object\n",
      "description.urls                                  object\n",
      "description.mentions                              object\n",
      "description.hashtags                              object\n",
      "description.cashtags                              object\n",
      "posts                                             object\n",
      "following_followers_ratio                        float64\n",
      "tweet_followers_ratio                            float64\n",
      "tweet_following_ratio                            float64\n",
      "sampled_post_count                                UInt64\n",
      "profile_desc_len                                   int64\n",
      "profile_username_len                               int64\n",
      "profile_has_location                                bool\n",
      "profile_desc_mentions_count                      float64\n",
      "profile_desc_hashtag_count                       float64\n",
      "profile_desc_url_count                           float64\n",
      "tweet_has_media_ratio                            float64\n",
      "tweet_has_geo_ratio                              float64\n",
      "total_rt                                         float64\n",
      "total_likes                                      float64\n",
      "total_quotes                                     float64\n",
      "average_rt                                       float64\n",
      "average_likes                                    float64\n",
      "average_quotes                                   float64\n",
      "likes_chi                                        float64\n",
      "rts_chi                                          float64\n",
      "likes_zero_ratio                                 float64\n",
      "rts_zero_ratio                                   float64\n",
      "entropy_between_post_times                       float64\n",
      "entropy_between_post_hours                       float64\n",
      "entropy_between_post_days                        float64\n",
      "entropy_between_post_weekdays                    float64\n",
      "tweet_has_hashtags_ratio                         float64\n",
      "tweet_has_urls_ratio                             float64\n",
      "tweet_urls_total                                 float64\n",
      "tweet_hashtags_total                             float64\n",
      "avg_hashtags_in_tweet                            float64\n",
      "avg_urls_in_tweet                                float64\n",
      "tweet_urls_top_x                                  object\n",
      "tweet_hashtags_top_x                              object\n",
      "tweet_has_hashtag_weekday_entropy                float64\n",
      "tweet_has_hashtag_hour_entropy                   float64\n",
      "tweet_has_url_weekday_entropy                    float64\n",
      "tweet_has_url_hour_entropy                       float64\n",
      "dtype: object\n",
      "Min/Max sampled posts on a user: 1, (7071)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>username</th>\n",
       "      <th>label</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet_urls_total</th>\n",
       "      <th>tweet_hashtags_total</th>\n",
       "      <th>avg_hashtags_in_tweet</th>\n",
       "      <th>avg_urls_in_tweet</th>\n",
       "      <th>tweet_urls_top_x</th>\n",
       "      <th>tweet_hashtags_top_x</th>\n",
       "      <th>tweet_has_hashtag_weekday_entropy</th>\n",
       "      <th>tweet_has_hashtag_hour_entropy</th>\n",
       "      <th>tweet_has_url_weekday_entropy</th>\n",
       "      <th>tweet_has_url_hour_entropy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1217628182611927040</th>\n",
       "      <td>2020-01-16 02:02:55+00:00</td>\n",
       "      <td>Theoretical Computer Scientist. See also https...</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>Boaz Barak</td>\n",
       "      <td>https://t.co/BoMip9FF17</td>\n",
       "      <td>boazbaraktcs</td>\n",
       "      <td>human</td>\n",
       "      <td>7316</td>\n",
       "      <td>215</td>\n",
       "      <td>3098</td>\n",
       "      <td>...</td>\n",
       "      <td>39940.0</td>\n",
       "      <td>27598.0</td>\n",
       "      <td>24.208772</td>\n",
       "      <td>35.035088</td>\n",
       "      <td>{'twitter.com': 110, 'horoscoponegro.com': 29,...</td>\n",
       "      <td>{'Aries': 323, 'ARIES': 286, 'aries': 3, 'Part...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664730894</th>\n",
       "      <td>2014-07-02 17:56:46+00:00</td>\n",
       "      <td>creative _</td>\n",
       "      <td>🎈</td>\n",
       "      <td>olawale 💨</td>\n",
       "      <td></td>\n",
       "      <td>wale_io</td>\n",
       "      <td>human</td>\n",
       "      <td>123</td>\n",
       "      <td>1090</td>\n",
       "      <td>1823</td>\n",
       "      <td>...</td>\n",
       "      <td>5154.0</td>\n",
       "      <td>4005.0</td>\n",
       "      <td>4.009009</td>\n",
       "      <td>5.159159</td>\n",
       "      <td>{'twitter.com': 5, 'xkcd.com': 1, 'swag.github...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   created_at  \\\n",
       "id                                              \n",
       "1217628182611927040 2020-01-16 02:02:55+00:00   \n",
       "2664730894          2014-07-02 17:56:46+00:00   \n",
       "\n",
       "                                                           description  \\\n",
       "id                                                                       \n",
       "1217628182611927040  Theoretical Computer Scientist. See also https...   \n",
       "2664730894                                                  creative _   \n",
       "\n",
       "                          location        name                      url  \\\n",
       "id                                                                        \n",
       "1217628182611927040  Cambridge, MA  Boaz Barak  https://t.co/BoMip9FF17   \n",
       "2664730894                       🎈   olawale 💨                            \n",
       "\n",
       "                         username  label  followers_count  following_count  \\\n",
       "id                                                                           \n",
       "1217628182611927040  boazbaraktcs  human             7316              215   \n",
       "2664730894                wale_io  human              123             1090   \n",
       "\n",
       "                     tweet_count  ...  tweet_urls_total tweet_hashtags_total  \\\n",
       "id                                ...                                          \n",
       "1217628182611927040         3098  ...           39940.0              27598.0   \n",
       "2664730894                  1823  ...            5154.0               4005.0   \n",
       "\n",
       "                    avg_hashtags_in_tweet avg_urls_in_tweet  \\\n",
       "id                                                            \n",
       "1217628182611927040             24.208772         35.035088   \n",
       "2664730894                       4.009009          5.159159   \n",
       "\n",
       "                                                      tweet_urls_top_x  \\\n",
       "id                                                                       \n",
       "1217628182611927040  {'twitter.com': 110, 'horoscoponegro.com': 29,...   \n",
       "2664730894           {'twitter.com': 5, 'xkcd.com': 1, 'swag.github...   \n",
       "\n",
       "                                                  tweet_hashtags_top_x  \\\n",
       "id                                                                       \n",
       "1217628182611927040  {'Aries': 323, 'ARIES': 286, 'aries': 3, 'Part...   \n",
       "2664730894                                                          {}   \n",
       "\n",
       "                    tweet_has_hashtag_weekday_entropy  \\\n",
       "id                                                      \n",
       "1217628182611927040                               NaN   \n",
       "2664730894                                        NaN   \n",
       "\n",
       "                     tweet_has_hashtag_hour_entropy  \\\n",
       "id                                                    \n",
       "1217628182611927040                             NaN   \n",
       "2664730894                                      NaN   \n",
       "\n",
       "                     tweet_has_url_weekday_entropy  tweet_has_url_hour_entropy  \n",
       "id                                                                              \n",
       "1217628182611927040                            NaN                         NaN  \n",
       "2664730894                                     NaN                         NaN  \n",
       "\n",
       "[2 rows x 55 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved selected indices at /dataset/twibot22/generated_data/selected.csv. Loading...\n",
      "Loaded 100000 selected samples from saved file.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def stratify_samples(df, label_col='label', target_col='sampled_post_count', strategy='oversample'):\n",
    "    \"\"\"\n",
    "    Stratify samples to handle class imbalances.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input data with labels and target column.\n",
    "        label_col (str): Column name for class labels.\n",
    "        target_col (str): Column name for target data (if needed).\n",
    "        strategy (str): 'oversample' or 'undersample'. Default is 'oversample'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Stratified data.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    bots = df[df[label_col] == 'bot']\n",
    "    humans = df[df[label_col] == 'human']\n",
    "\n",
    "    if strategy == 'oversample':\n",
    "        # Oversample the minority class\n",
    "        if len(bots) < len(humans):\n",
    "            bots = resample(bots, replace=True, n_samples=len(humans), random_state=42)\n",
    "        else:\n",
    "            humans = resample(humans, replace=True, n_samples=len(bots), random_state=42)\n",
    "    elif strategy == 'undersample':\n",
    "        # Undersample the majority class\n",
    "        if len(bots) < len(humans):\n",
    "            humans = resample(humans, replace=False, n_samples=len(bots), random_state=42)\n",
    "        else:\n",
    "            bots = resample(bots, replace=False, n_samples=len(humans), random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be either 'oversample' or 'undersample'\")\n",
    "\n",
    "    # Combine and shuffle\n",
    "    stratified_df = pd.concat([bots, humans]).sample(frac=1, random_state=42)\n",
    "    stratified_df = stratified_df.loc[~stratified_df.index.duplicated()]\n",
    "    return stratified_df\n",
    "\n",
    "def limit_samples_per_label(df, label_col='label', sample_set_size_per_label=1000):\n",
    "    \"\"\"\n",
    "    Limit the number of samples per label in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input data with labels.\n",
    "        label_col (str): Column name for class labels.\n",
    "        sample_set_size_per_label (int): Maximum number of samples per label.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with limited samples per label.\n",
    "    \"\"\"\n",
    "    limited_dfs = []\n",
    "    for label in df[label_col].unique():\n",
    "        label_df = df[df[label_col] == label]\n",
    "        limited_label_df = label_df.sample(n=min(sample_set_size_per_label, len(label_df)), random_state=42)\n",
    "        limited_dfs.append(limited_label_df)\n",
    "\n",
    "    limited_df = pd.concat(limited_dfs).sample(frac=1, random_state=42)\n",
    "    return limited_df.loc[~limited_df.index.duplicated()]\n",
    "\n",
    "\n",
    "\n",
    "def mask_selected_users_by_sampled_post_count(df, count):\n",
    "    return pd.Series(user_detail_data['sampled_post_count'] >= count)\n",
    "\n",
    "\n",
    "def _label_counts(df):\n",
    "    return (df.loc[df['label'] == 'bot'].shape[0], df.loc[df['label'] == 'human'].shape[0])\n",
    "    \n",
    "debug_print(f\"Loading data from parquet at {generated_data_output}\",1)\n",
    "user_detail_data = get_data(\"assembled_user_details\", index='id')\n",
    "\n",
    "debug_print(f\"Loaded parquet. Column names: {user_detail_data.dtypes}\",5)\n",
    "user_detail_data['sampled_post_count'] = user_detail_data['sampled_post_count'].fillna(0).astype('int32')\n",
    "max_sampled_posts = user_detail_data.loc[user_detail_data['sampled_post_count'] > 0, 'sampled_post_count'].max()\n",
    "min_sampled_posts = user_detail_data.loc[user_detail_data['sampled_post_count'] > 0, 'sampled_post_count'].min()\n",
    "\n",
    "debug_print(f\"Min/Max sampled posts on a user: {min_sampled_posts}, ({max_sampled_posts})\", 3)\n",
    "\n",
    "display(user_detail_data.head(2))\n",
    "# ls_selected_indices_output = rf\"{generated_data_output}/selected.csv\"\n",
    "# If we have a saved file specifying the indices of our selected samples, we can just load those and skip the following sections e.g. we are continuing an experiment\n",
    "# If not, continue and then depending on the sample_set_save_selected_indices flag, save the indices for the next run.\n",
    "\n",
    "if os.path.exists(ls_selected_indices_output):\n",
    "    debug_print(f\"Found saved selected indices at {ls_selected_indices_output}. Loading...\", 1)\n",
    "    selected_indices = pd.read_csv(ls_selected_indices_output)\n",
    "    selected_user_data = user_detail_data.loc[selected_indices['selected_indices']]\n",
    "    debug_print(f\"Loaded {len(selected_indices)} selected samples from saved file.\", 5)\n",
    "else:\n",
    "    debug_print(f\"No saved indices found. Proceeding with filtering and selection.\", 1)\n",
    "    \n",
    "    # Randomization\n",
    "    if sample_set_randomization:\n",
    "        user_detail_data = _shuffle(user_detail_data)\n",
    "        print(\"Sample rows randomized.\")\n",
    "\n",
    "    bot_users_count, human_users_count = _label_counts(user_detail_data)\n",
    "    debug_print(f\"{user_detail_data.shape[0]} samples available. {bot_users_count} are bots, and {human_users_count} are humans (labeled).\", 5)\n",
    "\n",
    "    # Filter out samples to users that have at least X number of sampled posts in the post data.\n",
    "    constrained_user_data_mask = mask_selected_users_by_sampled_post_count(user_detail_data, sample_set_constrain_minimum_posts)\n",
    "    selected_user_data = user_detail_data.loc[constrained_user_data_mask] if constrained_user_data_mask is not None else user_detail_data\n",
    "\n",
    "    selected_bot_users_count = selected_user_data.loc[selected_user_data['label'] == 'bot'].shape[0]\n",
    "    selected_human_users_count = selected_user_data.loc[selected_user_data['label'] == 'human'].shape[0]\n",
    "    debug_print(f\"Of these, {selected_user_data.shape[0]} meet sampled minimum post constraints. {selected_bot_users_count} bots meet this constraint, and {selected_human_users_count} humans meet this constraint.\", 5)\n",
    "\n",
    "    # Calculate balance\n",
    "    balance = (selected_bot_users_count / float(selected_human_users_count)) - (selected_human_users_count / float(selected_bot_users_count))\n",
    "    skew = abs(balance * 100)\n",
    "    debug_print(f\"The balance between human and bot users is skewed by about ~{skew}% on the side of {'humans' if balance < 0 else 'bots'}\", 3)\n",
    "\n",
    "    # Stratification\n",
    "    if sample_set_stratification:\n",
    "        selected_user_data = stratify_samples(\n",
    "            selected_user_data,\n",
    "            label_col='label',\n",
    "            target_col='sampled_post_count',\n",
    "            strategy='undersample'  # Change to 'oversample' if preferred\n",
    "        )\n",
    "        stratified_bot_count, stratified_human_count = _label_counts(selected_user_data)\n",
    "        debug_print(f\"Post-stratification: {selected_user_data.shape[0]} samples available. {stratified_bot_count} bots, {stratified_human_count} humans.\", 5)\n",
    "        balance_after = (stratified_bot_count / float(stratified_human_count)) - (stratified_human_count / float(stratified_bot_count))\n",
    "        skew_after = abs(balance_after * 100)\n",
    "        debug_print(f\"Post-stratification skew is ~{skew_after}%.\", 3)\n",
    "\n",
    "    # Truncate data to limit samples per label\n",
    "    bot_users_count, human_users_count = _label_counts(selected_user_data)\n",
    "    sample_set_size_per_label = min(sample_set_size_per_label, min(bot_users_count, human_users_count))\n",
    "    debug_print(f\"Truncating data to sample_set_size_per_label: {sample_set_size_per_label}\", 1)\n",
    "    selected_user_data = limit_samples_per_label(\n",
    "        selected_user_data,\n",
    "        label_col='label',\n",
    "        sample_set_size_per_label=sample_set_size_per_label\n",
    "    )\n",
    "    debug_print(f\"After truncation: {selected_user_data.shape[0]} samples available.\", 5)\n",
    "    limited_bot_count, limited_human_count = _label_counts(selected_user_data)\n",
    "    debug_print(f\"Truncated data contains {limited_bot_count} bots and {limited_human_count} humans.\", 5)\n",
    "\n",
    "    # Save the selected indices if the flag is set\n",
    "    if sample_set_save_selected_indices:\n",
    "        debug_print(f\"Saving selected indices to {ls_selected_indices_output}\", 1)\n",
    "        selected_indices = pd.DataFrame({'selected_indices': selected_user_data.index})\n",
    "        selected_indices.to_csv(ls_selected_indices_output, index=False)\n",
    "        debug_print(f\"Saved {len(selected_user_data)} selected indices to {ls_selected_indices_output}.\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d60594-5c46-4b11-b3a6-3aa0fd83843f",
   "metadata": {},
   "source": [
    "## Step 2: Convert edge file into relationship dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6672e5de-ff08-4484-92da-79295fb39936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading relationship map from /dataset/twibot22/generated_data/gs_output/relationship_map.pkl...\n",
      "Loaded relationship map with 10290934 nodes.\n",
      "Constrinaed Preview (Number of entities in relationship types limited to 10):\n",
      "{\n",
      " \"u251372955\": {\n",
      "  \"11\": \"['u51934480', 'u76966092', 'u1711571520', 'u218379543', 'u187044118', 'u56986684', 'u1008681429423837185', 'u354582942', 'u1244426959322599428', 'u96117173']\",\n",
      "  \"9\": \"['u76966092', 'u51934480', 'u1008681429423837185', 'u56986684', 'u218379543', 'u386538440', 'u899480529296379904', 'u8062702', 'u34373064', 'u14777850']\",\n",
      "  \"12\": \"['t1438602507018719232', 't1359932716863651844', 't1321741120989265920', 't1376208963243507723', 't1442699035178065922', 't1308750685865992203', 't1481429296501739520', 't1335726528253079553', 't1375154351975530498', 't1329492903283073024']\",\n",
      "  \"4\": \"['t1278210024763011073', 't1272197193940770816', 't1271050880532520962', 't1280016344965152768', 't1254289206664454146', 't1273113212867788802', 't1290331795637055489', 't1272445048689475584', 't1283431292458205185', 't1268935739808665600']\"\n",
      " },\n",
      " \"u123231225\": {\n",
      "  \"11\": \"['u1112074849315364865', 'u263838682', 'u1474020494592684034', 'u1338491543053692928', 'u84422136', 'u822298712991035393', 'u3919734737', 'u3100383489', 'u1436240030368894982', 'u1451959161823051778']\",\n",
      "  \"9\": \"['u1046783815937662976', 'u188104515', 'u1019593080188669952', 'u161791703', 'u69181624', 'u1078749802765139968', 'u766072848268091392', 'u55896443', 'u633980404', 'u16421419']\",\n",
      "  \"12\": \"['t1490715946889134084', 't1491769666494210048', 't1490772055175880708', 't1493932998219251713', 't1491407981639979011', 't1490770467543781379', 't1488996708239364097', 't1488948058402533382', 't1490802042599710720', 't1495065986030047236']\",\n",
      "  \"13\": \"['l760541156803432449', 'l1378091769808744453']\",\n",
      "  \"5\": \"['t1433858388358750210']\",\n",
      "  \"4\": \"['t1412884741766926338', 't1450505270224957447', 't1417836916146425858', 't1403758287661588480', 't1466110577315651593', 't1461739415684976651', 't1448730367578550272', 't1430161502141616128', 't1475579443791994883', 't1403426646930210818']\"\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "ENTITY_USER = 0\n",
    "ENTITY_GROUP = 1\n",
    "ENTITY_POST = 2\n",
    "entity_types = { 'u' : ENTITY_USER, 'l': ENTITY_GROUP, 't': ENTITY_POST, 'user': ENTITY_USER, 'group':ENTITY_GROUP, 'post': ENTITY_POST}\n",
    "\n",
    "def strip_id(x):\n",
    "    prefix_map = {'u': 'u', 'l': 'l', 't': 't'}\n",
    "    return int(x[1:]) if x and x[0] in prefix_map else -1\n",
    "\n",
    "def entity_type_from_id(any_str_id):\n",
    "    return entity_types.get(any_str_id[0], -1)\n",
    "    \n",
    "REL_TYPE_DISC = 0\n",
    "REL_TYPE_FOLLWD = 1\n",
    "REL_TYPE_REPLD = 2\n",
    "REL_TYPE_RTD = 3\n",
    "REL_TYPE_POSTED = 4\n",
    "REL_TYPE_PINNED = 5\n",
    "REL_TYPE_MEMB = 6\n",
    "REL_TYPE_QUOTED = 7\n",
    "REL_TYPE_CONTAINS = 8\n",
    "REL_TYPE_FOLLWG = 9\n",
    "REL_TYPE_MENT = 10\n",
    "REL_TYPE_FOLLWR = 11\n",
    "REL_TYPE_LIKED = 12\n",
    "REL_TYPE_OWN = 13\n",
    "\n",
    "relationship_types = {\n",
    "                      'discuss':REL_TYPE_DISC,\n",
    "                      'followed':REL_TYPE_FOLLWD,\n",
    "                      'replied_to':REL_TYPE_REPLD,\n",
    "                      'retweeted':REL_TYPE_RTD,\n",
    "                      'post':REL_TYPE_POSTED,\n",
    "                      'pinned':REL_TYPE_PINNED,\n",
    "                      'membership':REL_TYPE_MEMB,\n",
    "                      'quoted':REL_TYPE_QUOTED,\n",
    "                      'contain':REL_TYPE_CONTAINS,\n",
    "                      'following':REL_TYPE_FOLLWG,\n",
    "                      'mentioned':REL_TYPE_MENT,\n",
    "                      'followers':REL_TYPE_FOLLWR,\n",
    "                      'like':REL_TYPE_LIKED,\n",
    "                      'own':REL_TYPE_OWN\n",
    "                    }\n",
    "\n",
    "\n",
    "def dd():\n",
    "    return defaultdict(set)\n",
    "    \n",
    "def convert_edge_file_to_rel_map(relationship_map_file='relationship_map.pkl'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert edge file to relationship_map\n",
    "    If the relationship_map file exists, load it. Otherwise, parse the edge parquet and save the result.\n",
    "    Args:\n",
    "        relationship_map_file (str): Path to the file to save/load the relationship map.\n",
    "\n",
    "    Returns:\n",
    "        dict: The relationship map.\n",
    "    \"\"\"\n",
    "\n",
    "    global selected_user_data\n",
    "    # Check if the relationship map file exists\n",
    "    if os.path.exists(relationship_map_file):\n",
    "        print(f\"Loading relationship map from {relationship_map_file}...\")\n",
    "        with open(relationship_map_file, 'rb') as f:\n",
    "            relationship_map = pickle.load(f)\n",
    "        print(f\"Loaded relationship map with {len(relationship_map)} nodes.\")\n",
    "        return relationship_map\n",
    "\n",
    "    print(\"Relationship map file not found. Parsing edge parquet to create new map.\")\n",
    "    \n",
    "    relationship_map = defaultdict(dd)\n",
    "    next_pass_targets = set('u'+str(x) for x in selected_user_data.index.values) \n",
    "    print(f\"Total passes scheduled: {gnn_dataset_build_num_pass} on {len(next_pass_targets)} initial target entities.\")\n",
    "    relevant_data = get_data(\"edges\")\n",
    "\n",
    "    try:\n",
    "        for i in range(gnn_dataset_build_num_pass):\n",
    "            print(f\"Pass {i}...\")\n",
    "\n",
    "            sub_pass = [ relevant_data.loc[relevant_data['id1'].isin(next_pass_targets)] ]\n",
    "            next_pass_targets.clear()            \n",
    "            \n",
    "            # src, dest\n",
    "            def graph_dir_pass(rw, k1, k2):                 \n",
    "                relationship_map[rw[k1]][str(relationship_types[rw['relationship']])].add(rw[k2])\n",
    "                next_pass_targets.add(rw[k2])\n",
    "                \n",
    "            sub_pass[0].apply(graph_dir_pass, args=('id1', 'id2'), axis=1)             \n",
    "            print(f\"Entities collected: {len(next_pass_targets)}\")\n",
    "            \n",
    "        edge_counter = sum(len(v) for v in relationship_map.values())\n",
    "        print(\"All passes completed. Nodes collected:\", len(relationship_map), \"Edges collected:\", edge_counter)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Problem collecting relationship edges: {str(e)} Freeing resources.\")\n",
    "        del relevant_data, relationship_map\n",
    "        gc.collect()\n",
    "        raise StopExecution\n",
    "        \n",
    "    # Save the relationship map to file\n",
    "    print(f\"Saving relationship map to {relationship_map_file}...\")\n",
    "    with open(relationship_map_file, 'wb') as f:\n",
    "        pickle.dump(relationship_map, f)\n",
    "    print(\"Relationship map saved successfully.\")\n",
    "    del relevant_data\n",
    "    gc.collect()\n",
    "    return relationship_map\n",
    "\n",
    "\n",
    "gnn_rel_map = convert_edge_file_to_rel_map(relationship_map_file=gnn_output_relmap_name)\n",
    "\n",
    "\n",
    "print(\"Constrinaed Preview (Number of entities in relationship types limited to 10):\")\n",
    "def new_encoder(unknown_var):\n",
    "    if type(unknown_var) is set:\n",
    "        return str(list(unknown_var)[0:10])\n",
    "    if type(unknown_var) is list:\n",
    "        return str(list(unknown_var[0:10]))\n",
    "    return str(unknown_var)\n",
    "print(json.dumps(dict(list(gnn_rel_map.items())[0:2]),indent=1, default=new_encoder))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e56a063-5451-4eba-b119-bf98a6f28fb8",
   "metadata": {},
   "source": [
    "## Step 3: Normalize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d201a620-2198-44e8-8273-b2e00f181b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing user information...\n",
      "Sample user features for graph node embedding (before normalization):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_following_ratio</th>\n",
       "      <th>profile_desc_len</th>\n",
       "      <th>account_age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1217628182611927040</th>\n",
       "      <td>7316</td>\n",
       "      <td>215</td>\n",
       "      <td>69</td>\n",
       "      <td>3098</td>\n",
       "      <td>14.409302</td>\n",
       "      <td>92</td>\n",
       "      <td>158635127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664730894</th>\n",
       "      <td>123</td>\n",
       "      <td>1090</td>\n",
       "      <td>0</td>\n",
       "      <td>1823</td>\n",
       "      <td>1.672477</td>\n",
       "      <td>10</td>\n",
       "      <td>333451496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     followers_count  following_count  listed_count  \\\n",
       "id                                                                    \n",
       "1217628182611927040             7316              215            69   \n",
       "2664730894                       123             1090             0   \n",
       "\n",
       "                     tweet_count  tweet_following_ratio  profile_desc_len  \\\n",
       "id                                                                          \n",
       "1217628182611927040         3098              14.409302                92   \n",
       "2664730894                  1823               1.672477                10   \n",
       "\n",
       "                     account_age  \n",
       "id                                \n",
       "1217628182611927040    158635127  \n",
       "2664730894             333451496  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample user features for graph nodes after normalization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_following_ratio</th>\n",
       "      <th>profile_desc_len</th>\n",
       "      <th>account_age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1217628182611927040</th>\n",
       "      <td>5.601278e-05</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>8.576703e-07</td>\n",
       "      <td>0.290221</td>\n",
       "      <td>0.041333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664730894</th>\n",
       "      <td>9.417129e-07</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>9.954916e-08</td>\n",
       "      <td>0.031546</td>\n",
       "      <td>0.147461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     followers_count  following_count  listed_count  \\\n",
       "id                                                                    \n",
       "1217628182611927040     5.601278e-05         0.000051      0.000128   \n",
       "2664730894              9.417129e-07         0.000262      0.000000   \n",
       "\n",
       "                     tweet_count  tweet_following_ratio  profile_desc_len  \\\n",
       "id                                                                          \n",
       "1217628182611927040     0.000061           8.576703e-07          0.290221   \n",
       "2664730894              0.000036           9.954916e-08          0.031546   \n",
       "\n",
       "                     account_age  \n",
       "id                                \n",
       "1217628182611927040     0.041333  \n",
       "2664730894              0.147461  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting post information...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_7.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_8.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_4.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_1.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_6.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_2.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_5.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_0.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_3.parquet...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1499547298828886016</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499544740148264962</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499542095224360962</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499539485092155402</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499535965089632262</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     quote_count  like_count  retweet_count  reply_count\n",
       "id                                                                      \n",
       "1499547298828886016            0           4              0            0\n",
       "1499544740148264962            0           1              0            0\n",
       "1499542095224360962            0           2              0            0\n",
       "1499539485092155402            0           1              0            0\n",
       "1499535965089632262            0           2              0            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding relevant node IDs...\n",
      "Normalizing...\n",
      "Completed. Sample of normalized thread data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35819845282955264</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.303699e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32800663024959489</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26434597420007424</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24628078877605888</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130691131179009</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34938672109322240</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727225741606913</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32959274892132353</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.612459e-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7940735991021568</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20547389144170496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   quote_count    like_count  retweet_count  reply_count\n",
       "id                                                                      \n",
       "35819845282955264          0.0  3.303699e-07   0.000000e+00          0.0\n",
       "32800663024959489          0.0  0.000000e+00   0.000000e+00          0.0\n",
       "26434597420007424          0.0  0.000000e+00   0.000000e+00          0.0\n",
       "24628078877605888          0.0  0.000000e+00   0.000000e+00          0.0\n",
       "2130691131179009           0.0  0.000000e+00   0.000000e+00          0.0\n",
       "34938672109322240          0.0  0.000000e+00   0.000000e+00          0.0\n",
       "727225741606913            0.0  0.000000e+00   0.000000e+00          0.0\n",
       "32959274892132353          0.0  0.000000e+00   2.612459e-07          0.0\n",
       "7940735991021568           0.0  0.000000e+00   0.000000e+00          0.0\n",
       "20547389144170496          0.0  0.000000e+00   0.000000e+00          0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def datetime_to_age_in_seconds(dt):\n",
    "    if pd.api.types.is_datetime64_any_dtype(dt):\n",
    "        dt = dt.date\n",
    "    if not isinstance(dt, datetime):\n",
    "        raise ValueError(\"The input must be a datetime object.\")\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)  # Correct use of timezone.utc\n",
    "    now = datetime.now(timezone.utc)  # Correct use of timezone.utc\n",
    "    age_in_seconds = int((now - dt).total_seconds())\n",
    "    return age_in_seconds\n",
    "\n",
    "\n",
    "\n",
    "#--------------\n",
    "# User Features\n",
    "#--------------\n",
    "debug_print(\"Normalizing user information...\", 3)\n",
    "target_user_feature_keys = [\n",
    "    'followers_count',\n",
    "    'following_count',\n",
    "    'listed_count',\n",
    "    'tweet_count',\n",
    "    'tweet_following_ratio',\n",
    "    'created_at',\n",
    "    'profile_desc_len'\n",
    "]\n",
    "\n",
    "copied_user_details_for_graph = user_detail_data.loc[:, target_user_feature_keys].copy()\n",
    "copied_user_details_for_graph.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "copied_user_details_for_graph.dropna(inplace=True)\n",
    "copied_user_details_for_graph['account_age'] = copied_user_details_for_graph['created_at'].apply(datetime_to_age_in_seconds)\n",
    "copied_user_details_for_graph.drop('created_at', axis=1, inplace=True)\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "transformed = scaler.fit_transform(copied_user_details_for_graph)\n",
    "normalized_details_for_graph = pd.DataFrame(\n",
    "    transformed, \n",
    "    index=copied_user_details_for_graph.index, \n",
    "    columns=copied_user_details_for_graph.columns\n",
    ")\n",
    "\n",
    "debug_print(\"Sample user features for graph node embedding (before normalization):\", 5)\n",
    "if verbosity >= 5: \n",
    "    display(copied_user_details_for_graph.head(2))\n",
    "\n",
    "debug_print(\"Sample user features for graph nodes after normalization:\", 5)\n",
    "if verbosity >= 5: \n",
    "    display(normalized_details_for_graph.head(2))\n",
    "\n",
    "\n",
    "#--------------\n",
    "# Post Features\n",
    "#--------------\n",
    "\n",
    "debug_print(\"Getting post information...\", 3)\n",
    "relevant_data = get_post_chunks(cols=[\"id\",\"quote_count\",\"like_count\",\"retweet_count\",\"reply_count\"], index=\"id\")\n",
    "display(relevant_data.head(5))\n",
    "\n",
    "relevant_data.fillna({'like_count': 0, 'quote_count': 0, 'retweet_count': 0, 'reply_count': 0}, inplace=True)\n",
    "\n",
    "debug_print(\"Finding relevant node IDs...\", 3)\n",
    "post_ids = [strip_id(node) for node in gnn_rel_map if entity_type_from_id(node) == ENTITY_POST]\n",
    "valid_post_ids = list(set(relevant_data.index).intersection(post_ids))\n",
    "valid_posts = relevant_data.loc[valid_post_ids]\n",
    "\n",
    "print(\"Normalizing...\")\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "transformed = scaler.fit_transform(valid_posts)\n",
    "normalized_thread_feature_data = pd.DataFrame(\n",
    "    transformed, \n",
    "    index=valid_posts.index, \n",
    "    columns=valid_posts.columns\n",
    ")\n",
    "\n",
    "# Convert to a dictionary for `thread_feature_data`\n",
    "#thread_feature_data = normalized_thread_feature_data[['like_count', 'quote_count', 'retweet_count', 'reply_count']].to_dict(orient='index')\n",
    "\n",
    "debug_print(\"Completed. Sample of normalized thread data:\",3)\n",
    "if verbosity >= 5: \n",
    "    display(normalized_thread_feature_data.head(10))\n",
    "\n",
    "\n",
    "del relevant_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ffe09a-0bf9-4caa-aa01-3d17259b2034",
   "metadata": {},
   "source": [
    "# Stage 2: Train Embedded Models \n",
    "### Part A: URL Scoring\n",
    "\n",
    "Bots have a purpose, and that purpose generally means needing to spread external information. Sometimes that means using external URLS. If we put all those URLS people post in a bag, can we spot the patterns in how bots post URLs versus humans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6939a3c7-2cbc-4be5-bf18-52b8d2209274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24123 domains selected for modeling.\n",
      "Accuracy: 0.5719\n",
      "F1 Score: 0.5651\n",
      "ROC AUC: 0.6061\n",
      "Model saved to /dataset/twibot22/generated_data/url_model.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability_class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89612950</td>\n",
       "      <td>bot</td>\n",
       "      <td>0.478117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>466835426</td>\n",
       "      <td>bot</td>\n",
       "      <td>0.495663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>429208947</td>\n",
       "      <td>human</td>\n",
       "      <td>0.500569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1181476422327689216</td>\n",
       "      <td>human</td>\n",
       "      <td>0.518565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1381627326753038341</td>\n",
       "      <td>human</td>\n",
       "      <td>0.516660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1486417203092148227</td>\n",
       "      <td>bot</td>\n",
       "      <td>0.489038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>372830746</td>\n",
       "      <td>human</td>\n",
       "      <td>0.552072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>44653376</td>\n",
       "      <td>human</td>\n",
       "      <td>0.515908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>2337682316</td>\n",
       "      <td>human</td>\n",
       "      <td>0.536984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>237083435</td>\n",
       "      <td>bot</td>\n",
       "      <td>0.489038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   user_id prediction  probability_class_1\n",
       "0                 89612950        bot             0.478117\n",
       "1                466835426        bot             0.495663\n",
       "2                429208947      human             0.500569\n",
       "3      1181476422327689216      human             0.518565\n",
       "4      1381627326753038341      human             0.516660\n",
       "...                    ...        ...                  ...\n",
       "99995  1486417203092148227        bot             0.489038\n",
       "99996            372830746      human             0.552072\n",
       "99997             44653376      human             0.515908\n",
       "99998           2337682316      human             0.536984\n",
       "99999            237083435        bot             0.489038\n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib  # For saving and loading models\n",
    "\n",
    "class URLConfidenceModel:\n",
    "    def __init__(self, exclude_domains=None, domain_bag_min_frequency=2, domain_bag_max_domains=20):\n",
    "        self.exclude_domains = exclude_domains or {\"twitter.com\", \"www.twitter.com\", \"t.co\"}\n",
    "        self.domain_bag_min_frequency = domain_bag_min_frequency\n",
    "        self.domain_bag_max_domains = domain_bag_max_domains\n",
    "        self.domain_index = None\n",
    "        self.model = None\n",
    "\n",
    "    def preprocess_data(self, data, one_hot_encode=True):\n",
    "        if not one_hot_encode:\n",
    "            raise NotImplementedError(\"Only one-hot encoding is supported in this version.\")\n",
    "\n",
    "        domain_counts = Counter(\n",
    "            domain\n",
    "            for row in data['tweet_urls_top_x'].apply(dict)\n",
    "            for domain, freq in row.items()\n",
    "        )\n",
    "\n",
    "        filtered_domains = {\n",
    "            domain for domain, count in domain_counts.items()\n",
    "            if count >= self.domain_bag_min_frequency and domain not in self.exclude_domains\n",
    "        }\n",
    "\n",
    "        print(f\"{len(filtered_domains)} domains selected for modeling.\")\n",
    "\n",
    "        def calculate_domain_frequencies(row):\n",
    "            domain_data = {}\n",
    "            idx = 0\n",
    "            for domain, freq in row.items():\n",
    "                if domain in filtered_domains:\n",
    "                    domain_data[domain] = {'frequency': freq}\n",
    "                    idx += 1\n",
    "                    if idx >= self.domain_bag_max_domains:\n",
    "                        break\n",
    "            return domain_data\n",
    "\n",
    "        X_one_hot_dict = data['tweet_urls_top_x'].apply(calculate_domain_frequencies)\n",
    "\n",
    "        attributes = ['frequency']\n",
    "        self.domain_index = {\n",
    "            (domain, attr): idx\n",
    "            for idx, (domain, attr) in enumerate(\n",
    "                (domain, attr) for domain in filtered_domains for attr in attributes\n",
    "            )\n",
    "        }\n",
    "\n",
    "        rows, cols, values = [], [], []\n",
    "        for row_idx, domain_data in enumerate(X_one_hot_dict):\n",
    "            for domain, features in domain_data.items():\n",
    "                for attr, value in features.items():\n",
    "                    col_idx = self.domain_index[(domain, attr)]\n",
    "                    rows.append(row_idx)\n",
    "                    cols.append(col_idx)\n",
    "                    values.append(value)\n",
    "\n",
    "        X_sparse = csr_matrix((values, (rows, cols)), shape=(len(X_one_hot_dict), len(self.domain_index)))\n",
    "        return X_sparse\n",
    "\n",
    "    def train_model(self, X, y, refit=False, param_distributions=None, refit_num_iter=20):\n",
    "        base_params = {\n",
    "            'bootstrap': False,\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        if refit:\n",
    "            param_distributions = param_distributions or {\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'n_estimators': [300, 350],\n",
    "                'max_depth': [30, 20],\n",
    "                'min_samples_split': [5, None],\n",
    "                'min_samples_leaf': [2, None],\n",
    "                'max_features': [\"sqrt\", None],\n",
    "                'class_weight': ['balanced', None],\n",
    "            }\n",
    "\n",
    "            clf = RandomForestClassifier(**base_params)\n",
    "            random_search = RandomizedSearchCV(\n",
    "                clf, param_distributions=param_distributions, n_iter=refit_num_iter,\n",
    "                scoring='f1_weighted', n_jobs=3, cv=3\n",
    "            )\n",
    "            random_search.fit(X, y)\n",
    "            self.model = random_search.best_estimator_\n",
    "            print(f\"Best parameters: {random_search.best_params_}\")\n",
    "        else:\n",
    "            params = {\n",
    "                'n_estimators': 300,\n",
    "                'min_samples_split': 15,\n",
    "                'min_samples_leaf': 2,\n",
    "                'max_features': \"sqrt\",\n",
    "                'max_depth': 30,\n",
    "                'class_weight': 'balanced',\n",
    "                'criterion': 'entropy'\n",
    "            }\n",
    "            self.model = RandomForestClassifier(random_state=42, **params)\n",
    "            self.model.fit(X, y)\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_prob = self.model.predict_proba(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        roc_auc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "     \n",
    "        return acc, f1, roc_auc\n",
    "\n",
    "    def make_predictions(self, X, user_ids, output_file=None):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model and return associated user IDs, binary predictions,\n",
    "        and probability scores for class '1'.\n",
    "    \n",
    "        Args:\n",
    "            X (csr_matrix): Preprocessed feature matrix.\n",
    "            user_ids (list or array-like): List of user IDs corresponding to the rows in X.\n",
    "    \n",
    "        Returns:\n",
    "            DataFrame: A DataFrame containing user IDs, binary predictions, and probability scores.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been trained or loaded. Please train or load a model before making predictions.\")\n",
    "        \n",
    "        # Predict class probabilities and binary labels\n",
    "        y_prob = self.model.predict_proba(X)[:, 1]  # Probability scores for class '1'\n",
    "        y_pred = self.model.predict(X)  # Binary predictions\n",
    "    \n",
    "        # Create a DataFrame with results\n",
    "        results = pd.DataFrame({\n",
    "            'user_id': user_ids,\n",
    "            'prediction': y_pred,\n",
    "            'probability_class_1': y_prob\n",
    "        })\n",
    "\n",
    "        if output_file is not None:\n",
    "            results.to_csv(output_file, index=False)\n",
    "        return results\n",
    "        \n",
    "    def save_model(self, filepath):\n",
    "        joblib.dump(self.model, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        self.model = joblib.load(filepath)\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "    def plot_feature_importances(self, top_n=25):\n",
    "        importances = self.model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        feature_names = [f\"{domain}_{attr}\" for domain, attr in self.domain_index.keys()]\n",
    "\n",
    "        top_indices = [idx for idx in indices if \"frequency\" in feature_names[idx]][:top_n]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(f\"Top {top_n} Feature Importances\", fontsize=14)\n",
    "        plt.barh(range(len(top_indices)), importances[top_indices][::-1], color=\"b\", align=\"center\")\n",
    "        plt.yticks(range(len(top_indices)), [feature_names[i] for i in top_indices[::-1]])\n",
    "        plt.xlabel(\"Weight Coefficient\", fontsize=12)\n",
    "        plt.ylabel(\"Feature\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "model = URLConfidenceModel()\n",
    "load_url_model_from_file = False\n",
    "\n",
    "X_lab = selected_user_data.index.array\n",
    "X_sparse = model.preprocess_data(selected_user_data)\n",
    "y = selected_user_data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sparse, y, test_size=0.2, stratify=y)\n",
    "\n",
    "if load_url_model_from_file:\n",
    "    model.load_model(url_model_output)\n",
    "else:   \n",
    "    model.train_model(X_train, y_train)\n",
    "    model.evaluate_model(X_test, y_test)\n",
    "    model.save_model(url_model_output)\n",
    "\n",
    "model.make_predictions(X_sparse,selected_user_data.index.array,output_file=url_model_prediction_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e830729-5a88-46e3-95bb-1bf9daa7eb38",
   "metadata": {},
   "source": [
    "# Stage 2 Part B: Relationship Modeling\n",
    "\n",
    "There are patterns in how bots choose to reply to certain tweets, keywords, or users. If we see these interactions as graphed relationships, can we find patterns?\n",
    "\n",
    "## Warnings: CUDA USE (if it exists)\n",
    "\n",
    "## Step 1: Import torch, prepare constants, prepare cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c70af02c-c9f9-4925-a6c4-3b62d79db376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from cogdl.experiments import experiment, output_results\n",
    "import torch\n",
    "import os\n",
    "from cogdl import experiment\n",
    "from cogdl.datasets import build_dataset\n",
    "from cogdl.models import build_model\n",
    "from cogdl.options import get_default_args\n",
    "\n",
    "\n",
    "\n",
    "dataset = None\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"PYTORCH_DEBUG\"] = \"1\"\n",
    "\n",
    "experiment_params =  {\n",
    "                'epochs':200,\n",
    "                'do_valid':False,\n",
    "                'gnn_output_checkpoint_name':gnn_output_checkpoint_name,\n",
    "                'n_trials':2,\n",
    "                'hidden_size':[175],\n",
    "                'batch_size':1024,\n",
    "                #weight_decay=1e-4,\n",
    "                #eval_step=5,\n",
    "                'lr':0.01,\n",
    "                'patience':20,\n",
    "                'dropout':0.4,               # Reduced dropout for large graphs\n",
    "                'sample_size':[15, 10],      # Increased neighbors sampled\n",
    "                'num_layers':2,              # Deeper model\n",
    "            }\n",
    "\n",
    "graphsage_run_params = {\n",
    "        'dropout':0.3,               # Reduced dropout for large graphs\n",
    "        'sample_size':[15, 10],      # Increased neighbors sampled        \n",
    "        'hidden_size':[175],    # Hidden feature dimension\n",
    "        'num_layers':2,      # Number of layers\n",
    "        'aggr':'mean'\n",
    "}\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad179a3-3cc4-44b8-9a07-38f22049c8c2",
   "metadata": {},
   "source": [
    "## Step 2: Convert intermediate format dictionary into CogDL appropriate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116c8cd5-bbf0-4152-b891-10680c1843b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/cogdl/datasets/customized_data.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Statistics:\n",
      " - Number of nodes: 10290934\n",
      " - Number of edges: 17037422\n",
      " - Feature dimension: 8\n",
      " - Edge Attr dimension: torch.Size([17037422])\n",
      " - Number of unique labels: 3\n",
      " - Label distribution: {tensor(-1): tensor(9747710), tensor(0): tensor(465201), tensor(1): tensor(78023)}\n",
      " - Training nodes: 355627\n",
      " - Validation nodes: 147787\n",
      " - Test nodes: 147873\n",
      "Train label distribution: {tensor(0): tensor(279248), tensor(1): tensor(76379)}\n",
      "Validation label distribution: {tensor(0): tensor(93002), tensor(1): tensor(54785)}\n",
      "Test label distribution: {tensor(0): tensor(92951), tensor(1): tensor(54922)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import gc\n",
    "from cogdl.data import Graph\n",
    "from cogdl.datasets import NodeDataset, generate_random_graph\n",
    "from cogdl.utils.graph_utils import to_undirected, remove_self_loops\n",
    "\n",
    "def get_rel_map_graph(relationship_map, normalized_user_features, normalized_thread_features):\n",
    "    \"\"\"\n",
    "    Create a graph representation from relationship map and features.\n",
    "    \n",
    "    Args:\n",
    "        relationship_map (dict): Node relationships where keys are node IDs and values are dictionaries \n",
    "                                 with relationship types and their connected nodes (sets).\n",
    "        normalized_user_features (DataFrame): Pre-normalized user feature data indexed by user ID.\n",
    "        normalized_thread_features (DataFrame): Pre-normalized thread feature data indexed by thread ID.\n",
    "\n",
    "    Returns:\n",
    "        entity_index_map (dict): Mapping from original entity IDs to sequential node indices.\n",
    "        Graph: A CogDL Graph object with features, labels, edges, and edge attributes.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Creating graph...\")\n",
    "\n",
    "    # Precompute sizes\n",
    "    num_nodes = len(relationship_map)\n",
    "    feature_size = max(normalized_user_features.shape[1], normalized_thread_features.shape[1]) + 1  # +1 for entity type\n",
    "    num_edges = sum(len(rel_dict.get(rel, [])) for rel_dict in relationship_map.values() for rel in rel_dict if '_' not in rel)\n",
    "\n",
    "    # Initialize tensors\n",
    "    node_features = torch.full((num_nodes, feature_size), -1.0, dtype=torch.float)\n",
    "    labels = torch.full((num_nodes,), -1, dtype=torch.long)\n",
    "    edges = torch.empty((2, num_edges), dtype=torch.long)\n",
    "    edge_features = torch.empty((num_edges,), dtype=torch.long)\n",
    "\n",
    "    \n",
    "    # Precompute entity index map. Our node IDs should be sequential ints.\n",
    "    entity_index_map = {uid: idx for idx, uid in enumerate(relationship_map.keys())}\n",
    "\n",
    "    # Populate tensors\n",
    "    edge_idx = 0\n",
    "    for node_idx, (uid1, rel_dict) in enumerate(relationship_map.items()):\n",
    "        entity_type = entity_type_from_id(uid1)\n",
    "        stripped_id = strip_id(uid1)\n",
    "\n",
    "        # Assign features and labels based on entity type\n",
    "        if entity_type == ENTITY_USER and stripped_id in normalized_user_features.index:\n",
    "            feature_values = [entity_type] + normalized_user_features.loc[stripped_id].tolist()\n",
    "        elif entity_type == ENTITY_POST and stripped_id in normalized_thread_features.index:\n",
    "            feature_values = [entity_type] + normalized_thread_features.loc[stripped_id].tolist()\n",
    "        else:\n",
    "            feature_values = [-1] * feature_size  # Default values for missing data\n",
    "        \n",
    "        # Pad or truncate to match feature size\n",
    "        feature_values = feature_values[:feature_size] + [-1] * (feature_size - len(feature_values))\n",
    "        node_features[node_idx] = torch.tensor(feature_values, dtype=torch.float)\n",
    "        labels[node_idx] = (\n",
    "            1 if entity_type == ENTITY_USER and stripped_id in user_detail_data.index\n",
    "                 and user_detail_data.loc[stripped_id, 'label'] == 'bot'\n",
    "            else 0 if entity_type == ENTITY_USER and stripped_id in user_detail_data.index\n",
    "                 and user_detail_data.loc[stripped_id, 'label'] == 'human'\n",
    "            else -1\n",
    "        )\n",
    "\n",
    "        # Process edges\n",
    "        for rel, uid_list in rel_dict.items():\n",
    "            if '_' in rel:\n",
    "                continue  # Skip backreferences\n",
    "            valid_uids = [uid2 for uid2 in uid_list if uid2 in entity_index_map]\n",
    "            for uid2 in valid_uids:\n",
    "                edges[0, edge_idx] = node_idx\n",
    "                edges[1, edge_idx] = entity_index_map[uid2]\n",
    "                edge_features[edge_idx] = int(rel)\n",
    "                edge_idx += 1\n",
    "\n",
    "    # Trim unused edge entries\n",
    "    edges = edges[:, :edge_idx]\n",
    "    edge_features = edge_features[:edge_idx]\n",
    "\n",
    "    print(f\"Graph created. Nodes: {num_nodes}, Edges: {edge_idx}\")\n",
    "    return entity_index_map, Graph(\n",
    "        x=node_features,\n",
    "        y=labels,\n",
    "        edge_index=edges,\n",
    "        edge_attr=edge_features\n",
    "    )\n",
    "\n",
    "class LockstepRelDataset(NodeDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        relationship_map=None,\n",
    "        normalized_user_features=None,\n",
    "        normalized_thread_features=None,\n",
    "        path=gnn_output_dataset_name,\n",
    "        index_map_path=\"index_map.json\",\n",
    "        sample_limit=0,\n",
    "        train_ratio=0.6,\n",
    "        val_ratio=0.1,\n",
    "        stratify=True,\n",
    "        stratification_mode=\"undersample\",  # \"undersample\" or \"oversample\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A custom NodeDataset class for creating and managing graph datasets.\n",
    "\n",
    "        Args:\n",
    "            relationship_map (dict): A map defining the relationships between nodes.\n",
    "            path (str): Path to save/load the graph data.\n",
    "            index_map_path (str): Path to save/load the node index mapping.\n",
    "            sample_limit (int): Limit the number of samples to process (0 for no limit).\n",
    "            train_ratio (float): Proportion of nodes used for training (0 < train_ratio < 1).\n",
    "            val_ratio (float): Proportion of nodes used for validation (0 < val_ratio < 1).\n",
    "            stratify (bool): Whether to stratify the dataset based on class labels.\n",
    "            stratification_mode (str): \"undersample\" to downsample the majority class, \n",
    "                                       \"oversample\" to upsample the minority class.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.index_map_path = index_map_path\n",
    "        self.sample_limit = sample_limit\n",
    "        self.relationship_map = relationship_map\n",
    "        self.index_mapping = None\n",
    "        self.data_cached = None\n",
    "        self.normalized_thread_features = normalized_thread_features\n",
    "        self.normalized_user_features = normalized_user_features\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.stratify = stratify\n",
    "        self.stratification_mode = stratification_mode\n",
    "\n",
    "        if relationship_map is None and not os.path.exists(path):\n",
    "            raise ValueError(\"Relationship map must be provided if data at the path does not exist.\")\n",
    "\n",
    "        if train_ratio + val_ratio >= 1.0:\n",
    "            raise ValueError(\"The sum of train_ratio and val_ratio must be less than 1.0.\")\n",
    "\n",
    "        super(LockstepRelDataset, self).__init__(path, scale_feat=False, metric=\"accuracy\")\n",
    "\n",
    "    def stratify_indices(self, valid_indices, labels, ignore_labels=None):\n",
    "        \"\"\"\n",
    "        Stratify the dataset by balancing class distributions, with an option to ignore specific labels.\n",
    "    \n",
    "        Args:\n",
    "            valid_indices (Tensor): Indices of nodes with valid labels.\n",
    "            labels (Tensor): Labels corresponding to the nodes.\n",
    "            ignore_labels (list, optional): List of labels to ignore during stratification.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: Stratified indices.\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        from sklearn.utils import resample\n",
    "    \n",
    "        # Default to an empty list if no labels to ignore are specified\n",
    "        if ignore_labels is None:\n",
    "            ignore_labels = []\n",
    "    \n",
    "        # Filter out indices corresponding to ignored labels\n",
    "        valid_mask = ~torch.isin(labels, torch.tensor(ignore_labels))\n",
    "        valid_indices = valid_indices[valid_mask]\n",
    "        labels = labels[valid_mask]\n",
    "    \n",
    "        # Group indices by class\n",
    "        class_indices = {label.item(): valid_indices[labels == label] for label in torch.unique(labels)}\n",
    "        min_class_size = min(len(indices) for indices in class_indices.values())\n",
    "        max_class_size = max(len(indices) for indices in class_indices.values())\n",
    "    \n",
    "        # Apply stratification\n",
    "        stratified_indices = []\n",
    "        for label, indices in class_indices.items():\n",
    "            if self.stratification_mode == \"undersample\":\n",
    "                stratified_indices.append(indices[torch.randperm(len(indices))[:min_class_size]])\n",
    "            elif self.stratification_mode == \"oversample\":\n",
    "                extra_indices = indices[torch.randint(len(indices), (max_class_size - len(indices),))]\n",
    "                stratified_indices.append(torch.cat([indices, extra_indices], dim=0))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid stratification_mode. Choose 'undersample' or 'oversample'.\")\n",
    "    \n",
    "        return torch.cat(stratified_indices, dim=0)\n",
    "        \n",
    "    def load_index_map(self):\n",
    "        \"\"\"\n",
    "        Load the index mapping from the file specified in `index_map_path`.\n",
    "\n",
    "        Returns:\n",
    "            dict: The loaded index mapping.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.index_map_path):\n",
    "            raise FileNotFoundError(f\"Index map file not found at {self.index_map_path}\")\n",
    "        \n",
    "        print(f\"Loading index mapping from {self.index_map_path}...\")\n",
    "        with open(self.index_map_path, 'r') as f:\n",
    "            self.index_mapping = json.load(f)\n",
    "        print(f\"Index mapping loaded successfully. Total entries: {len(self.index_mapping)}\")\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Process the relationship map to generate graph data and index mapping.\n",
    "\n",
    "        Returns:\n",
    "            Graph: The processed graph object with node features, edges, and masks.\n",
    "        \"\"\"\n",
    "        do_save = False\n",
    "\n",
    "        # Load existing data if available\n",
    "        if os.path.exists(self.path):\n",
    "            print(f\"Loading graph data from {self.path}\")\n",
    "            data = torch.load(self.path)\n",
    "            self.load_index_map()\n",
    "        else:\n",
    "           \n",
    "            # Create graph data from the relationship map\n",
    "            if self.relationship_map is None:\n",
    "                raise ValueError(\"Relationship map must be provided to generate graph data.\")\n",
    "\n",
    "            print(\"Processing relationship map to create graph...\")\n",
    "            do_save = True\n",
    "            index_map, data = get_rel_map_graph(\n",
    "                self.relationship_map,\n",
    "                self.normalized_user_features,\n",
    "                self.normalized_thread_features\n",
    "            )\n",
    "            self.index_mapping = index_map\n",
    "\n",
    "            # Save the index mapping\n",
    "            with open(self.index_map_path, 'w') as f:\n",
    "                json.dump(self.index_mapping, f)\n",
    "            print(f\"Index mapping saved to {self.index_map_path}\")\n",
    "\n",
    "        # Cache data\n",
    "        if self.data_cached is not None:\n",
    "            print(\"Using cached data.\")\n",
    "            return self.data_cached\n",
    "\n",
    "        print(\"Sampling graph...\")\n",
    "        # Masks for training, validation, and testing\n",
    "        num_nodes = data.num_nodes\n",
    "        labels = data.y\n",
    "        valid_indices = (labels != -1).nonzero(as_tuple=True)[0]  # Nodes with valid labels\n",
    "\n",
    "        if self.stratify:\n",
    "            valid_indices = self.stratify_indices(valid_indices, labels[valid_indices], [-1])\n",
    "\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "        # Random sampling of valid nodes\n",
    "        valid_indices = valid_indices[torch.randperm(len(valid_indices))]  # Shuffle valid indices\n",
    "        num_valid = len(valid_indices)\n",
    "\n",
    "        train_size = int(self.train_ratio * num_valid)  # Training nodes\n",
    "        val_size = int(self.val_ratio * num_valid)      # Validation nodes\n",
    "        test_size = num_valid - train_size - val_size   # Test nodes\n",
    "\n",
    "        train_mask[valid_indices[:train_size]] = True\n",
    "        val_mask[valid_indices[train_size:train_size + val_size]] = True\n",
    "        test_mask[valid_indices[train_size + val_size:]] = True\n",
    "\n",
    "        # Assign masks to the graph data\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data.test_mask = test_mask\n",
    "\n",
    "        if do_save:\n",
    "            print(f\"Saving graph data to {self.path}\")\n",
    "            p = Path(self.path).parents[0]\n",
    "            if not p.exists:\n",
    "                os.mkdir(p)\n",
    "            torch.save(data, self.path)\n",
    "\n",
    "        self.data_cached = data\n",
    "        gc.collect()\n",
    "        return data\n",
    "\n",
    "\n",
    "def print_graph_statistics(data):\n",
    "    print(\"Graph Statistics:\")\n",
    "    print(f\" - Number of nodes: {data.num_nodes}\")\n",
    "    num_edges = len(data.edge_index[0]) if isinstance(data.edge_index, tuple) else data.edge_index.shape[1]\n",
    "    print(f\" - Number of edges: {num_edges}\")\n",
    "    print(f\" - Feature dimension: {data.x.shape[1]}\")\n",
    "    print(f\" - Edge Attr dimension: {data.edge_attr.shape}\")\n",
    "    print(f\" - Number of unique labels: {data.y.unique().numel()}\")\n",
    "    print(f\" - Label distribution: {dict(zip(*torch.unique(data.y, return_counts=True)))}\")\n",
    "    print(f\" - Training nodes: {data.train_mask.sum().item()}\")\n",
    "    print(f\" - Validation nodes: {data.val_mask.sum().item()}\")\n",
    "    print(f\" - Test nodes: {data.test_mask.sum().item()}\")\n",
    "\n",
    "def print_label_distribution(data):\n",
    "    train_labels = data.y[data.train_mask]\n",
    "    val_labels = data.y[data.val_mask]\n",
    "    test_labels = data.y[data.test_mask]\n",
    "\n",
    "    print(\"Train label distribution:\", dict(zip(*torch.unique(train_labels, return_counts=True))))\n",
    "    print(\"Validation label distribution:\", dict(zip(*torch.unique(val_labels, return_counts=True))))\n",
    "    print(\"Test label distribution:\", dict(zip(*torch.unique(test_labels, return_counts=True))))\n",
    "\n",
    "if gnn_data_load_from_file:\n",
    "    print(\"Loading dataset from file...\")\n",
    "    try:\n",
    "        dataset = LockstepRelDataset(path=gnn_output_dataset_name,\n",
    "                                     index_map_path=gnn_output_map_name,\n",
    "                                     train_ratio=0.6,\n",
    "                                     val_ratio=0.2,\n",
    "                                     stratify=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load from file...{e}\")\n",
    "else:\n",
    "    dataset = LockstepRelDataset(relationship_map=gnn_rel_map,\n",
    "                                 normalized_thread_features=normalized_thread_feature_data,\n",
    "                                 normalized_user_features=normalized_details_for_graph,\n",
    "                                 stratification_mode=\"oversample\",\n",
    "                                 train_ratio=0.6,\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify=True)\n",
    "\n",
    "print_graph_statistics(dataset.data)\n",
    "print_label_distribution(dataset.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dd0f4-83d4-4e2d-81f4-869ba6f8a118",
   "metadata": {},
   "source": [
    "## STEP 6. Train GraphSAGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ca465-a43d-46ab-87f4-24393e0e1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gnn_output_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85077fa1-bb04-4af8-a22e-4cdad39cbc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if gnn_doing_experiment:\n",
    "    try:\n",
    "        # Run the experiment with GPU configuration\n",
    "        results = experiment(\n",
    "            dataset=dataset,\n",
    "            model=gnn_model_type,\n",
    "            **experiment_params,\n",
    "            devices=[device]  # Pass device explicitly\n",
    "        )\n",
    "\n",
    "        # Save the model checkpoint to CPU for portability\n",
    "        #model = results[1]  # Assuming the model is returned as the second item\n",
    "        #torch.save(model.to(\"cpu\").state_dict(), gnn_output_model_path)\n",
    "        output_results(results)\n",
    "        print(\"Experiment results:\", results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Problem encountered while training:\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "003a1034-a932-4535-9e10-70aac2e91b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1500526/1873458489.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(gnn_output_checkpoint_name)\n",
      "/tmp/ipykernel_1500526/1545586940.py:207: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading graph data from /dataset/twibot22/generated_data/gs_output/gnn_graph_data.pt\n",
      "Loading index mapping from /dataset/twibot22/generated_data/gs_output/index_map.json...\n",
      "Index mapping loaded successfully. Total entries: 10290934\n",
      "Sampling graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered predictions saved to '/dataset/twibot22/generated_data/gs_output/gs_predictions.csv'\n",
      "Test Accuracy: 0.6745\n",
      "Node ID: 0 | Feature: tensor([0.0000e+00, 6.7145e-06, 1.6582e-04, 1.4090e-04, 1.6425e-04, 7.1375e-07,\n",
      "        4.7634e-01, 2.1228e-01]) | Label: 1\n",
      "Node ID: 2 | Feature: tensor([0.0000e+00, 3.5892e-05, 2.0812e-04, 3.1887e-04, 4.2004e-05, 1.4548e-07,\n",
      "        4.0063e-01, 2.3310e-01]) | Label: 0\n",
      "Node ID: 3 | Feature: tensor([0.0000e+00, 1.2587e-05, 9.2285e-05, 7.9718e-05, 6.5434e-05, 5.1034e-07,\n",
      "        2.0820e-01, 2.2603e-01]) | Label: 0\n",
      "Node ID: 4 | Feature: tensor([0.0000e+00, 5.7062e-05, 3.1002e-05, 2.1505e-04, 5.5107e-06, 1.2729e-07,\n",
      "        4.9211e-01, 3.6093e-02]) | Label: 1\n",
      "Node ID: 6 | Feature: tensor([0.0000e+00, 1.1607e-05, 1.1632e-04, 4.0786e-05, 1.2865e-05, 7.9649e-08,\n",
      "        2.9968e-01, 9.0938e-02]) | Label: 0\n",
      "Feature 0 - Correlation with Label: -0.9366\n",
      "Feature 1 - Correlation with Label: -0.0468\n",
      "Feature 2 - Correlation with Label: -0.0462\n",
      "Feature 3 - Correlation with Label: -0.0473\n",
      "Feature 4 - Correlation with Label: -0.0474\n",
      "Feature 5 - Correlation with Label: 0.9486\n",
      "Feature 6 - Correlation with Label: 0.9298\n",
      "Feature 7 - Correlation with Label: 0.9414\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from cogdl.models.nn.gcn import GCN\n",
    "from cogdl.models.nn.dropedge_gcn import DropEdge_GCN\n",
    "from cogdl.models.nn.graphsage import Graphsage  # Import GraphSAGE\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "def strip_model_root(chkp):\n",
    "    new_state_dict = {}\n",
    "    # Clean up state_dict keys if needed\n",
    "    for key, value in chkp.items():\n",
    "        new_key = key.replace('model.', '')  # Remove the 'model.' prefix\n",
    "        new_state_dict[new_key] = value\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "# Model setup based on selection\n",
    "if gnn_model_type == \"gcn\":   \n",
    "    checkpoint = torch.load(gnn_output_checkpoint_name)\n",
    "    print(checkpoint)\n",
    "    \n",
    "    \n",
    "    new_state_dict = strip_model_root(checkpoint)\n",
    "\n",
    "    model = GCN(in_feats=23, hidden_size=64, out_feats=2, num_layers=2, dropout=0.2)\n",
    "\n",
    "    # Load model state\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "elif gnn_model_type == \"dropedge_gcn\":\n",
    "    checkpoint = torch.load(gnn_output_checkpoint_name)\n",
    "\n",
    "    model = DropEdge_GCN(\n",
    "        nfeat=23,                # Input feature dimension\n",
    "        nhid=24,                 # Hidden feature dimension\n",
    "        nclass=2,                # Output feature dimension\n",
    "        nhidlayer=1,             # Number of hidden blocks\n",
    "        dropout=0.3,             # Dropout ratio\n",
    "        baseblock=\"mutigcn\",     # Baseblock type\n",
    "        inputlayer=\"gcn\",        # Input layer type\n",
    "        outputlayer=\"gcn\",       # Output layer type\n",
    "        nbaselayer=1,            # Number of layers in one hidden block\n",
    "        activation=torch.relu,   # Activation function\n",
    "        withbn=False,            # Use batch normalization\n",
    "        withloop=False,          # Use self-feature modeling\n",
    "        aggrmethod=\"default\"     # Aggregation function for baseblock\n",
    "    )\n",
    "\n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "elif gnn_model_type == \"graphsage\":\n",
    "\n",
    "    if os.path.exists(gnn_output_checkpoint_name):\n",
    "        checkpoint = torch.load(gnn_output_checkpoint_name)    \n",
    "        checkpoint = strip_model_root(checkpoint)\n",
    "    \n",
    "    model = Graphsage(\n",
    "        num_features=dataset.data.x.shape[1],       # Input feature dimension\n",
    "        num_classes=2,   \n",
    "        **graphsage_run_params\n",
    "    )\n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "data = dataset.data.to(device)  # Ensure data is on the same device as the model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "ind_map = dataset.index_mapping\n",
    "\n",
    "if ind_map is None:\n",
    "    # needs process()\n",
    "    dataset.process()\n",
    "    ind_map = dataset.index_mapping\n",
    "    if ind_map is None:\n",
    "        # actual problem\n",
    "        raise StopExecution\n",
    "node_to_entity_mapping = {idx: entity_id for entity_id, idx in ind_map.items()}\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "    predicted_labels = output.argmax(dim=1)  # Get the class with the highest score for each node\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    \n",
    "    # Filter out nodes with label -1 (thread nodes)\n",
    "    valid_node_indices = (data.y != -1).nonzero(as_tuple=True)[0]\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Node ID': valid_node_indices.cpu().numpy(),\n",
    "        'Entity ID': [node_to_entity_mapping[i.item()] for i in valid_node_indices],\n",
    "        'Predicted Label': predicted_labels[valid_node_indices].cpu().numpy(),\n",
    "        'Probability Class 0': probabilities[valid_node_indices, 0].cpu().numpy(),\n",
    "        'Probability Class 1': probabilities[valid_node_indices, 1].cpu().numpy()\n",
    "    })\n",
    "    \n",
    "    # Output to a CSV file\n",
    "    predictions_df.to_csv(gnn_output_predictions_name, index=False)\n",
    "    print(f\"Filtered predictions saved to '{gnn_output_predictions_name}'\")\n",
    " \n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "true_labels = data.y\n",
    "\n",
    "# Example: Calculate accuracy on the test set\n",
    "correct = (predicted_labels[data.test_mask] == true_labels[data.test_mask]).sum().item()\n",
    "total = data.test_mask.sum().item()\n",
    "accuracy = correct / total if total != 0 else 0\n",
    "\n",
    "# Print the accuracy on the test set\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "train_nodes = data.train_mask.nonzero(as_tuple=True)[0]  # Get indices of training nodes\n",
    "\n",
    "# Example: print the first 5 training nodes with their features and labels\n",
    "for idx in train_nodes[:5]:\n",
    "    feature = data.x[idx]  # Features of the node\n",
    "    label = data.y[idx]    # Label of the node\n",
    "    print(f\"Node ID: {idx.item()} | Feature: {feature} | Label: {label.item()}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Convert features and labels to numpy arrays for correlation\n",
    "features_np = data.x.cpu().numpy()  # Move features to CPU if on GPU\n",
    "labels_np = data.y.cpu().numpy()    # Similarly move labels to CPU if needed\n",
    "\n",
    "# Calculate correlation of each feature with the label\n",
    "correlations = [np.corrcoef(features_np[:, i], labels_np)[0, 1] for i in range(features_np.shape[1])]\n",
    "\n",
    "# Print correlations\n",
    "for i, corr in enumerate(correlations):\n",
    "    print(f\"Feature {i} - Correlation with Label: {corr:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e126a4a-b585-472f-aa12-dfea6d8dfb6d",
   "metadata": {},
   "source": [
    "## End Training Part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ca81d-3ff7-4086-a544-37b644431ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
