{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287cbec8-1d22-4de4-98d7-fc89a2707f03",
   "metadata": {},
   "source": [
    "# Lockstep, the full recipe, Part 2: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f1cfb3-7b76-4d0b-b10c-17c4c79403ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "import pandas as pd\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from fastparquet import ParquetFile\n",
    "\n",
    "verbosity = 5\n",
    "\n",
    "twibot_path = r\"/dataset/twibot22\"\n",
    "twibot_user = r\"/dataset/twibot22/user.json\"\n",
    "twibot_label = r\"/dataset/twibot22/label.csv\"\n",
    "twibot_graph_file = f\"{twibot_path}/edge.csv\"\n",
    "\n",
    "\n",
    "concurrent_max_workers = 2\n",
    "\n",
    "# Files in the path specified by twibot_path, that begin with %twibot_node_identifier_str%, will be assumed as node files and converted if needed.\n",
    "twibot_node_identifier_str = \"tweet_\" \n",
    "\n",
    "generated_data_output = r\"/dataset/twibot22/generated_data\" # output is saved in this directory\n",
    "ls_userdata_output = rf\"{generated_data_output}/userdata.jsonl\" # the desired filename of bot detail output\n",
    "url_model_output = rf\"{generated_data_output}/url_model.pkl\"\n",
    "test_constraint_minimum_sample_posts = 25\n",
    "\n",
    "sample_set_size_per_label = 50000 # per label, sample this many users\n",
    "sample_set_stratification = True # If at any point during selection our set becomes unbalanced, should we stratify?\n",
    "sample_set_sampling_strategy = (1,0) # (1over/0under, 1major/0minor)\n",
    "\n",
    "graph_sampling_depth = 3\n",
    "\n",
    "scores = {}\n",
    "\n",
    "NODE_FILE_LIST = list(filter(lambda fileName: twibot_node_identifier_str in fileName, \n",
    "                                        [child.name for child in Path(generated_data_output).iterdir()]))\n",
    "def debug_print(m, level=5, r=None):\n",
    "    if level <= verbosity:\n",
    "        print(m)\n",
    "        if r:\n",
    "            raise r\n",
    "    \n",
    "def is_data(name, _dir=generated_data_output):\n",
    "    file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "    return os.path.exists(file_path)\n",
    "    \n",
    "def get_data(name, _dir=generated_data_output, pqargs={}, **kwargs):\n",
    "    if is_data(name, _dir):\n",
    "        file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "        print(f\"Loading existing data from {file_path}\")\n",
    "        #return pd.read_parquet(file_path)\n",
    "        pf = ParquetFile(file_path, **pqargs)\n",
    "        return pf.to_pandas(**kwargs)\n",
    "    return False\n",
    "        \n",
    "def save_data(name, _dir=generated_data_output, df=None, **kwargs):\n",
    "    if df is None:\n",
    "            raise ValueError(\"No dataframe provided to save.\")\n",
    "    file_path = os.path.join(_dir, f\"{name}.parquet\")\n",
    "    print(f\"Saving data to {file_path}\")\n",
    "    os.makedirs(_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    df.to_parquet(file_path, **kwargs)\n",
    "    return df      \n",
    "    \n",
    "def _shuffle(df):\n",
    "    return df.sample(frac = 1)\n",
    "    \n",
    "shuffle_method = _shuffle\n",
    "\n",
    "# To quietly stop cell execution\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "import json\n",
    "def get_post_counts():\n",
    "    tweetNodeFilesParquet = list(filter(lambda fileName: twibot_node_identifier_str in fileName, \n",
    "                                        [child.name for child in Path(generated_data_output).iterdir()]))\n",
    "    post_count_dict = defaultdict(int)\n",
    "    debug_print(f\"Called: get_post_counts\", 5)\n",
    "    for targetFile in tweetNodeFilesParquet:\n",
    "        targetInput = Path(f\"{generated_data_output}/{targetFile}\")\n",
    "        try:\n",
    "            debug_print(\"Looking in \" + targetInput.__str__(), 5)\n",
    "            pf = ParquetFile(targetInput)\n",
    "            df = pf.to_pandas(columns=['author_id'])\n",
    "            for uid in df['author_id']:\n",
    "                post_count_dict[uid] = post_count_dict[uid] + 1\n",
    "            del pf, df\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            debug_print(f\"Failed to load node parquet: {e}\", 5)\n",
    "            raise RuntimeError(\"Error processing Parquet files.\")\n",
    "    debug_print(f\"Completed: get_post_counts\", 5)\n",
    "    return post_count_dict\n",
    "    \n",
    "def get_post_chunks(cols = '*', index=\"author_id\", pqargs={}, pdkwargs={}, margs={}):\n",
    "    # Result: Dataframe, index with one or more features.\n",
    "    # Index: from users\n",
    "    global NODE_FILE_LIST\n",
    "    result_builder = None\n",
    "    for targetFile in NODE_FILE_LIST:\n",
    "        targetInput = Path(f\"{generated_data_output}/{targetFile}\")\n",
    "        debug_print(f\"Extracting from {targetInput.__str__()}...\", 5)\n",
    "        \n",
    "        if cols != '*':\n",
    "            # Set the columns to pull from the parquet, either through pqargs directly or here, through cols\n",
    "            pdkwargs['columns'] = cols\n",
    "            \n",
    "        pdkwargs['index'] = index       \n",
    "        try:\n",
    "            pfinput = ParquetFile(targetInput, **pqargs)    \n",
    "            process_group = pfinput.to_pandas(**pdkwargs)  \n",
    "            result_builder = pd.concat([result_builder, process_group])         \n",
    "        except Exception as e:\n",
    "            debug_print(f\"Failed to load node parquet: {e}\", 5)\n",
    "            raise RuntimeError(\"Error processing Parquet files.\")\n",
    "    return result_builder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b719f-c57e-4fb4-92f1-39ad552695ee",
   "metadata": {},
   "source": [
    "# Lockstep, the full recipe, Part 2: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b5ab4-aae1-4a52-b0fc-b00426fb3e26",
   "metadata": {},
   "source": [
    "## Stage 1: Load resources, print statistics, and prepare for sample selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75492b5b-eb89-48a6-8309-bdc3da9a2cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from parquet at /dataset/twibot22/generated_data\n",
      "Loading existing data from /dataset/twibot22/generated_data/assembled_user_details.parquet\n",
      "Loaded parquet. Column names: created_at                           datetime64[ns, UTC]\n",
      "description                                       object\n",
      "location                                          object\n",
      "name                                              object\n",
      "url                                               object\n",
      "username                                          object\n",
      "label                                             object\n",
      "followers_count                                    int64\n",
      "following_count                                    int64\n",
      "tweet_count                                        int64\n",
      "listed_count                                       int64\n",
      "url.urls                                          object\n",
      "description.urls                                  object\n",
      "description.mentions                              object\n",
      "description.hashtags                              object\n",
      "description.cashtags                              object\n",
      "posts                                             object\n",
      "following_followers_ratio                        float64\n",
      "tweet_followers_ratio                            float64\n",
      "tweet_following_ratio                            float64\n",
      "sampled_post_count                                UInt64\n",
      "profile_desc_len                                   int64\n",
      "profile_username_len                               int64\n",
      "profile_has_location                                bool\n",
      "profile_desc_mentions_count                      float64\n",
      "profile_desc_hashtag_count                       float64\n",
      "profile_desc_url_count                           float64\n",
      "tweet_has_media_ratio                            float64\n",
      "tweet_has_geo_ratio                              float64\n",
      "total_rt                                         float64\n",
      "total_likes                                      float64\n",
      "total_quotes                                     float64\n",
      "average_rt                                       float64\n",
      "average_likes                                    float64\n",
      "average_quotes                                   float64\n",
      "likes_chi                                        float64\n",
      "rts_chi                                          float64\n",
      "likes_zero_ratio                                 float64\n",
      "rts_zero_ratio                                   float64\n",
      "entropy_between_post_times                       float64\n",
      "entropy_between_post_hours                       float64\n",
      "entropy_between_post_days                        float64\n",
      "entropy_between_post_weekdays                    float64\n",
      "tweet_has_hashtags_ratio                         float64\n",
      "tweet_has_urls_ratio                             float64\n",
      "tweet_urls_total                                 float64\n",
      "tweet_hashtags_total                             float64\n",
      "avg_hashtags_in_tweet                            float64\n",
      "avg_urls_in_tweet                                float64\n",
      "tweet_urls_top_x                                  object\n",
      "tweet_hashtags_top_x                              object\n",
      "tweet_has_hashtag_weekday_entropy                float64\n",
      "tweet_has_hashtag_hour_entropy                   float64\n",
      "tweet_has_url_weekday_entropy                    float64\n",
      "tweet_has_url_hour_entropy                       float64\n",
      "dtype: object\n",
      "Min/Max sampled posts on a user: 1, (7071)\n",
      "1000000 samples availiable. 139943 are bots, and 860057 are humans (labelled).\n",
      "Of these,863671 meet sampled minimum post constraints. 93070 bots meet this constraint, and 770601 humans meet this constraint.\n",
      "The balance between human and bot users is skewed by about ~815.9024288467316% on the side of humans\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load user information\n",
    "\n",
    "\n",
    "\n",
    "def mask_selected_users_by_sampled_post_count(df, count):\n",
    "    return pd.Series(user_detail_data['sampled_post_count'] >= count)\n",
    "\n",
    "debug_print(f\"Loading data from parquet at {generated_data_output}\",1)\n",
    "user_detail_data = get_data(\"assembled_user_details\")\n",
    "\n",
    "print(f\"Loaded parquet. Column names: {user_detail_data.dtypes}\")\n",
    "user_detail_data['sampled_post_count'] = user_detail_data['sampled_post_count'].fillna(0).astype('int32')\n",
    "\n",
    "# Ensure you target the column 'sampled_post_count' explicitly\n",
    "max_sampled_posts = user_detail_data.loc[user_detail_data['sampled_post_count'] > 0, 'sampled_post_count'].max()\n",
    "min_sampled_posts = user_detail_data.loc[user_detail_data['sampled_post_count'] > 0, 'sampled_post_count'].min()\n",
    "\n",
    "print(f\"Min/Max sampled posts on a user: {min_sampled_posts}, ({max_sampled_posts})\")\n",
    "\n",
    "# sample_randomization_before_constraints\n",
    "user_detail_data = _shuffle(user_detail_data)\n",
    "bot_users_count = user_detail_data.loc[user_detail_data['label'] == 'bot'].shape[0]\n",
    "human_users_count = user_detail_data.loc[user_detail_data['label'] == 'human'].shape[0]\n",
    "debug_print(f\"{user_detail_data.shape[0]} samples availiable. {bot_users_count} are bots, and {human_users_count} are humans (labelled).\", 5)\n",
    "\n",
    "constrained_user_data_mask = mask_selected_users_by_sampled_post_count(user_detail_data, test_constraint_minimum_sample_posts)\n",
    "\n",
    "selected_user_data = user_detail_data.loc[constrained_user_data_mask] if constrained_user_data_mask is not None else user_detail_data\n",
    "selected_bot_users_count = selected_user_data.loc[selected_user_data['label'] == 'bot'].shape[0]\n",
    "selected_human_users_count = selected_user_data.loc[selected_user_data['label'] == 'human'].shape[0]\n",
    "\n",
    "debug_print(f\"Of these,{selected_user_data.shape[0]} meet sampled minimum post constraints. {selected_bot_users_count} bots meet this constraint, and {selected_human_users_count} humans meet this constraint.\", 5)\n",
    "\n",
    "balance =(selected_bot_users_count / float(selected_human_users_count)) - (selected_human_users_count / float(selected_bot_users_count))\n",
    "# optimally, we only want either side to be +-2x the other side maximum.\n",
    "\n",
    "skew = abs(balance*100)\n",
    "debug_print(f\"The balance between human and bot users is skewed by about ~{skew}% on the side of {'humans' if balance < 0 else 'bots'}\",3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f4fae-ff69-4f8c-874a-32718613147c",
   "metadata": {},
   "source": [
    "## Step 2: Sample stratification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff40834-e933-47a4-a04a-98d2197f645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stratification: 186140 samples available.\n",
      "Stratified data contains 93070 bots and 93070 humans.\n",
      "Post-stratification skew is ~0.0%.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def stratify_samples(df, label_col='label', target_col='sampled_post_count', strategy='oversample'):\n",
    "    \"\"\"\n",
    "    Stratify samples to handle class imbalance.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input data with labels and target column.\n",
    "        label_col (str): Column name for class labels.\n",
    "        target_col (str): Column name for target data (if needed).\n",
    "        strategy (str): 'oversample' or 'undersample'. Default is 'oversample'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Stratified data.\n",
    "    \"\"\"\n",
    "    # Separate classes\n",
    "    bots = df[df[label_col] == 'bot']\n",
    "    humans = df[df[label_col] == 'human']\n",
    "\n",
    "    if strategy == 'oversample':\n",
    "        # Oversample the minority class\n",
    "        if len(bots) < len(humans):\n",
    "            bots = resample(bots, replace=True, n_samples=len(humans), random_state=42)\n",
    "        else:\n",
    "            humans = resample(humans, replace=True, n_samples=len(bots), random_state=42)\n",
    "    elif strategy == 'undersample':\n",
    "        # Undersample the majority class\n",
    "        if len(bots) < len(humans):\n",
    "            humans = resample(humans, replace=False, n_samples=len(bots), random_state=42)\n",
    "        else:\n",
    "            bots = resample(bots, replace=False, n_samples=len(humans), random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be either 'oversample' or 'undersample'\")\n",
    "\n",
    "    # Combine and shuffle\n",
    "    stratified_df = pd.concat([bots, humans]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return stratified_df\n",
    "\n",
    "# Applying stratification\n",
    "selected_user_data_stratified = stratify_samples(\n",
    "    selected_user_data,\n",
    "    label_col='label',\n",
    "    target_col='sampled_post_count',\n",
    "    strategy='undersample'  # Change to 'undersample' if preferred\n",
    ")\n",
    "\n",
    "# Count and balance debug info\n",
    "debug_print(f\"After stratification: {selected_user_data_stratified.shape[0]} samples available.\", 5)\n",
    "stratified_bot_count = selected_user_data_stratified.loc[selected_user_data_stratified['label'] == 'bot'].shape[0]\n",
    "stratified_human_count = selected_user_data_stratified.loc[selected_user_data_stratified['label'] == 'human'].shape[0]\n",
    "\n",
    "debug_print(f\"Stratified data contains {stratified_bot_count} bots and {stratified_human_count} humans.\", 5)\n",
    "balance_after = (stratified_bot_count / float(stratified_human_count)) - (stratified_human_count / float(stratified_bot_count))\n",
    "skew_after = abs(balance_after * 100)\n",
    "debug_print(f\"Post-stratification skew is ~{skew_after}%.\", 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1726aa-0efc-4f54-8c71-acc2cc4113dd",
   "metadata": {},
   "source": [
    "## Step 3: Cut to an appropriate test size! E.g. 10k, 25k, or 50k either label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a16d377-7533-464a-8b2a-ed0fc6d9cabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After limiting: 100000 samples available.\n",
      "Limited data contains 50000 bots and 50000 humans.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cut the samples in the dataframe selected_user_data_stratified\n",
    "# based on the label contained in the 'label' column, limiting the samples for each label to sample_set_size_per_label\n",
    "# Limit the samples for each label\n",
    "def limit_samples_per_label(df, label_col='label', sample_set_size_per_label=1000):\n",
    "    \"\"\"\n",
    "    Limit the number of samples per label in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input data with labels.\n",
    "        label_col (str): Column name for class labels.\n",
    "        sample_set_size_per_label (int): Maximum number of samples per label.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with limited samples per label.\n",
    "    \"\"\"\n",
    "    limited_dfs = []\n",
    "    for label in df[label_col].unique():\n",
    "        label_df = df[df[label_col] == label]\n",
    "        limited_label_df = label_df.sample(n=min(sample_set_size_per_label, len(label_df)), random_state=42)\n",
    "        limited_dfs.append(limited_label_df)\n",
    "    \n",
    "    return pd.concat(limited_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "selected_user_data_limited = limit_samples_per_label(\n",
    "    selected_user_data_stratified,\n",
    "    label_col='label',\n",
    "    sample_set_size_per_label=sample_set_size_per_label\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "debug_print(f\"After limiting: {selected_user_data_limited.shape[0]} samples available.\", 5)\n",
    "limited_bot_count = selected_user_data_limited.loc[selected_user_data_limited['label'] == 'bot'].shape[0]\n",
    "limited_human_count = selected_user_data_limited.loc[selected_user_data_limited['label'] == 'human'].shape[0]\n",
    "\n",
    "debug_print(f\"Limited data contains {limited_bot_count} bots and {limited_human_count} humans.\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ffe09a-0bf9-4caa-aa01-3d17259b2034",
   "metadata": {},
   "source": [
    "# Stage 2: Train Embedded Models \n",
    "### Part A: URL Scoring\n",
    "\n",
    "Bots have a purpose, and that purpose generally means needing to spread external information. Sometimes that means using external URLS. If we put all those URLS people post in a bag, can we spot the patterns in how bots post URLs versus humans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6939a3c7-2cbc-4be5-bf18-52b8d2209274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24129 domains selected for modeling.\n",
      "Model saved to /dataset/twibot22/generated_data/url_model.pkl\n",
      "Accuracy: 0.5717\n",
      "F1 Score: 0.5665\n",
      "ROC AUC: 0.6025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5717, 0.5665310386851464, 0.6025216999999998)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib  # For saving and loading models\n",
    "\n",
    "class URLConfidenceModel:\n",
    "    def __init__(self, exclude_domains=None, domain_bag_min_frequency=2, domain_bag_max_domains=20):\n",
    "        self.exclude_domains = exclude_domains or {\"twitter.com\", \"www.twitter.com\", \"t.co\"}\n",
    "        self.domain_bag_min_frequency = domain_bag_min_frequency\n",
    "        self.domain_bag_max_domains = domain_bag_max_domains\n",
    "        self.domain_index = None\n",
    "        self.model = None\n",
    "\n",
    "    def preprocess_data(self, data, one_hot_encode=True):\n",
    "        if not one_hot_encode:\n",
    "            raise NotImplementedError(\"Only one-hot encoding is supported in this version.\")\n",
    "\n",
    "        domain_counts = Counter(\n",
    "            domain\n",
    "            for row in data['tweet_urls_top_x'].apply(dict)\n",
    "            for domain, freq in row.items()\n",
    "        )\n",
    "        \n",
    "        filtered_domains = {\n",
    "            domain for domain, count in domain_counts.items()\n",
    "            if count >= self.domain_bag_min_frequency and domain not in self.exclude_domains\n",
    "        }\n",
    "\n",
    "        print(f\"{len(filtered_domains)} domains selected for modeling.\")\n",
    "\n",
    "        def calculate_domain_frequencies(row):\n",
    "            domain_data = {}\n",
    "            idx = 0\n",
    "            for domain, freq in row.items():\n",
    "                if domain in filtered_domains:\n",
    "                    domain_data[domain] = {'frequency': freq}\n",
    "                    idx += 1\n",
    "                    if idx >= self.domain_bag_max_domains:\n",
    "                        break\n",
    "            return domain_data\n",
    "\n",
    "        X_one_hot_dict = data['tweet_urls_top_x'].apply(calculate_domain_frequencies)\n",
    "\n",
    "        attributes = ['frequency']\n",
    "        self.domain_index = {\n",
    "            (domain, attr): idx\n",
    "            for idx, (domain, attr) in enumerate(\n",
    "                (domain, attr) for domain in filtered_domains for attr in attributes\n",
    "            )\n",
    "        }\n",
    "\n",
    "        rows, cols, values = [], [], []\n",
    "        for row_idx, domain_data in enumerate(X_one_hot_dict):\n",
    "            for domain, features in domain_data.items():\n",
    "                for attr, value in features.items():\n",
    "                    col_idx = self.domain_index[(domain, attr)]\n",
    "                    rows.append(row_idx)\n",
    "                    cols.append(col_idx)\n",
    "                    values.append(value)\n",
    "\n",
    "        X_sparse = csr_matrix((values, (rows, cols)), shape=(len(X_one_hot_dict), len(self.domain_index)))\n",
    "        return X_sparse\n",
    "\n",
    "    def train_model(self, X, y, refit=False, param_distributions=None, refit_num_iter=20):\n",
    "        base_params = {\n",
    "            'bootstrap': False,\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        if refit:\n",
    "            param_distributions = param_distributions or {\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'n_estimators': [300, 350],\n",
    "                'max_depth': [30, 20],\n",
    "                'min_samples_split': [5, None],\n",
    "                'min_samples_leaf': [2, None],\n",
    "                'max_features': [\"sqrt\", None],\n",
    "                'class_weight': ['balanced', None],\n",
    "            }\n",
    "\n",
    "            clf = RandomForestClassifier(**base_params)\n",
    "            random_search = RandomizedSearchCV(\n",
    "                clf, param_distributions=param_distributions, n_iter=refit_num_iter,\n",
    "                scoring='f1_weighted', n_jobs=3, cv=3\n",
    "            )\n",
    "            random_search.fit(X, y)\n",
    "            self.model = random_search.best_estimator_\n",
    "            print(f\"Best parameters: {random_search.best_params_}\")\n",
    "        else:\n",
    "            params = {\n",
    "                'n_estimators': 300,\n",
    "                'min_samples_split': 15,\n",
    "                'min_samples_leaf': 2,\n",
    "                'max_features': \"sqrt\",\n",
    "                'max_depth': 30,\n",
    "                'class_weight': 'balanced',\n",
    "                'criterion': 'entropy'\n",
    "            }\n",
    "            self.model = RandomForestClassifier(random_state=42, **params)\n",
    "            self.model.fit(X, y)\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_prob = self.model.predict_proba(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        roc_auc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "        return acc, f1, roc_auc\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        joblib.dump(self.model, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        self.model = joblib.load(filepath)\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "    def plot_feature_importances(self, top_n=25):\n",
    "        importances = self.model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        feature_names = [f\"{domain}_{attr}\" for domain, attr in self.domain_index.keys()]\n",
    "\n",
    "        top_indices = [idx for idx in indices if \"frequency\" in feature_names[idx]][:top_n]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(f\"Top {top_n} Feature Importances\", fontsize=14)\n",
    "        plt.barh(range(len(top_indices)), importances[top_indices][::-1], color=\"b\", align=\"center\")\n",
    "        plt.yticks(range(len(top_indices)), [feature_names[i] for i in top_indices[::-1]])\n",
    "        plt.xlabel(\"Weight Coefficient\", fontsize=12)\n",
    "        plt.ylabel(\"Feature\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "load_url_model_from_file = False\n",
    "\n",
    "\n",
    "X_sparse = model.preprocess_data(selected_user_data_limited)\n",
    "y = selected_user_data_limited['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sparse, y, test_size=0.2, stratify=y)\n",
    "model = URLConfidenceModel()\n",
    "if load_url_model_from_file:\n",
    "    model.load_model(url_model_output)\n",
    "else:   \n",
    "    model.train_model(X_train, y_train)\n",
    "    model.save_model(url_model_output)\n",
    "model.evaluate_model(X_test, y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e830729-5a88-46e3-95bb-1bf9daa7eb38",
   "metadata": {},
   "source": [
    "# Stage 2 Part B: Relationship Modeling\n",
    "\n",
    "There are patterns in how bots choose to reply to certain tweets, keywords, or users. If we see these interactions as graphed relationships, can we find patterns?\n",
    "\n",
    "## Warnings: CUDA USE (if it exists)\n",
    "\n",
    "## Step 1: Import torch, prepare constants, prepare cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c70af02c-c9f9-4925-a6c4-3b62d79db376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from cogdl.experiments import experiment, output_results\n",
    "import torch\n",
    "import os\n",
    "from cogdl import experiment\n",
    "from cogdl.datasets import build_dataset\n",
    "from cogdl.models import build_model\n",
    "from cogdl.options import get_default_args\n",
    "\n",
    "dataset_path = generated_data_output+\"/data.pt\"\n",
    "num_pass = 3\n",
    "scan_missing_thread_features = True\n",
    "\n",
    "g_output_path = checkpoint_path = generated_data_output+\"/gs_output/\"\n",
    "checkpoint_path = g_output_path+\"graphsage_model.ckpt\"\n",
    "model_path = g_output_path+\"graphsage_final.pt\"\n",
    "if not Path(g_output_path).exists:\n",
    "    os.mkdir(g_output_path)\n",
    "\n",
    "# Experiment control variables\n",
    "doing_experiment = True\n",
    "load_from_checkpoint = True\n",
    "dataset = None\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Preprocessing control\n",
    "\n",
    "model_type = \"graphsage\"  # Options: \"dropedge_gcn\", \"gcn\", or \"graphsage\"\n",
    "os.environ[\"PYTORCH_DEBUG\"] = \"1\"\n",
    "\n",
    "experiment_params =  {\n",
    "                'epochs':200,\n",
    "                'do_valid':False,\n",
    "                'checkpoint_path':checkpoint_path,\n",
    "                'n_trials':2,\n",
    "                'hidden_size':[175],\n",
    "                'batch_size':1024,\n",
    "                #weight_decay=1e-4,\n",
    "                #eval_step=5,\n",
    "                'lr':0.01,\n",
    "                'patience':20,\n",
    "                'dropout':0.4,               # Reduced dropout for large graphs\n",
    "                'sample_size':[15, 10],      # Increased neighbors sampled\n",
    "                'num_layers':2,              # Deeper model\n",
    "            }\n",
    "\n",
    "graphsage_run_params = {\n",
    "        'dropout':0.3,               # Reduced dropout for large graphs\n",
    "        'sample_size':[15, 10],      # Increased neighbors sampled        \n",
    "        'hidden_size':[175],    # Hidden feature dimension\n",
    "        'num_layers':2,      # Number of layers\n",
    "        'aggr':'mean'\n",
    "}\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649508b-9c5f-4dcb-b08e-38360aaf5083",
   "metadata": {},
   "source": [
    "## Step 2: Normalize features for user nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aeac811-1c24-4b0f-a3fb-ea73cb18da73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample features for graph node embedding:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_following_ratio</th>\n",
       "      <th>profile_desc_len</th>\n",
       "      <th>account_age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1484761392406089730</th>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>94867785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40019186</th>\n",
       "      <td>1852</td>\n",
       "      <td>559</td>\n",
       "      <td>122</td>\n",
       "      <td>4229</td>\n",
       "      <td>7.565295</td>\n",
       "      <td>79</td>\n",
       "      <td>495381468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     followers_count  following_count  listed_count  \\\n",
       "id                                                                    \n",
       "1484761392406089730                1              105             0   \n",
       "40019186                        1852              559           122   \n",
       "\n",
       "                     tweet_count  tweet_following_ratio  profile_desc_len  \\\n",
       "id                                                                          \n",
       "1484761392406089730            0               0.000000                 2   \n",
       "40019186                    4229               7.565295                79   \n",
       "\n",
       "                     account_age  \n",
       "id                                \n",
       "1484761392406089730     94867785  \n",
       "40019186               495381468  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample normalized features for graph node embedding:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_following_ratio</th>\n",
       "      <th>profile_desc_len</th>\n",
       "      <th>account_age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1484761392406089730</th>\n",
       "      <td>7.656202e-09</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.006309</td>\n",
       "      <td>0.002669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40019186</th>\n",
       "      <td>1.417929e-05</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>4.503014e-07</td>\n",
       "      <td>0.249211</td>\n",
       "      <td>0.245813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     followers_count  following_count  listed_count  \\\n",
       "id                                                                    \n",
       "1484761392406089730     7.656202e-09         0.000025      0.000000   \n",
       "40019186                1.417929e-05         0.000134      0.000226   \n",
       "\n",
       "                     tweet_count  tweet_following_ratio  profile_desc_len  \\\n",
       "id                                                                          \n",
       "1484761392406089730     0.000000           0.000000e+00          0.006309   \n",
       "40019186                0.000084           4.503014e-07          0.249211   \n",
       "\n",
       "                     account_age  \n",
       "id                                \n",
       "1484761392406089730     0.002669  \n",
       "40019186                0.245813  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def datetime_to_age_in_seconds(dt):\n",
    "    if pd.api.types.is_datetime64_any_dtype(dt):\n",
    "        dt = dt.date\n",
    "    if not isinstance(dt, datetime):\n",
    "        raise ValueError(\"The input must be a datetime object.\")\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)  # Correct use of timezone.utc\n",
    "    now = datetime.now(timezone.utc)  # Correct use of timezone.utc\n",
    "    age_in_seconds = int((now - dt).total_seconds())\n",
    "    return age_in_seconds\n",
    "\n",
    "target_feature_keys = [\n",
    "    'followers_count',\n",
    "    'following_count',\n",
    "    'listed_count',\n",
    "    'tweet_count',\n",
    "    \n",
    "    'tweet_following_ratio',\n",
    "    'created_at',\n",
    "    'profile_desc_len'\n",
    "]\n",
    "\n",
    "# Copy selected columns\n",
    "copied_user_details_for_graph = user_detail_data.loc[:, target_feature_keys].copy()\n",
    "copied_user_details_for_graph.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "copied_user_details_for_graph.dropna(inplace=True)\n",
    "\n",
    "# Convert created_at to account_age in seconds\n",
    "copied_user_details_for_graph['account_age'] = copied_user_details_for_graph['created_at'].apply(datetime_to_age_in_seconds)\n",
    "\n",
    "# Drop original created_at column\n",
    "copied_user_details_for_graph.drop('created_at', axis=1, inplace=True)\n",
    "\n",
    "# Normalize all columns (including account_age)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "transformed = scaler.fit_transform(copied_user_details_for_graph)\n",
    "\n",
    "# Create normalized DataFrame\n",
    "normalized_details_for_graph = pd.DataFrame(\n",
    "    transformed, \n",
    "    index=copied_user_details_for_graph.index, \n",
    "    columns=copied_user_details_for_graph.columns\n",
    ")\n",
    "\n",
    "print(\"Sample features for graph node embedding:\")\n",
    "display(copied_user_details_for_graph.head(2))\n",
    "\n",
    "print(\"Sample normalized features for graph node embedding:\")\n",
    "display(normalized_details_for_graph.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a1e1f-389a-485f-9a5a-82a8b1432589",
   "metadata": {},
   "source": [
    "## Step 3: Convert edges in dataset to an intermediate format before graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ccb9f7-a3eb-405f-8dfb-f88c51d5df2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading relationship map from relationship_map.pkl...\n",
      "Loaded relationship map with 499279 nodes.\n",
      "Constrinaed Preview (Number of entities in relationship types limited to 10):\n",
      "{\n",
      " \"u10078\": {\n",
      "  \"11\": \"['u1414756126365736960', 'u479184910', 'u1392398196', 'u1477531837941624836', 'u1483743555579682822', 'u1484068618258190337', 'u619559599', 'u375157680', 'u1483845391753830400', 'u2666969059']\",\n",
      "  \"9\": \"['u87818409', 'u131926473', 'u1586501', 'u61542667', 'u806689', 'u73147282', 'u1854401', 'u8453452', 'u57203', 'u1482581556']\",\n",
      "  \"12\": \"['t1456717498255089665', 't1407347696025968660', 't1471528360132038657', 't1471164597587505156', 't1396274163925491714', 't1396911529195409408', 't1404848565998940160', 't1439919473121759235', 't1432750322649231361', 't1422605386788466696']\",\n",
      "  \"13\": \"['l1913694', 'l1913745', 'l111194136', 'l1913710']\",\n",
      "  \"5\": \"['t1461355016388632588']\",\n",
      "  \"4\": \"['t1394811317945454592', 't1246526024189267968', 't1311059141914275840', 't1364731299731152899', 't1236125302322937857', 't1359200859985350657', 't1290342030426566657', 't1298296313549750272', 't1196876316806701056', 't1194726509346349056']\"\n",
      " },\n",
      " \"u324\": {\n",
      "  \"11\": \"['u14897601', 'u1477383222715691011', 'u11360522', 'u1474165492030099467', 'u632343', 'u191334657', 'u18391846', 'u1200246625144147971', 'u1477696416604725254', 'u1477531837941624836']\",\n",
      "  \"9\": \"['u1355560235205713924', 'u18686681', 'u14642331', 'u237467855', 'u131926473', 'u358278852', 'u21633141', 'u22545732', 'u15111030', 'u8453452']\",\n",
      "  \"12\": \"['t1494752273926213635', 't1495616560555249666', 't1494411120488206336', 't1495444476118003712', 't1494190476886360069', 't1495415591687106563', 't1495215750449025026', 't1494455262169772046', 't1494892623710101505', 't1493740186424860677']\",\n",
      "  \"13\": \"['l1271787801193992194', 'l5125422', 'l865']\",\n",
      "  \"5\": \"['t941288359787487232']\",\n",
      "  \"4\": \"['t1463295165595062277', 't1483089573756424196', 't1458476515050631185', 't1463854532245528578', 't1462038135676805129', 't1483601279184216069', 't1459250165013954561', 't1474415748319694850', 't1475494105035718657', 't1495214918315954184']\"\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "entity_types = { 'user' : 0, 'group': 1, 'post': 2}\n",
    "REL_TYPE_DISC = 0\n",
    "REL_TYPE_FOLLWD = 1\n",
    "REL_TYPE_REPLD = 2\n",
    "REL_TYPE_RTD = 3\n",
    "REL_TYPE_POSTED = 4\n",
    "REL_TYPE_PINNED = 5\n",
    "REL_TYPE_MEMB = 6\n",
    "REL_TYPE_QUOTED = 7\n",
    "REL_TYPE_CONTAINS = 8\n",
    "REL_TYPE_FOLLWG = 9\n",
    "REL_TYPE_MENT = 10\n",
    "REL_TYPE_FOLLWR = 11\n",
    "REL_TYPE_LIKED = 12\n",
    "REL_TYPE_OWN = 13\n",
    "\n",
    "relationship_types = {\n",
    "                      'discuss':REL_TYPE_DISC,\n",
    "                      'followed':REL_TYPE_FOLLWD,\n",
    "                      'replied_to':REL_TYPE_REPLD,\n",
    "                      'retweeted':REL_TYPE_RTD,\n",
    "                      'post':REL_TYPE_POSTED,\n",
    "                      'pinned':REL_TYPE_PINNED,\n",
    "                      'membership':REL_TYPE_MEMB,\n",
    "                      'quoted':REL_TYPE_QUOTED,\n",
    "                      'contain':REL_TYPE_CONTAINS,\n",
    "                      'following':REL_TYPE_FOLLWG,\n",
    "                      'mentioned':REL_TYPE_MENT,\n",
    "                      'followers':REL_TYPE_FOLLWR,\n",
    "                      'like':REL_TYPE_LIKED,\n",
    "                      'own':REL_TYPE_OWN\n",
    "                    }\n",
    "\n",
    "\n",
    "def dd():\n",
    "    return defaultdict(set)\n",
    "    \n",
    "def convert_edge_file_to_rel_map(relationship_map_file='relationship_map.pkl'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert edge file to relationship_map\n",
    "    If the relationship_map file exists, load it. Otherwise, parse the edge parquet and save the result.\n",
    "    Args:\n",
    "        relationship_map_file (str): Path to the file to save/load the relationship map.\n",
    "\n",
    "    Returns:\n",
    "        dict: The relationship map.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the relationship map file exists\n",
    "    if os.path.exists(relationship_map_file):\n",
    "        print(f\"Loading relationship map from {relationship_map_file}...\")\n",
    "        with open(relationship_map_file, 'rb') as f:\n",
    "            relationship_map = pickle.load(f)\n",
    "        print(f\"Loaded relationship map with {len(relationship_map)} nodes.\")\n",
    "        return relationship_map\n",
    "\n",
    "    print(\"Relationship map file not found. Parsing edge parquet to create new map.\")\n",
    "    \n",
    "    \n",
    "    relationship_map = defaultdict(dd)\n",
    "    target_entities = set('u'+str(x) for x in selected_user_data_limited.index.values) \n",
    "    print(f\"Total passes scheduled: {num_pass}\")\n",
    " \n",
    "    next_pass_targets = target_entities\n",
    "    relevant_data = get_data(\"edges\")\n",
    "\n",
    "    try:\n",
    "        for i in range(num_pass):\n",
    "            print(f\"Pass {i}...\")\n",
    "\n",
    "            sub_pass = [ relevant_data.loc[relevant_data['id1'].isin(next_pass_targets)] ]\n",
    "                       # relevant_data.loc[relevant_data['id2'].isin(next_pass_targets)] ]\n",
    "            next_pass_targets.clear()            \n",
    "            \n",
    "            # src, dest\n",
    "            def graph_dir_pass(rw, k1, k2):  \n",
    "                relationship_map[rw[k1]][str(relationship_types[rw['relationship']])].add(rw[k2])\n",
    "                next_pass_targets.add(rw[k2])\n",
    "                \n",
    "            sub_pass[0].apply(graph_dir_pass, args=('id1', 'id2'), axis=1)             \n",
    "            print(f\"Entities collected: {len(next_pass_targets)}\")\n",
    "        edge_counter = sum(len(v) for v in relationship_map.values())\n",
    "        print(\"All passes completed. Nodes collected:\", len(relationship_map), \"Edges collected:\", edge_counter)\n",
    "    except Exception as e:\n",
    "        print(f\"Problem collecting relationship edges: {str(e)} Freeing resources.\")\n",
    "        del relevant_data, relationship_map\n",
    "        gc.collect()\n",
    "        raise StopExecution\n",
    "        \n",
    "    # Save the relationship map to file\n",
    "    print(f\"Saving relationship map to {relationship_map_file}...\")\n",
    "    with open(relationship_map_file, 'wb') as f:\n",
    "        pickle.dump(relationship_map, f)\n",
    "    print(\"Relationship map saved successfully.\")\n",
    "    del relevant_data\n",
    "    gc.collect()\n",
    "    return relationship_map\n",
    "\n",
    "\n",
    "gnn_rel_map = convert_edge_file_to_rel_map()\n",
    "\n",
    "\n",
    "print(\"Constrinaed Preview (Number of entities in relationship types limited to 10):\")\n",
    "def new_encoder(unknown_var):\n",
    "    if type(unknown_var) is set:\n",
    "        return str(list(unknown_var)[0:10])\n",
    "    if type(unknown_var) is list:\n",
    "        return str(list(unknown_var[0:10]))\n",
    "    return str(unknown_var)\n",
    "print(json.dumps(dict(list(gnn_rel_map.items())[0:2]),indent=1, default=new_encoder))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0896d570-cc87-47fd-bf4f-143aa0d4f4ad",
   "metadata": {},
   "source": [
    "### Sample of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa11f6e-b96e-4a1c-8563-d5f6cbde3f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcf91bf9-8cdf-4f84-8a1d-8e8c08572c11",
   "metadata": {},
   "source": [
    "### Cross Population of thread data into user data\n",
    "\n",
    "The GNN will be given threads as nodes along with some default feature values. Here is where we calculate the feature values for the posts before normalizing and then compiling everything for use as rows in a feature matrice. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1ef9140-4b91-40f2-8214-0666b9188502",
   "metadata": {},
   "source": [
    "Relation types: ['discuss', 'followed', 'replied_to', 'retweeted', 'post', 'pinned', 'membership', 'quoted', 'contain', 'following', 'mentioned', 'followers', 'like', 'own']\n",
    "{\n",
    "  \"followers\": \"u980749991491682304,followers,u1480979504696864775\",\n",
    "  \"following\": \"u105387876,following,u402576793\",\n",
    "  \"own\": \"u117599559,own,l1147234749037105152\",\n",
    "  \"pinned\": \"u1221922726778155014,pinned,t1429897760287993864\",\n",
    "  \"post\": \"u124137240,post,t1497615862374998018\",\n",
    "  \"contain\": \"l172852452,contain,t1502774663025463296\",\n",
    "  \"discuss\": \"t1146699924525977611,discuss,h4051\",\n",
    "  \"mentioned\": \"t1462752725523906563,mentioned,u2765959278\",\n",
    "  \"like\": \"u1006320912164163585,like,t1486736342667042825\",\n",
    "  \"followed\": \"l1311578498742521861,followed,u90464612\",\n",
    "  \"replied_to\": \"t1268274950907015168,replied_to,t1268161763138297858\",\n",
    "  \"retweeted\": \"t1250366502454296576,retweeted,t1250345547874979842\",\n",
    "  \"quoted\": \"t1374380360067284993,quoted,t1374360270299148293\",\n",
    "  \"membership\": \"l2840140,membership,u28142596\"\n",
    "}\n",
    "\n",
    "created_at             datetime64[ns]\n",
    "text                           object\n",
    "quote_count                    UInt32\n",
    "like_count                     UInt32\n",
    "retweet_count                  UInt32\n",
    "reply_count                    UInt32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b055322-a5f1-4ae3-b7c1-b0f55e362c43",
   "metadata": {},
   "source": [
    "## Step 4: Normalize thread node information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae7dcbe7-1acb-494f-a701-11b20bfed8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting from /dataset/twibot22/generated_data/tweet_7.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_8.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_4.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_1.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_6.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_2.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_5.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_0.parquet...\n",
      "Extracting from /dataset/twibot22/generated_data/tweet_3.parquet...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1499547298828886016</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499544740148264962</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499542095224360962</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499539485092155402</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499535965089632262</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     quote_count  like_count  retweet_count  reply_count\n",
       "id                                                                      \n",
       "1499547298828886016            0           4              0            0\n",
       "1499544740148264962            0           1              0            0\n",
       "1499542095224360962            0           2              0            0\n",
       "1499539485092155402            0           1              0            0\n",
       "1499535965089632262            0           2              0            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing...\n",
      "Completed. Sample of normalized thread data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31166656206077952</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25710722633695233</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32629968655613952</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.095285e-05</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8960551275200512</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.098117e-06</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32270326490464256</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.739629e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32248372928708608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.011327e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32490782493507584</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.257737e-05</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12741590875373568</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.641512e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31180522075979776</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.375478e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31557582376140801</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.739629e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   quote_count    like_count  retweet_count  reply_count\n",
       "id                                                                      \n",
       "31166656206077952          0.0  0.000000e+00       0.000014          0.0\n",
       "25710722633695233          0.0  0.000000e+00       0.000000          0.0\n",
       "32629968655613952          0.0  3.095285e-05       0.000087          0.0\n",
       "8960551275200512           0.0  5.098117e-06       0.000015          0.0\n",
       "32270326490464256          0.0  8.739629e-06       0.000009          0.0\n",
       "32248372928708608          0.0  8.011327e-06       0.000007          0.0\n",
       "32490782493507584          0.0  2.257737e-05       0.000056          0.0\n",
       "12741590875373568          0.0  3.641512e-07       0.000004          0.0\n",
       "31180522075979776          0.0  8.375478e-06       0.000005          0.0\n",
       "31557582376140801          0.0  8.739629e-06       0.000006          0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Okay, but a little bit different:\n",
    "\n",
    "relevant_data = get_post_chunks(cols=[\"id\",\"quote_count\",\"like_count\",\"retweet_count\",\"reply_count\"], index=\"id\")\n",
    "display(relevant_data.head(5))\n",
    "\n",
    "ENTITY_USER = 0\n",
    "ENTITY_GROUP = 1\n",
    "ENTITY_POST = 2\n",
    "entity_types = { 'u' : ENTITY_USER, 'l': ENTITY_GROUP, 't': ENTITY_POST}\n",
    "\n",
    "def strip_id(x):\n",
    "    prefix_map = {'u': 'u', 'l': 'l', 't': 't'}\n",
    "    return int(x[1:]) if x and x[0] in prefix_map else -1\n",
    "\n",
    "def entity_type_from_id(any_str_id):\n",
    "    return entity_types.get(any_str_id[0], -1)\n",
    "    \n",
    "relevant_data.fillna({'like_count': 0, 'quote_count': 0, 'retweet_count': 0, 'reply_count': 0}, inplace=True)\n",
    "\n",
    "post_ids = [strip_id(node) for node in gnn_rel_map if entity_type_from_id(node) == ENTITY_POST]\n",
    "valid_post_ids = list(set(relevant_data.index).intersection(post_ids))\n",
    "valid_posts = relevant_data.loc[valid_post_ids]\n",
    "\n",
    "print(\"Normalizing...\")\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "transformed = scaler.fit_transform(valid_posts)\n",
    "normalized_thread_feature_data = pd.DataFrame(\n",
    "    transformed, \n",
    "    index=valid_posts.index, \n",
    "    columns=valid_posts.columns\n",
    ")\n",
    "\n",
    "# Convert to a dictionary for `thread_feature_data`\n",
    "#thread_feature_data = normalized_thread_feature_data[['like_count', 'quote_count', 'retweet_count', 'reply_count']].to_dict(orient='index')\n",
    "\n",
    "print(\"Completed. Sample of normalized thread data:\")\n",
    "display(normalized_thread_feature_data.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad179a3-3cc4-44b8-9a07-38f22049c8c2",
   "metadata": {},
   "source": [
    "## Step 5: Convert intermediate format dictionary into CogDL appropriate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2c27b-cd85-4164-a9ee-8846a79038ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "116c8cd5-bbf0-4152-b891-10680c1843b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n",
      "Processing relationship map to create graph...\n",
      "Creating graph...\n",
      "Graph created. Nodes: 499279, Edges: 1615228\n",
      "Index mapping saved to index_map.json\n",
      "Sampling graph...\n",
      "Saving graph data to data.pt\n",
      "Graph Statistics:\n",
      " - Number of nodes: 499279\n",
      " - Number of edges: 1615228\n",
      " - Feature dimension: 8\n",
      " - Edge Attr dimension: torch.Size([1615228])\n",
      " - Number of unique labels: 3\n",
      " - Label distribution: {tensor(-1): tensor(401922), tensor(0): tensor(90242), tensor(1): tensor(7115)}\n",
      " - Training nodes: 61341\n",
      " - Validation nodes: 24636\n",
      " - Test nodes: 24497\n",
      "Train label distribution: {tensor(0): tensor(54232), tensor(1): tensor(7109)}\n",
      "Validation label distribution: {tensor(0): tensor(18062), tensor(1): tensor(6574)}\n",
      "Test label distribution: {tensor(0): tensor(17948), tensor(1): tensor(6549)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/cogdl/datasets/customized_data.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import gc\n",
    "from cogdl.data import Graph\n",
    "from cogdl.datasets import NodeDataset, generate_random_graph\n",
    "from cogdl.utils.graph_utils import to_undirected, remove_self_loops\n",
    "\n",
    "def get_rel_map_graph(relationship_map, normalized_user_features, normalized_thread_features):\n",
    "    \"\"\"\n",
    "    Create a graph representation from relationship map and features.\n",
    "    \n",
    "    Args:\n",
    "        relationship_map (dict): Node relationships where keys are node IDs and values are dictionaries \n",
    "                                 with relationship types and their connected nodes (sets).\n",
    "        normalized_user_features (DataFrame): Pre-normalized user feature data indexed by user ID.\n",
    "        normalized_thread_features (DataFrame): Pre-normalized thread feature data indexed by thread ID.\n",
    "\n",
    "    Returns:\n",
    "        entity_index_map (dict): Mapping from original entity IDs to sequential node indices.\n",
    "        Graph: A CogDL Graph object with features, labels, edges, and edge attributes.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Creating graph...\")\n",
    "\n",
    "    # Precompute sizes\n",
    "    num_nodes = len(relationship_map)\n",
    "    feature_size = max(normalized_user_features.shape[1], normalized_thread_features.shape[1]) + 1  # +1 for entity type\n",
    "    num_edges = sum(len(rel_dict.get(rel, [])) for rel_dict in relationship_map.values() for rel in rel_dict if '_' not in rel)\n",
    "\n",
    "\n",
    "    # Initialize tensors\n",
    "    node_features = torch.full((num_nodes, feature_size), -1.0, dtype=torch.float)\n",
    "    labels = torch.full((num_nodes,), -1, dtype=torch.long)\n",
    "    edges = torch.empty((2, num_edges), dtype=torch.long)\n",
    "    edge_features = torch.empty((num_edges,), dtype=torch.long)\n",
    "\n",
    "    \n",
    "    # Precompute entity index map. Our node IDs should be sequential ints.\n",
    "    entity_index_map = {uid: idx for idx, uid in enumerate(relationship_map.keys())}\n",
    "\n",
    "    # Populate tensors\n",
    "    edge_idx = 0\n",
    "    for node_idx, (uid1, rel_dict) in enumerate(relationship_map.items()):\n",
    "        entity_type = entity_type_from_id(uid1)\n",
    "        stripped_id = strip_id(uid1)\n",
    "\n",
    "        # Assign features and labels based on entity type\n",
    "        if entity_type == ENTITY_USER and stripped_id in normalized_user_features.index:\n",
    "            feature_values = [entity_type] + normalized_user_features.loc[stripped_id].tolist()\n",
    "        elif entity_type == ENTITY_POST and stripped_id in normalized_thread_features.index:\n",
    "            feature_values = [entity_type] + normalized_thread_features.loc[stripped_id].tolist()\n",
    "        else:\n",
    "            feature_values = [-1] * feature_size  # Default values for missing data\n",
    "        \n",
    "        # Pad or truncate to match feature size\n",
    "        feature_values = feature_values[:feature_size] + [-1] * (feature_size - len(feature_values))\n",
    "        node_features[node_idx] = torch.tensor(feature_values, dtype=torch.float)\n",
    "        labels[node_idx] = (\n",
    "            1 if entity_type == ENTITY_USER and stripped_id in user_detail_data.index\n",
    "                 and user_detail_data.loc[stripped_id, 'label'] == 'bot'\n",
    "            else 0 if entity_type == ENTITY_USER and stripped_id in user_detail_data.index\n",
    "                 and user_detail_data.loc[stripped_id, 'label'] == 'human'\n",
    "            else -1\n",
    "        )\n",
    "\n",
    "        # Process edges\n",
    "        for rel, uid_list in rel_dict.items():\n",
    "            if '_' in rel:\n",
    "                continue  # Skip backreferences\n",
    "            valid_uids = [uid2 for uid2 in uid_list if uid2 in entity_index_map]\n",
    "            for uid2 in valid_uids:\n",
    "                edges[0, edge_idx] = node_idx\n",
    "                edges[1, edge_idx] = entity_index_map[uid2]\n",
    "                edge_features[edge_idx] = int(rel)\n",
    "                edge_idx += 1\n",
    "\n",
    "    # Trim unused edge entries\n",
    "    edges = edges[:, :edge_idx]\n",
    "    edge_features = edge_features[:edge_idx]\n",
    "\n",
    "    print(f\"Graph created. Nodes: {num_nodes}, Edges: {edge_idx}\")\n",
    "    return entity_index_map, Graph(\n",
    "        x=node_features,\n",
    "        y=labels,\n",
    "        edge_index=edges,\n",
    "        edge_attr=edge_features\n",
    "    )\n",
    "\n",
    "class LockstepRelDataset(NodeDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        relationship_map=None,\n",
    "        normalized_user_features=None,\n",
    "        normalized_thread_features=None,\n",
    "        path=\"data.pt\",\n",
    "        index_map_path=\"index_map.json\",\n",
    "        sample_limit=0,\n",
    "        train_ratio=0.6,\n",
    "        val_ratio=0.1,\n",
    "        stratify=True,\n",
    "        stratification_mode=\"undersample\",  # \"undersample\" or \"oversample\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A custom NodeDataset class for creating and managing graph datasets.\n",
    "\n",
    "        Args:\n",
    "            relationship_map (dict): A map defining the relationships between nodes.\n",
    "            path (str): Path to save/load the graph data.\n",
    "            index_map_path (str): Path to save/load the node index mapping.\n",
    "            sample_limit (int): Limit the number of samples to process (0 for no limit).\n",
    "            train_ratio (float): Proportion of nodes used for training (0 < train_ratio < 1).\n",
    "            val_ratio (float): Proportion of nodes used for validation (0 < val_ratio < 1).\n",
    "            stratify (bool): Whether to stratify the dataset based on class labels.\n",
    "            stratification_mode (str): \"undersample\" to downsample the majority class, \n",
    "                                       \"oversample\" to upsample the minority class.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.index_map_path = index_map_path\n",
    "        self.sample_limit = sample_limit\n",
    "        self.relationship_map = relationship_map\n",
    "        self.index_mapping = None\n",
    "        self.data_cached = None\n",
    "        self.normalized_thread_features = normalized_thread_features\n",
    "        self.normalized_user_features = normalized_user_features\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.stratify = stratify\n",
    "        self.stratification_mode = stratification_mode\n",
    "\n",
    "        if relationship_map is None and not os.path.exists(path):\n",
    "            raise ValueError(\"Relationship map must be provided if data at the path does not exist.\")\n",
    "\n",
    "        if train_ratio + val_ratio >= 1.0:\n",
    "            raise ValueError(\"The sum of train_ratio and val_ratio must be less than 1.0.\")\n",
    "\n",
    "        super(LockstepRelDataset, self).__init__(path, scale_feat=False, metric=\"accuracy\")\n",
    "\n",
    "    def stratify_indices(self, valid_indices, labels, ignore_labels=None):\n",
    "        \"\"\"\n",
    "        Stratify the dataset by balancing class distributions, with an option to ignore specific labels.\n",
    "    \n",
    "        Args:\n",
    "            valid_indices (Tensor): Indices of nodes with valid labels.\n",
    "            labels (Tensor): Labels corresponding to the nodes.\n",
    "            ignore_labels (list, optional): List of labels to ignore during stratification.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: Stratified indices.\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        from sklearn.utils import resample\n",
    "    \n",
    "        # Default to an empty list if no labels to ignore are specified\n",
    "        if ignore_labels is None:\n",
    "            ignore_labels = []\n",
    "    \n",
    "        # Filter out indices corresponding to ignored labels\n",
    "        valid_mask = ~torch.isin(labels, torch.tensor(ignore_labels))\n",
    "        valid_indices = valid_indices[valid_mask]\n",
    "        labels = labels[valid_mask]\n",
    "    \n",
    "        # Group indices by class\n",
    "        class_indices = {label.item(): valid_indices[labels == label] for label in torch.unique(labels)}\n",
    "        min_class_size = min(len(indices) for indices in class_indices.values())\n",
    "        max_class_size = max(len(indices) for indices in class_indices.values())\n",
    "    \n",
    "        # Apply stratification\n",
    "        stratified_indices = []\n",
    "        for label, indices in class_indices.items():\n",
    "            if self.stratification_mode == \"undersample\":\n",
    "                stratified_indices.append(indices[torch.randperm(len(indices))[:min_class_size]])\n",
    "            elif self.stratification_mode == \"oversample\":\n",
    "                extra_indices = indices[torch.randint(len(indices), (max_class_size - len(indices),))]\n",
    "                stratified_indices.append(torch.cat([indices, extra_indices], dim=0))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid stratification_mode. Choose 'undersample' or 'oversample'.\")\n",
    "    \n",
    "        return torch.cat(stratified_indices, dim=0)\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Process the relationship map to generate graph data and index mapping.\n",
    "\n",
    "        Returns:\n",
    "            Graph: The processed graph object with node features, edges, and masks.\n",
    "        \"\"\"\n",
    "        do_save = False\n",
    "\n",
    "        # Load existing data if available\n",
    "        if os.path.exists(self.path):\n",
    "            print(f\"Loading graph data from {self.path}\")\n",
    "            data = torch.load(self.path)\n",
    "            self.load_index_map()\n",
    "        else:\n",
    "           \n",
    "            # Create graph data from the relationship map\n",
    "            if self.relationship_map is None:\n",
    "                raise ValueError(\"Relationship map must be provided to generate graph data.\")\n",
    "\n",
    "            print(\"Processing relationship map to create graph...\")\n",
    "            do_save = True\n",
    "            index_map, data = get_rel_map_graph(\n",
    "                self.relationship_map,\n",
    "                self.normalized_user_features,\n",
    "                self.normalized_thread_features\n",
    "            )\n",
    "            self.index_mapping = index_map\n",
    "\n",
    "            # Save the index mapping\n",
    "            with open(self.index_map_path, 'w') as f:\n",
    "                json.dump(self.index_mapping, f)\n",
    "            print(f\"Index mapping saved to {self.index_map_path}\")\n",
    "\n",
    "        # Cache data\n",
    "        if self.data_cached is not None:\n",
    "            print(\"Using cached data.\")\n",
    "            return self.data_cached\n",
    "\n",
    "        print(\"Sampling graph...\")\n",
    "        # Masks for training, validation, and testing\n",
    "        num_nodes = data.num_nodes\n",
    "        labels = data.y\n",
    "        valid_indices = (labels != -1).nonzero(as_tuple=True)[0]  # Nodes with valid labels\n",
    "\n",
    "        if self.stratify:\n",
    "            valid_indices = self.stratify_indices(valid_indices, labels[valid_indices], [-1])\n",
    "\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "        # Random sampling of valid nodes\n",
    "        valid_indices = valid_indices[torch.randperm(len(valid_indices))]  # Shuffle valid indices\n",
    "        num_valid = len(valid_indices)\n",
    "\n",
    "        train_size = int(self.train_ratio * num_valid)  # Training nodes\n",
    "        val_size = int(self.val_ratio * num_valid)      # Validation nodes\n",
    "        test_size = num_valid - train_size - val_size   # Test nodes\n",
    "\n",
    "        train_mask[valid_indices[:train_size]] = True\n",
    "        val_mask[valid_indices[train_size:train_size + val_size]] = True\n",
    "        test_mask[valid_indices[train_size + val_size:]] = True\n",
    "\n",
    "        # Assign masks to the graph data\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data.test_mask = test_mask\n",
    "\n",
    "        if do_save:\n",
    "            print(f\"Saving graph data to {self.path}\")\n",
    "            p = Path(self.path).parents[0]\n",
    "            if not p.exists:\n",
    "                os.mkdir(p)\n",
    "            torch.save(data, self.path)\n",
    "\n",
    "        self.data_cached = data\n",
    "        gc.collect()\n",
    "        return data\n",
    "\n",
    "\n",
    "def print_graph_statistics(data):\n",
    "    print(\"Graph Statistics:\")\n",
    "    print(f\" - Number of nodes: {data.num_nodes}\")\n",
    "    num_edges = len(data.edge_index[0]) if isinstance(data.edge_index, tuple) else data.edge_index.shape[1]\n",
    "    print(f\" - Number of edges: {num_edges}\")\n",
    "    print(f\" - Feature dimension: {data.x.shape[1]}\")\n",
    "    print(f\" - Edge Attr dimension: {data.edge_attr.shape}\")\n",
    "    print(f\" - Number of unique labels: {data.y.unique().numel()}\")\n",
    "    print(f\" - Label distribution: {dict(zip(*torch.unique(data.y, return_counts=True)))}\")\n",
    "    print(f\" - Training nodes: {data.train_mask.sum().item()}\")\n",
    "    print(f\" - Validation nodes: {data.val_mask.sum().item()}\")\n",
    "    print(f\" - Test nodes: {data.test_mask.sum().item()}\")\n",
    "\n",
    "def print_label_distribution(data):\n",
    "    train_labels = data.y[data.train_mask]\n",
    "    val_labels = data.y[data.val_mask]\n",
    "    test_labels = data.y[data.test_mask]\n",
    "\n",
    "    print(\"Train label distribution:\", dict(zip(*torch.unique(train_labels, return_counts=True))))\n",
    "    print(\"Validation label distribution:\", dict(zip(*torch.unique(val_labels, return_counts=True))))\n",
    "    print(\"Test label distribution:\", dict(zip(*torch.unique(test_labels, return_counts=True))))\n",
    "\n",
    "load_from_file = False\n",
    "if load_from_file:\n",
    "    print(\"Loading dataset from file...\")\n",
    "    try:\n",
    "        dataset = LockstepRelDataset(path=dataset_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load from file...{e}\")\n",
    "else:\n",
    "    dataset = LockstepRelDataset(relationship_map=gnn_rel_map,\n",
    "                                 normalized_thread_features=normalized_thread_feature_data,\n",
    "                                 normalized_user_features=normalized_details_for_graph,\n",
    "                                 stratification_mode=\"oversample\",\n",
    "                                 train_ratio=0.6,\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify=True)\n",
    "\n",
    "print_graph_statistics(dataset.data)\n",
    "print_label_distribution(dataset.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dd0f4-83d4-4e2d-81f4-869ba6f8a118",
   "metadata": {},
   "source": [
    "## STEP 6. Train GraphSAGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f7ca465-a43d-46ab-87f4-24393e0e1abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dataset/twibot22/generated_data/data.pt\n"
     ]
    }
   ],
   "source": [
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85077fa1-bb04-4af8-a22e-4cdad39cbc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=[1], epochs=200, max_epoch=None, patience=20, lr=0.01, weight_decay=0, n_warmup_steps=0.0, split=[0], clip_grad_norm=5.0, checkpoint_path='/dataset/twibot22/generated_data/gs_output/graphsage_model.ckpt', save_emb_path=None, load_emb_path=None, resume_training=False, logger=None, log_path='.', project='cogdl-exp', use_best_config=False, unsup=False, nstage=1, eval_step=1, n_trials=2, devices=[device(type='cuda')], cpu=False, cpu_inference=False, distributed=False, progress_bar='epoch', local_rank=0, master_port=13425, master_addr='localhost', return_model=False, actnn=False, fp16=False, rp_ratio=1, do_test=True, do_valid=False, dataset=['data.pt'], model=['graphsage'], dw='graphsage_dw', mw='graphsage_mw', hidden_size=[175], num_layers=2, dropout=0.4, aggr='mean', batch_size=1024, sample_size=[15, 10])\n",
      " \n",
      "|-----------------------------------------------------------------------------|\n",
      "    *** Running (`data.pt`, `graphsage`, `graphsage_dw`, `graphsage_mw`)\n",
      "|-----------------------------------------------------------------------------|\n",
      "Model Parameters: 3677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/opt/anaconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Epoch: 200, train_loss:  0.2934: 100%|██████████| 200/200 [09:52<00:00,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 0-th model to /dataset/twibot22/generated_data/gs_output/graphsage_model.ckpt ...\n",
      "Loading model from /dataset/twibot22/generated_data/gs_output/graphsage_model.ckpt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/anaconda3/lib/python3.12/site-packages/cogdl/trainer/trainer_utils.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': 0.7642976691023391}\n",
      "| Variant                | test_acc      |\n",
      "|------------------------|---------------|\n",
      "| (data.pt, 'graphsage') | 0.7643±0.0000 |\n",
      "| Variant                | test_acc      |\n",
      "|------------------------|---------------|\n",
      "| (data.pt, 'graphsage') | 0.7643±0.0000 |\n",
      "Experiment results: defaultdict(<class 'list'>, {(data.pt, 'graphsage'): [{'test_acc': 0.7642976691023391}]})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if doing_experiment:\n",
    "    try:\n",
    "        # Run the experiment with GPU configuration\n",
    "        results = experiment(\n",
    "            dataset=dataset,\n",
    "            model=model_type,\n",
    "            **experiment_params,\n",
    "            devices=[device]  # Pass device explicitly\n",
    "        )\n",
    "\n",
    "        # Save the model checkpoint to CPU for portability\n",
    "        #model = results[1]  # Assuming the model is returned as the second item\n",
    "        #torch.save(model.to(\"cpu\").state_dict(), model_path)\n",
    "        output_results(results)\n",
    "        print(\"Experiment results:\", results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Problem encountered while training:\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "003a1034-a932-4535-9e10-70aac2e91b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1444792/2429588035.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Filtered predictions saved to 'predictions.csv'\n",
      "Test Accuracy: 0.7641\n",
      "Node ID: 0 | Feature: tensor([0.0000e+00, 4.7654e-03, 1.4492e-04, 1.8500e-02, 8.2422e-05, 4.0976e-07,\n",
      "        1.1041e-01, 2.9486e-01], device='cuda:0') | Label: 0\n",
      "Node ID: 2 | Feature: tensor([0.0000e+00, 5.3754e-05, 9.5794e-04, 3.3741e-04, 4.3715e-04, 3.2923e-07,\n",
      "        4.2271e-01, 2.9270e-01], device='cuda:0') | Label: 0\n",
      "Node ID: 3 | Feature: tensor([0.0000e+00, 6.3355e-05, 1.3963e-04, 7.0819e-04, 7.0925e-05, 3.6593e-07,\n",
      "        9.1483e-02, 3.0558e-01], device='cuda:0') | Label: 0\n",
      "Node ID: 4 | Feature: tensor([0.0000e+00, 1.6875e-04, 3.4535e-04, 1.5944e-03, 1.3069e-04, 2.7290e-07,\n",
      "        2.6498e-01, 2.9920e-01], device='cuda:0') | Label: 0\n",
      "Node ID: 5 | Feature: tensor([0.0000e+00, 3.7707e-05, 2.1533e-04, 1.1680e-04, 1.9182e-04, 6.4213e-07,\n",
      "        1.5773e-01, 3.0558e-01], device='cuda:0') | Label: 0\n",
      "Feature 0 - Correlation with Label: -0.9503\n",
      "Feature 1 - Correlation with Label: -0.0136\n",
      "Feature 2 - Correlation with Label: -0.0193\n",
      "Feature 3 - Correlation with Label: -0.0139\n",
      "Feature 4 - Correlation with Label: -0.0196\n",
      "Feature 5 - Correlation with Label: 0.9616\n",
      "Feature 6 - Correlation with Label: 0.9468\n",
      "Feature 7 - Correlation with Label: 0.9541\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from cogdl.models.nn.gcn import GCN\n",
    "from cogdl.models.nn.dropedge_gcn import DropEdge_GCN\n",
    "from cogdl.models.nn.graphsage import Graphsage  # Import GraphSAGE\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "def strip_model_root(chkp):\n",
    "    new_state_dict = {}\n",
    "    # Clean up state_dict keys if needed\n",
    "    for key, value in chkp.items():\n",
    "        new_key = key.replace('model.', '')  # Remove the 'model.' prefix\n",
    "        new_state_dict[new_key] = value\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "# Model setup based on selection\n",
    "if model_type == \"gcn\":   \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    print(checkpoint)\n",
    "    \n",
    "    \n",
    "    new_state_dict = strip_model_root(checkpoint)\n",
    "\n",
    "    model = GCN(in_feats=23, hidden_size=64, out_feats=2, num_layers=2, dropout=0.2)\n",
    "\n",
    "    # Load model state\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "elif model_type == \"dropedge_gcn\":\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    model = DropEdge_GCN(\n",
    "        nfeat=23,                # Input feature dimension\n",
    "        nhid=24,                 # Hidden feature dimension\n",
    "        nclass=2,                # Output feature dimension\n",
    "        nhidlayer=1,             # Number of hidden blocks\n",
    "        dropout=0.3,             # Dropout ratio\n",
    "        baseblock=\"mutigcn\",     # Baseblock type\n",
    "        inputlayer=\"gcn\",        # Input layer type\n",
    "        outputlayer=\"gcn\",       # Output layer type\n",
    "        nbaselayer=1,            # Number of layers in one hidden block\n",
    "        activation=torch.relu,   # Activation function\n",
    "        withbn=False,            # Use batch normalization\n",
    "        withloop=False,          # Use self-feature modeling\n",
    "        aggrmethod=\"default\"     # Aggregation function for baseblock\n",
    "    )\n",
    "\n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "elif model_type == \"graphsage\":\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)    \n",
    "        checkpoint = strip_model_root(checkpoint)\n",
    "    \n",
    "    model = Graphsage(\n",
    "        num_features=dataset.data.x.shape[1],       # Input feature dimension\n",
    "        num_classes=2,   \n",
    "        **graphsage_run_params\n",
    "    )\n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare for evaluation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "data = dataset.data.to(device)  # Ensure data is on the same device as the model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "ind_map = dataset.index_mapping\n",
    "node_to_entity_mapping = {idx: entity_id for entity_id, idx in ind_map.items()}\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "    predicted_labels = output.argmax(dim=1)  # Get the class with the highest score for each node\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    \n",
    "    # Filter out nodes with label -1 (thread nodes)\n",
    "    valid_node_indices = (data.y != -1).nonzero(as_tuple=True)[0]\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Node ID': valid_node_indices.cpu().numpy(),\n",
    "        'Entity ID': [node_to_entity_mapping[i.item()] for i in valid_node_indices],\n",
    "        'Predicted Label': predicted_labels[valid_node_indices].cpu().numpy(),\n",
    "        'Probability Class 0': probabilities[valid_node_indices, 0].cpu().numpy(),\n",
    "        'Probability Class 1': probabilities[valid_node_indices, 1].cpu().numpy()\n",
    "    })\n",
    "    \n",
    "    # Output to a CSV file\n",
    "    predictions_df.to_csv('predictions.csv', index=False)\n",
    "    print(f\"Filtered predictions saved to 'predictions.csv'\")\n",
    " \n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "true_labels = data.y\n",
    "\n",
    "# Example: Calculate accuracy on the test set\n",
    "correct = (predicted_labels[data.test_mask] == true_labels[data.test_mask]).sum().item()\n",
    "total = data.test_mask.sum().item()\n",
    "accuracy = correct / total if total != 0 else 0\n",
    "\n",
    "# Print the accuracy on the test set\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "train_nodes = data.train_mask.nonzero(as_tuple=True)[0]  # Get indices of training nodes\n",
    "\n",
    "# Example: print the first 5 training nodes with their features and labels\n",
    "for idx in train_nodes[:5]:\n",
    "    feature = data.x[idx]  # Features of the node\n",
    "    label = data.y[idx]    # Label of the node\n",
    "    print(f\"Node ID: {idx.item()} | Feature: {feature} | Label: {label.item()}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Convert features and labels to numpy arrays for correlation\n",
    "features_np = data.x.cpu().numpy()  # Move features to CPU if on GPU\n",
    "labels_np = data.y.cpu().numpy()    # Similarly move labels to CPU if needed\n",
    "\n",
    "# Calculate correlation of each feature with the label\n",
    "correlations = [np.corrcoef(features_np[:, i], labels_np)[0, 1] for i in range(features_np.shape[1])]\n",
    "\n",
    "# Print correlations\n",
    "for i, corr in enumerate(correlations):\n",
    "    print(f\"Feature {i} - Correlation with Label: {corr:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e126a4a-b585-472f-aa12-dfea6d8dfb6d",
   "metadata": {},
   "source": [
    "## End Training Part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ca81d-3ff7-4086-a544-37b644431ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
